{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Finetune model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers import RobertaTokenizer, TFBertForMaskedLM\n",
            "import numpy as np\n",
            "import json\n",
            "import os"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Use this cell to check whether CUDA cores are correctly configured and available.\n",
            "Num GPUS should be 1"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Num GPUs Available:  1\n"
               ]
            }
         ],
         "source": [
            "import tensorflow as tf\n",
            "\n",
            "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Select a model with one of the following strings"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model_string = \"microsoft/codebert-base-mlm\"\n",
            "#model_string = \"microsoft/graphcodebert-base\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Get tokenizer and model based on selected model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
                  "Some layers from the model checkpoint at microsoft/graphcodebert-base were not used when initializing TFBertForMaskedLM: ['roberta', 'lm_head']\n",
                  "- This IS expected if you are initializing TFBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                  "- This IS NOT expected if you are initializing TFBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                  "Some layers of TFBertForMaskedLM were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['bert', 'mlm___cls']\n",
                  "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
               ]
            }
         ],
         "source": [
            "tokenizer = RobertaTokenizer.from_pretrained(model_string)\n",
            "model = TFBertForMaskedLM.from_pretrained(model_string)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Define dataset based on given json of paths"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = json.loads(open(\"data/paths.json\").read())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Select single or multipath AST representation with one of the following lines"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "text_lst = [method + \"(...) \" + dataset[method][0] for method in dataset] # Single path\n",
            "#text_lst = [method + \"(...) \" + \" | \".join(dataset[method]) for method in dataset] # Multiple paths"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Get number of methods and check that dataset format is correct"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'text_lst' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m len_lst \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext_lst\u001b[49m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_lst[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(len_lst))\n",
                  "\u001b[1;31mNameError\u001b[0m: name 'text_lst' is not defined"
               ]
            }
         ],
         "source": [
            "len_lst = [len(line.split(\" \")) for line in text_lst]\n",
            "print(text_lst[0])\n",
            "print(max(len_lst))\n",
            "print(len(text_lst))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Split the dataset into training and testing"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "testInvalidHostConfiguration(...) program -> local_variable_declaration -> ResolveEndpointFailedException | program -> local_variable_declaration -> variable_declarator -> cause | program -> local_variable_declaration -> variable_declarator -> method_invocation -> assertIsInstanceOf | program -> local_variable_declaration -> variable_declarator -> method_invocation -> argument_list -> class_literal -> ResolveEndpointFailedException | program -> local_variable_declaration -> variable_declarator -> method_invocation -> argument_list -> method_invocation -> exception | program -> local_variable_declaration -> variable_declarator -> method_invocation -> argument_list -> method_invocation -> getCause | program -> local_variable_declaration -> variable_declarator -> method_invocation -> argument_list -> method_invocation -> argument_list | program -> expression_statement -> method_invocation -> assertTrue | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> method_invocation -> cause | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> method_invocation -> getMessage | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> method_invocation -> argument_list | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> endsWith | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list -> \"Unknown parameters=[{xxx=true}]\"\n",
                  "setParentApplicationContext(...) program -> expression_statement -> assignment_expression -> field_access -> this.parentApplicationContext | program -> expression_statement -> assignment_expression -> field_access -> this.parentApplicationContext | program -> expression_statement -> assignment_expression -> parentApplicationContext\n"
               ]
            }
         ],
         "source": [
            "np.random.seed(42)\n",
            "np.random.shuffle(text_lst)\n",
            "train = text_lst[:int(len(text_lst)*0.8)]\n",
            "test = text_lst[int(len(text_lst)*0.8):]\n",
            "print(train[0])\n",
            "print(test[0])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Tokenize inputs\n",
            "dict_keys should be 'input_ids' and 'attention_mask'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "dict_keys(['input_ids', 'attention_mask'])\n"
               ]
            }
         ],
         "source": [
            "inputs = tokenizer(\n",
            "    train, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
            ")\n",
            "\n",
            "\n",
            "print(inputs.keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Tokenize inputs\n",
            "dict_keys should be 'input_ids', 'attention_mask' and 'labels'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
               ]
            }
         ],
         "source": [
            "inputs[\"labels\"] = tokenizer(\n",
            "    train, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
            ")[\"input_ids\"]\n",
            "print(inputs.keys())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Mask function name in 'input_ids'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tf.Tensor(\n",
                  "[    0 50264 50264 50264 50264  1640 41137   586 43839   400  1215 48123\n",
                  "  1215 32639 36466 43839  4787 18224 18547  2300   597 13355 48847  1721\n",
                  "   586 43839   400  1215 48123  1215 32639 36466 43839 15594  1215 32639\n",
                  "   271  2630 43839  1303  1721   586 43839   400  1215 48123  1215 32639\n",
                  " 36466 43839 15594  1215 32639   271  2630 43839  5448  1215 24701 15644\n",
                  " 43839 18088  6209     2], shape=(64,), dtype=int32)\n",
                  "tf.Tensor(\n",
                  "[    0 21959 49695 40534 49602  1640 41137   586 43839   400  1215 48123\n",
                  "  1215 32639 36466 43839  4787 18224 18547  2300   597 13355 48847  1721\n",
                  "   586 43839   400  1215 48123  1215 32639 36466 43839 15594  1215 32639\n",
                  "   271  2630 43839  1303  1721   586 43839   400  1215 48123  1215 32639\n",
                  " 36466 43839 15594  1215 32639   271  2630 43839  5448  1215 24701 15644\n",
                  " 43839 18088  6209     2], shape=(64,), dtype=int32)\n",
                  "<s><mask><mask><mask><mask>(...) program -> local_variable_declaration -> ResolveEndpointFailedException | program -> local_variable_declaration -> variable_declarator -> cause | program -> local_variable_declaration -> variable_declarator -> method_invocation -> assertIs</s>\n"
               ]
            }
         ],
         "source": [
            "inp_ids = []\n",
            "for inp in tokenizer(\n",
            "    train, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"tf\"\n",
            ")[\"input_ids\"].numpy():\n",
            "    i = 1\n",
            "    while inp[i] != 1640:\n",
            "        inp[i] = tokenizer.mask_token_id\n",
            "        i += 1\n",
            "    inp_ids.append(inp)\n",
            "inp_ids = tf.convert_to_tensor(inp_ids)\n",
            "inputs[\"input_ids\"] = inp_ids\n",
            "print(inputs[\"input_ids\"][0])\n",
            "print(inputs[\"labels\"][0])\n",
            "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Set checkpoint directory and callbacks"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "checkpoint_path = \"trained_models/2024-03-20.ckpt\"\n",
            "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
            "\n",
            "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
            "    filepath=checkpoint_path, save_weights_only=True, verbose=1\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Finetune model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 1/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.8934\n",
                  "Epoch 1: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 91s 116ms/step - loss: 1.0763 - accuracy: 0.8934\n",
                  "Epoch 2/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.9292\n",
                  "Epoch 2: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 78s 120ms/step - loss: 0.5565 - accuracy: 0.9292\n",
                  "Epoch 3/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9298\n",
                  "Epoch 3: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.5056 - accuracy: 0.9298\n",
                  "Epoch 4/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.4761 - accuracy: 0.9317\n",
                  "Epoch 4: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 78s 119ms/step - loss: 0.4761 - accuracy: 0.9317\n",
                  "Epoch 5/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.9346\n",
                  "Epoch 5: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.4387 - accuracy: 0.9346\n",
                  "Epoch 6/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.9375\n",
                  "Epoch 6: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.3948 - accuracy: 0.9375\n",
                  "Epoch 7/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.9418\n",
                  "Epoch 7: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.3434 - accuracy: 0.9418\n",
                  "Epoch 8/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9474\n",
                  "Epoch 8: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.2872 - accuracy: 0.9474\n",
                  "Epoch 9/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9550\n",
                  "Epoch 9: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.2297 - accuracy: 0.9550\n",
                  "Epoch 10/10\n",
                  "655/655 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9637\n",
                  "Epoch 10: saving model to trained_models\\2024-03-24.ckpt\n",
                  "655/655 [==============================] - 79s 120ms/step - loss: 0.1757 - accuracy: 0.9637\n"
               ]
            }
         ],
         "source": [
            "#Source:https://stackoverflow.com/questions/38445982/how-to-log-keras-loss-output-to-a-file\n",
            "from keras.callbacks import CSVLogger\n",
            "csv_logger = CSVLogger('logs.csv', append=False, separator=',')\n",
            "model.compile(\n",
            "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
            "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
            "    metrics=[\"accuracy\"],\n",
            ")\n",
            "history = model.fit(\n",
            "    [inputs.input_ids, inputs.attention_mask],\n",
            "    inputs.labels,\n",
            "    verbose=1,\n",
            "    batch_size=8,\n",
            "    epochs=10,\n",
            "    callbacks=[checkpoint_callback,csv_logger],\n",
            "    \n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Plot loss and accuracy"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "from tools import logs_to_list\n",
            "accuracy,loss= logs_to_list(\"logs.csv\")\n",
            "loss_epochs = range(1, len(accuracy) + 1)\n",
            "accuracy_epochs = range(1, len(accuracy) + 1)\n",
            "print(accuracy)\n",
            "print(loss)\n",
            "# figure for loss\n",
            "plt.plot(loss_epochs, loss, label='Loss', color='red')\n",
            "plt.title('Training Loss')\n",
            "plt.xlabel('Epochs')\n",
            "plt.ylabel('Loss')\n",
            "plt.legend()\n",
            "plt.show()\n",
            "# figure for accuracy\n",
            "plt.plot(accuracy_epochs, accuracy, label='Accuracy', color='red')\n",
            "plt.title('Accuracy')\n",
            "plt.xlabel('Epochs')\n",
            "plt.ylabel('Accuracy')\n",
            "plt.legend()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Utility cell to test fine tuned model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "createCPath\n"
               ]
            }
         ],
         "source": [
            "query = \"<mask><mask><mask>(...) program (return_statement (XPath))\"\n",
            "inp = tokenizer(query, return_tensors=\"tf\")\n",
            "mask_loc = np.where(inp.input_ids.numpy()[0] == tokenizer.mask_token_id)[0].tolist()\n",
            "out = model(inp).logits[0].numpy()\n",
            "predicted_tokens = np.argmax(out[mask_loc], axis=1).tolist()\n",
            "print(tokenizer.decode(predicted_tokens))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Push model to hugging face"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Token is valid (permission: write).\n",
                  "Your token has been saved in your configured git credential helpers (manager).\n",
                  "Your token has been saved to C:\\Users\\jaspe\\.cache\\huggingface\\token\n",
                  "Login successful\n"
               ]
            },
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "242493b60f244b7fa43836deb1a00569",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "tf_model.h5:   0%|          | 0.00/655M [00:00<?, ?B/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/plain": [
                     "CommitInfo(commit_url='https://huggingface.co/JasperGrant/ASTBERT-gb-5k-methods-multipath/commit/5aa1c434b458c82ce62ad08c6b7422c083f52e66', commit_message='Upload tokenizer', commit_description='', oid='5aa1c434b458c82ce62ad08c6b7422c083f52e66', pr_url=None, pr_revision=None, pr_num=None)"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from huggingface_hub import login\n",
            "\n",
            "login(token=\"TOKEN_HERE\", add_to_git_credential= True)\n",
            "\n",
            "model.push_to_hub(\"MODEL_NAME\")\n",
            "tokenizer.push_to_hub(\"MODEL_NAME\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.18"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
