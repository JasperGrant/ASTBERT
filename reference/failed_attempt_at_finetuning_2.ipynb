{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> AbstractMap', 'program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> SimpleImmutableEntry', 'program -> return_statement -> object_creation_expression -> generic_type -> type_arguments', 'program -> return_statement -> object_creation_expression -> argument_list -> \"a\"', 'program -> return_statement -> object_creation_expression -> argument_list -> 3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "dataset = json.loads(open(\"data/paths.json\").read())\n",
    "\n",
    "\n",
    "print(dataset[\"a3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method_name': 'a3', 'text': 'program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> AbstractMap | program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> SimpleImmutableEntry | program -> return_statement -> object_creation_expression -> generic_type -> type_arguments | program -> return_statement -> object_creation_expression -> argument_list -> \"a\" | program -> return_statement -> object_creation_expression -> argument_list -> 3'}\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset = [\n",
    "    {\"method_name\": method, \"text\": \" | \".join(dataset[method])} for method in dataset\n",
    "]\n",
    "\n",
    "\n",
    "print(formatted_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca00b41ae0347d3a4198ce3b36fa6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaspe\\Documents\\Group02\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jaspe\\.cache\\huggingface\\hub\\models--microsoft--codebert-base-mlm. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509bb843c6ba4111a8448ad465cdd85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6157d516cf94fbab52ecbe2ad40c11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3820547cfbe44efdade569a3812d0d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8620cd39d84f878c27415b53547585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/504 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c38eafc64b440409a499bc35cf54441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method_name': 'a3', 'text': 'program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> AbstractMap | program -> return_statement -> object_creation_expression -> generic_type -> scoped_type_identifier -> SimpleImmutableEntry | program -> return_statement -> object_creation_expression -> generic_type -> type_arguments | program -> return_statement -> object_creation_expression -> argument_list -> \"a\" | program -> return_statement -> object_creation_expression -> argument_list -> 3', 'input_ids': [0, 28644, 43839, 671, 1215, 29963, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 14569, 1215, 12528, 43839, 2850, 15911, 1215, 12528, 1215, 8009, 24072, 43839, 43649, 41151, 1721, 586, 43839, 671, 1215, 29963, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 14569, 1215, 12528, 43839, 2850, 15911, 1215, 12528, 1215, 8009, 24072, 43839, 21375, 37380, 41280, 46640, 1721, 586, 43839, 671, 1215, 29963, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 14569, 1215, 12528, 43839, 1907, 1215, 5384, 30179, 1721, 586, 43839, 671, 1215, 29963, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 4795, 1215, 8458, 43839, 22, 102, 113, 1721, 586, 43839, 671, 1215, 29963, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 4795, 1215, 8458, 43839, 155, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'masked_methods': [0, 102, 246, 2]}\n",
      "3620\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from datasets import Dataset\n",
    "from pandas import DataFrame\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "\n",
    "\n",
    "def tokenize_function(method):\n",
    "    model_inputs = tokenizer(\n",
    "        method[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    model_inputs[\"masked_methods\"] = tokenizer(method[\"method_name\"])[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = Dataset.from_pandas(DataFrame(formatted_dataset)).map(\n",
    "    tokenize_function, batched=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset[0])\n",
    "print(len(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool example of tokens to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some\n",
      "0 <s>\n",
      "28644 program\n",
      "43839  ->\n",
      "671  return\n",
      "1215 _\n",
      "29963 statement\n",
      "43839  ->\n",
      "7626  object\n",
      "1215 _\n",
      "39025 creation\n",
      "1215 _\n",
      "42819 expression\n",
      "43839  ->\n",
      "14569  generic\n",
      "1215 _\n",
      "12528 type\n",
      "43839  ->\n",
      "2850  sc\n",
      "15911 oped\n",
      "1215 _\n",
      "12528 type\n",
      "1215 _\n",
      "8009 ident\n",
      "24072 ifier\n",
      "43839  ->\n",
      "43649  Abstract\n",
      "41151 Map\n",
      "1721  |\n",
      "586  program\n",
      "43839  ->\n",
      "671  return\n",
      "1215 _\n",
      "29963 statement\n",
      "43839  ->\n",
      "7626  object\n",
      "1215 _\n",
      "39025 creation\n",
      "1215 _\n",
      "42819 expression\n",
      "43839  ->\n",
      "14569  generic\n",
      "1215 _\n",
      "12528 type\n",
      "43839  ->\n",
      "2850  sc\n",
      "15911 oped\n",
      "1215 _\n",
      "12528 type\n",
      "1215 _\n",
      "8009 ident\n",
      "24072 ifier\n",
      "43839  ->\n",
      "21375  Simple\n",
      "37380 Imm\n",
      "41280 utable\n",
      "46640 Entry\n",
      "1721  |\n",
      "586  program\n",
      "43839  ->\n",
      "671  return\n",
      "1215 _\n",
      "29963 statement\n",
      "43839  ->\n",
      "7626  object\n",
      "1215 _\n",
      "39025 creation\n",
      "1215 _\n",
      "42819 expression\n",
      "43839  ->\n",
      "14569  generic\n",
      "1215 _\n",
      "12528 type\n",
      "43839  ->\n",
      "1907  type\n",
      "1215 _\n",
      "5384 arg\n",
      "30179 uments\n",
      "1721  |\n",
      "586  program\n",
      "43839  ->\n",
      "671  return\n",
      "1215 _\n",
      "29963 statement\n",
      "43839  ->\n",
      "7626  object\n",
      "1215 _\n",
      "39025 creation\n",
      "1215 _\n",
      "42819 expression\n",
      "43839  ->\n",
      "4795  argument\n",
      "1215 _\n",
      "8458 list\n",
      "43839  ->\n",
      "22  \"\n",
      "102 a\n",
      "113 \"\n",
      "1721  |\n",
      "586  program\n",
      "43839  ->\n",
      "671  return\n",
      "1215 _\n",
      "29963 statement\n",
      "43839  ->\n",
      "7626  object\n",
      "1215 _\n",
      "39025 creation\n",
      "1215 _\n",
      "42819 expression\n",
      "43839  ->\n",
      "4795  argument\n",
      "1215 _\n",
      "8458 list\n",
      "43839  ->\n",
      "155  3\n",
      "2 </s>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n",
      "1 <pad>\n"
     ]
    }
   ],
   "source": [
    "for word in tokenized_dataset[0][\"input_ids\"]:\n",
    "\n",
    "    print(f\"{word} {tokenizer.decode(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method_name': 'createSourceAddrPanel', 'text': 'program -> local_variable_declaration -> modifiers | program -> local_variable_declaration -> JPanel | program -> local_variable_declaration -> variable_declarator -> sourceAddrPanel | program -> local_variable_declaration -> variable_declarator -> object_creation_expression -> HorizontalPanel | program -> local_variable_declaration -> variable_declarator -> object_creation_expression -> argument_list | program -> expression_statement -> method_invocation -> sourceAddrPanel | program -> expression_statement -> method_invocation -> setBorder | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> BorderFactory | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> createTitledBorder | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list -> method_invocation -> JMeterUtils | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list -> method_invocation -> getResString | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list -> method_invocation -> argument_list -> \"web_testing_source_ip\" | program -> expression_statement -> method_invocation -> sourceIpType | program -> expression_statement -> method_invocation -> setSelectedIndex | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> field_access -> field_access -> HTTPSamplerBase.SourceType.HOSTNAME | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> field_access -> field_access -> HTTPSamplerBase.SourceType.HOSTNAME | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> field_access -> HTTPSamplerBase.SourceType.HOSTNAME | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> ordinal | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list | program -> expression_statement -> method_invocation -> sourceAddrPanel | program -> expression_statement -> method_invocation -> add | program -> expression_statement -> method_invocation -> argument_list -> sourceIpType | program -> expression_statement -> assignment_expression -> sourceIpAddr | program -> expression_statement -> assignment_expression -> object_creation_expression -> JTextField | program -> expression_statement -> assignment_expression -> object_creation_expression -> argument_list | program -> expression_statement -> method_invocation -> sourceAddrPanel | program -> expression_statement -> method_invocation -> add | program -> expression_statement -> method_invocation -> argument_list -> sourceIpAddr | program -> return_statement -> sourceAddrPanel | program -> ERROR -> marker_annotation -> inheritDoc', 'input_ids': [0, 28644, 43839, 400, 1215, 48123, 1215, 32639, 36466, 43839, 47605, 1721, 586, 43839, 400, 1215, 48123, 1215, 32639, 36466, 43839, 344, 46968, 1721, 586, 43839, 400, 1215, 48123, 1215, 32639, 36466, 43839, 15594, 1215, 32639, 271, 2630, 43839, 1300, 20763, 338, 46968, 1721, 586, 43839, 400, 1215, 48123, 1215, 32639, 36466, 43839, 15594, 1215, 32639, 271, 2630, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 6746, 41539, 46968, 1721, 586, 43839, 400, 1215, 48123, 1215, 32639, 36466, 43839, 15594, 1215, 32639, 271, 2630, 43839, 7626, 1215, 39025, 1215, 42819, 43839, 4795, 1215, 8458, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 1300, 20763, 338, 46968, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 278, 43620, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 8323, 47249, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 1045, 565, 22764, 43620, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 39895, 5906, 41967, 5290, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 120, 20028, 34222, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 22, 10534, 1215, 33959, 1215, 17747, 1215, 1588, 113, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 1300, 100, 642, 40118, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 278, 14696, 17970, 45673, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 882, 1215, 28300, 43839, 882, 1215, 28300, 43839, 45217, 424, 30779, 34164, 4, 7061, 40118, 4, 725, 13556, 48307, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 882, 1215, 28300, 43839, 882, 1215, 28300, 43839, 45217, 424, 30779, 34164, 4, 7061, 40118, 4, 725, 13556, 48307, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 882, 1215, 28300, 43839, 45217, 424, 30779, 34164, 4, 7061, 40118, 4, 725, 13556, 48307, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 22474, 6204, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 1300, 20763, 338, 46968, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 1606, 1721, 586, 43839, 8151, 1215, 29963, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'masked_methods': [0, 32845, 7061, 20763, 338, 46968, 2]}\n",
      "{'method_name': 'checkEquals', 'text': 'program -> expression_statement -> method_invocation -> assertEquals | program -> expression_statement -> method_invocation -> argument_list -> jp1 | program -> expression_statement -> method_invocation -> argument_list -> jp2 | program -> expression_statement -> method_invocation -> assertEquals | program -> expression_statement -> method_invocation -> argument_list -> jp2 | program -> expression_statement -> method_invocation -> argument_list -> jp1 | program -> expression_statement -> method_invocation -> assertEquals | program -> expression_statement -> method_invocation -> argument_list -> jp1 | program -> expression_statement -> method_invocation -> argument_list -> jp1 | program -> expression_statement -> method_invocation -> assertEquals | program -> expression_statement -> method_invocation -> argument_list -> jp2 | program -> expression_statement -> method_invocation -> argument_list -> jp2 | program -> expression_statement -> method_invocation -> assertEquals | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> jp1 | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> hashCode | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> jp2 | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> hashCode | program -> expression_statement -> method_invocation -> argument_list -> method_invocation -> argument_list', 'input_ids': [0, 28644, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 18088, 28568, 1536, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 134, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 176, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 18088, 28568, 1536, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 176, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 134, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 18088, 28568, 1536, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 134, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 134, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 18088, 28568, 1536, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 176, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 1236, 642, 176, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 18088, 28568, 1536, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 1236, 642, 134, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 29524, 41555, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 1236, 642, 176, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 29524, 41555, 1721, 586, 43839, 8151, 1215, 29963, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 43839, 5448, 1215, 24701, 15644, 43839, 4795, 1215, 8458, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'masked_methods': [0, 15954, 28568, 1536, 2]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(split_tokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(split_tokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msplit_tokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Give the tokenized datset \"train and test\" labels\n",
    "split_tokenized_dataset = tokenized_dataset.train_test_split(\n",
    "    test_size=0.2, train_size=0.8, shuffle=True, seed=42\n",
    ")\n",
    "print(split_tokenized_dataset[\"train\"][0])\n",
    "print(split_tokenized_dataset[\"test\"][0])\n",
    "\n",
    "print(split_tokenized_dataset[\"train\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    predictions_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "\n",
    "    result[\"gen_len\"] = np.mean(predictions_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v, in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of DatasetDict object at 0x000001E20248C0B0>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 21\u001b[0m\n\u001b[0;32m      9\u001b[0m config\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#   \"microsoft/unixcoder-base\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     36\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     37\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32m<string>:126\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jaspe\\Documents\\Group02\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1489\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1482\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1488\u001b[0m ):\n\u001b[1;32m-> 1489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1492\u001b[0m     )\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1504\u001b[0m ):\n\u001b[0;32m   1505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1508\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertConfig, AdamW\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "\n",
    "\n",
    "config.is_decoder = True\n",
    "\n",
    "\n",
    "# model = AutoModel.from_pretrained(\n",
    "\n",
    "#   \"microsoft/unixcoder-base\",\n",
    "\n",
    "#   config=config,\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=split_tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.28434231877326965, 'token': 14445, 'token_str': ' div', 'sequence': '<s>public int div<mask>(a,b){return a * b;}</s>'}, {'score': 0.03896702826023102, 'token': 17895, 'token_str': ' sq', 'sequence': '<s>public int sq<mask>(a,b){return a * b;}</s>'}, {'score': 0.03892657905817032, 'token': 3023, 'token_str': ' x', 'sequence': '<s>public int x<mask>(a,b){return a * b;}</s>'}, {'score': 0.03314271569252014, 'token': 740, 'token_str': ' c', 'sequence': '<s>public int c<mask>(a,b){return a * b;}</s>'}, {'score': 0.02497895620763302, 'token': 6979, 'token_str': ' int', 'sequence': '<s>public int int<mask>(a,b){return a * b;}</s>'}]\n",
      "[{'score': 0.06596952676773071, 'token': 2765, 'token_str': 'By', 'sequence': '<s>public int<mask>By(a,b){return a * b;}</s>'}, {'score': 0.04894856736063957, 'token': 118, 'token_str': 'i', 'sequence': '<s>public int<mask>i(a,b){return a * b;}</s>'}, {'score': 0.045959293842315674, 'token': 10643, 'token_str': 'Of', 'sequence': '<s>public int<mask>Of(a,b){return a * b;}</s>'}, {'score': 0.03200862929224968, 'token': 176, 'token_str': '2', 'sequence': '<s>public int<mask>2(a,b){return a * b;}</s>'}, {'score': 0.025375008583068848, 'token': 1178, 'token_str': 'x', 'sequence': '<s>public int<mask>x(a,b){return a * b;}</s>'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM, pipeline\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "\n",
    "code_example = \"public int <mask><mask>(a,b){return a * b;}\"\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "for itw in fill_mask(code_example):\n",
    "    print(itw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
