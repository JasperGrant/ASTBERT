{
    "initialValue": "\n        return Pattern.compile(PARSE_PATTERN).matcher(\"\");\n      }\n  };\n",
    "checkExit": "\n      throw new ExitException(status);\n    }\n  }\n\n  private static class ExitException extends RuntimeException {\n    int status;\n\n    public ExitException(int status) {\n      this.status = status;\n    }\n\n    public int getStatus() {\n      return status;\n    }\n  }\n",
    "listStatusRecursively": "\n    if (isS3a(fs)) {\n      // S3A file system has an optimized recursive directory listing implementation however it doesn't support filtering.\n      // Therefore we filter the result set afterwards. This might be not so optimal in HDFS case (which does a tree walking) where a filter could have been used.\n      listS3FilesRecursive(fileStatus, fs, results);\n    } else {\n      generalListStatusRecursively(fs, fileStatus, results);\n    }\n  ",
    "testAsyncLoggingInitialization": "\n    HiveConf conf = new HiveConf();\n    conf.setBoolVar(ConfVars.HIVE_ASYNC_LOG_ENABLED, false);\n    LogUtils.initHiveLog4jCommon(conf, ConfVars.HIVE_LOG4J_FILE);\n    Log4jContextFactory log4jContextFactory = (Log4jContextFactory) LogManager.getFactory();\n    ContextSelector contextSelector = log4jContextFactory.getSelector();\n    assertTrue(contextSelector instanceof ClassLoaderContextSelector);\n\n    conf.setBoolVar(ConfVars.HIVE_ASYNC_LOG_ENABLED, true);\n    LogUtils.initHiveLog4jCommon(conf, ConfVars.HIVE_LOG4J_FILE);\n    log4jContextFactory = (Log4jContextFactory) LogManager.getFactory();\n    contextSelector = log4jContextFactory.getSelector();\n    assertTrue(contextSelector instanceof AsyncLoggerContextSelector);\n  ",
    "getColumns": "\n    if (!(assertConnection())) {\n      return null;\n    }\n    return getDatabaseConnection().getDatabaseMetaData().getColumns(\n        getDatabaseConnection().getDatabaseMetaData().getConnection().getCatalog(), null, table, \"%\");\n  ",
    "removeBackslashes": "\n      string = string.replaceAll(\"\\\\\\\\\", \"\");\n      length = string.length();\n    }\n  }\n\n  /**\n   * Construct a new instance.\n   *\n   * @param pattern Pattern to use for parsing or formatting\n   * @param forParsing Flag to indicate use of pattern\n   * @throws IllegalArgumentException if pattern is invalid\n   */\n  public HiveSqlDateTimeFormatter(final String pattern, final boolean forParsing) {\n    this(pattern, forParsing, Optional.absent());\n  }\n\n  /**\n   * Construct a new instance. An optional LocalDateTime can be provided when\n   * parsing must populate a field provided in the format string does not\n   * specify the date and time to use. If none is provided, the current\n   * {@link LocalDateTime#now()} will be used for each call to parse and format.\n   *\n   * @param pattern Pattern to use for parsing or formatting\n   * @param forParsing Flag to indicate use of pattern\n   * @param now Set an arbitrary context of the current local time\n   * @throws IllegalArgumentException if pattern is invalid\n   */\n  @VisibleForTesting\n  HiveSqlDateTimeFormatter(final String pattern, final boolean forParsing, final Optional<LocalDateTime> now) {\n    this.pattern = Objects.requireNonNull(pattern, \"Pattern cannot be null\");\n    this.now = Objects.requireNonNull(now);\n\n    this.tokens = new ArrayList<>();\n\n    Preconditions.checkArgument(pattern.length() < LONGEST_ACCEPTED_PATTERN, \"The input format is too long\");\n\n    parsePatternToTokens(pattern);\n\n    if (forParsing) {\n      verifyForParse();\n    } else {\n      verifyForFormat();\n    }\n  }\n\n  /**\n   * Parse pattern to list of tokens.\n   */\n  private void parsePatternToTokens(String pattern) {\n    tokens.clear();\n    String originalPattern = pattern;\n    pattern = pattern.toLowerCase();\n\n    // indexes of the substring we will check (includes begin, does not include end)\n    int begin=0, end=0;\n    String candidate;\n    Token lastAddedToken = null;\n    boolean fillMode = false;\n\n    while (begin < pattern.length()) {\n      // if begin hasn't progressed, then pattern is not parsable\n      if (begin != end) {\n        tokens.clear();\n        throw new IllegalArgumentException(\"Bad date/time conversion pattern: \" + pattern);\n      }\n\n      // find next token\n      for (int i = LONGEST_TOKEN_LENGTH; i > 0; i--) {\n        end = begin + i;\n        if (end > pattern.length()) { // don't go past the end of the pattern string\n          continue;\n        }\n        candidate = pattern.substring(begin, end);\n        if (isSeparator(candidate)) {\n          lastAddedToken = parseSeparatorToken(candidate, lastAddedToken, fillMode, begin);\n          begin = end;\n          break;\n        }\n        if (isIso8601Delimiter(candidate)) {\n          lastAddedToken = parseIso8601DelimiterToken(candidate, fillMode, begin);\n          begin = end;\n          break;\n        }\n        if (isNumericTemporalToken(candidate)) {\n          lastAddedToken = parseTemporalToken(originalPattern, candidate, fillMode, begin);\n          fillMode = false;\n          begin = end;\n          break;\n        }\n        if (isCharacterTemporalToken(candidate)) {\n          lastAddedToken = parseCharacterTemporalToken(originalPattern, candidate, fillMode, begin);\n          fillMode = false;\n          begin = end;\n          break;\n        }\n        if (isTimeZoneToken(candidate)) {\n          lastAddedToken = parseTimeZoneToken(candidate, fillMode, begin);\n          begin = end;\n          break;\n        }\n        if (isTextToken(candidate)) {\n          lastAddedToken = parseTextToken(originalPattern, fillMode, begin);\n          end = begin + lastAddedToken.length + 2; // skip 2 quotation marks\n          lastAddedToken.removeBackslashes();\n          begin = end;\n          break;\n        }\n        if (isFormatModifierToken(candidate)) {\n          checkFillModeOff(fillMode, begin);\n          fillMode = isFm(candidate);\n          if (!fillMode) {\n            formatExact = true;\n          }\n          begin = end;\n          break;\n        }\n      }\n    }\n  }\n\n  private boolean isSeparator(String candidate) {\n    return candidate.length() == 1 && VALID_SEPARATORS.contains(candidate);\n  }\n\n  private boolean isIso8601Delimiter(String candidate) {\n    return candidate.length() == 1 && VALID_ISO_8601_DELIMITERS.contains(candidate);\n  }\n\n  private boolean isNumericTemporalToken(String candidate) {\n    return NUMERIC_TEMPORAL_TOKENS.containsKey(candidate);\n  }\n\n  private boolean isCharacterTemporalToken(String candidate) {\n    return CHARACTER_TEMPORAL_TOKENS.containsKey(candidate);\n  }\n\n  private boolean isTimeZoneToken(String pattern) {\n    return TIME_ZONE_TOKENS.containsKey(pattern);\n  }\n\n  private boolean isTextToken(String candidate) {\n    return candidate.startsWith(\"\\\"\");\n  }\n\n  private boolean isFormatModifierToken(String candidate) {\n    return candidate.length() == 2 && VALID_FORMAT_MODIFIERS.contains(candidate);\n  }\n\n  private boolean isFm(String candidate) {\n    return \"fm\".equals(candidate);\n  }\n\n  private Token parseSeparatorToken(String candidate, Token lastAddedToken, boolean fillMode,\n      int begin) {\n    checkFillModeOff(fillMode, begin);\n    // try to clump separator with immediately preceding separators (e.g. \"---\" counts as one\n    // separator)\n    if (lastAddedToken != null && lastAddedToken.type == TokenType.SEPARATOR) {\n      lastAddedToken.string += candidate;\n      lastAddedToken.length += 1;\n    } else {\n      lastAddedToken = new Token(TokenType.SEPARATOR, candidate);\n      tokens.add(lastAddedToken);\n    }\n    return lastAddedToken;\n  }\n\n  private Token parseIso8601DelimiterToken(String candidate, boolean fillMode, int begin) {\n    checkFillModeOff(fillMode, begin);\n    Token lastAddedToken;\n    lastAddedToken = new Token(TokenType.ISO_8601_DELIMITER, candidate.toUpperCase());\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  }\n\n  private Token parseTemporalToken(String originalPattern, String candidate, boolean fillMode,\n      int begin) {\n    // for AM/PM, keep original case\n    if (NUMERIC_TEMPORAL_TOKENS.get(candidate) == ChronoField.AMPM_OF_DAY) {\n      int subStringEnd = begin + candidate.length();\n      candidate = originalPattern.substring(begin, subStringEnd);\n    }\n    Token lastAddedToken = new Token(TokenType.NUMERIC_TEMPORAL,\n        NUMERIC_TEMPORAL_TOKENS.get(candidate.toLowerCase()), candidate,\n        getTokenStringLength(candidate), fillMode);\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  }\n\n  private Token parseCharacterTemporalToken(String originalPattern, String candidate,\n      boolean fillMode, int begin) {\n    // keep original case\n    candidate = originalPattern.substring(begin, begin + candidate.length());\n\n    Token lastAddedToken = new Token(TokenType.CHARACTER_TEMPORAL,\n        CHARACTER_TEMPORAL_TOKENS.get(candidate.toLowerCase()), candidate,\n        getTokenStringLength(candidate), fillMode);\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  }\n\n  private Token parseTimeZoneToken(String candidate, boolean fillMode, int begin) {\n    checkFillModeOff(fillMode, begin);\n    Token lastAddedToken = new Token(TIME_ZONE_TOKENS.get(candidate), candidate,\n        getTokenStringLength(candidate), false);\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  }\n\n  private Token parseTextToken(String fullPattern, boolean fillMode, int begin) {\n    checkFillModeOff(fillMode, begin);\n    int end = begin;\n    do {\n      end = fullPattern.indexOf('\\\"', end + 1);\n      if (end == -1) {\n        throw new IllegalArgumentException(\n            \"Missing closing double quote (\\\") opened at index \" + begin);\n      }\n    // if double quote is escaped with a backslash, keep looking for the closing quotation mark\n    } while (\"\\\\\".equals(fullPattern.substring(end - 1, end)));\n    Token lastAddedToken = new Token(TokenType.TEXT, fullPattern.substring(begin + 1, end));\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  }\n\n  private void checkFillModeOff(boolean fillMode, int index) {\n    if (fillMode) {\n      throw new IllegalArgumentException(\"Bad date/time conversion pattern: \" + pattern +\n          \". Error at index \" + index + \": Fill mode modifier (FM) must \"\n          + \"be followed by a temporal token.\");\n    }\n  }\n\n  private int getTokenStringLength(String candidate) {\n    Integer length = SPECIAL_LENGTHS.get(candidate.toLowerCase());\n    if (length != null) {\n      return length;\n    }\n    return candidate.length();\n  }\n\n  /**\n   * Make sure the generated list of tokens is valid for parsing strings to datetime objects.\n   */\n  private void verifyForParse() {\n\n    // create a list of tokens' temporal fields\n    List<TemporalField> temporalFields = new ArrayList<>();\n    List<TemporalUnit> timeZoneTemporalUnits = new ArrayList<>();\n    int roundYearCount=0, yearCount=0;\n    boolean containsIsoFields=false, containsGregorianFields=false;\n    for (Token token : tokens) {\n      if (token.temporalField != null) {\n        temporalFields.add(token.temporalField);\n        if (token.temporalField == ChronoField.YEAR) {\n          if (token.string.startsWith(\"r\")) {\n            roundYearCount += 1;\n          } else {\n            yearCount += 1;\n          }\n        }\n        if (token.temporalField.isDateBased() && token.temporalField != ChronoField.DAY_OF_WEEK) {\n          if (ISO_8601_TEMPORAL_FIELDS.contains(token.temporalField)) {\n            containsIsoFields = true;\n          } else {\n            containsGregorianFields = true;\n          }\n        }\n      } else if (token.temporalUnit != null) {\n        timeZoneTemporalUnits.add(token.temporalUnit);\n      }\n    }\n\n    //check for illegal temporal fields\n    if (temporalFields.contains(IsoFields.QUARTER_OF_YEAR)) {\n      throw new IllegalArgumentException(\"Illegal field: q (\" + IsoFields.QUARTER_OF_YEAR + \")\");\n    }\n    if (temporalFields.contains(WeekFields.SUNDAY_START.dayOfWeek())) {\n      throw new IllegalArgumentException(\"Illegal field: d (\" + WeekFields.SUNDAY_START.dayOfWeek() + \")\");\n    }\n    if (temporalFields.contains(ChronoField.DAY_OF_WEEK) && containsGregorianFields) {\n      throw new IllegalArgumentException(\"Illegal field: dy/day (\" + ChronoField.DAY_OF_WEEK + \")\");\n    }\n    if (temporalFields.contains(ChronoField.ALIGNED_WEEK_OF_MONTH)) {\n      throw new IllegalArgumentException(\"Illegal field: w (\" + ChronoField.ALIGNED_WEEK_OF_MONTH + \")\");\n    }\n    if (temporalFields.contains(ChronoField.ALIGNED_WEEK_OF_YEAR)) {\n      throw new IllegalArgumentException(\"Illegal field: ww (\" + ChronoField.ALIGNED_WEEK_OF_YEAR + \")\");\n    }\n\n    if (containsGregorianFields && containsIsoFields) {\n      throw new IllegalArgumentException(\"Pattern cannot contain both ISO and Gregorian tokens\");\n    }\n    if (!(temporalFields.contains(ChronoField.YEAR)\n        || temporalFields.contains(IsoFields.WEEK_BASED_YEAR))) {\n      throw new IllegalArgumentException(\"Missing year token.\");\n    }\n    if (containsGregorianFields &&\n        !(temporalFields.contains(ChronoField.MONTH_OF_YEAR) &&\n            temporalFields.contains(ChronoField.DAY_OF_MONTH) ||\n            temporalFields.contains(ChronoField.DAY_OF_YEAR))) {\n      throw new IllegalArgumentException(\"Missing day of year or (month of year + day of month)\"\n          + \" tokens.\");\n    }\n    if (containsIsoFields &&\n        !(temporalFields.contains(IsoFields.WEEK_OF_WEEK_BASED_YEAR) &&\n            temporalFields.contains(ChronoField.DAY_OF_WEEK))) {\n      throw new IllegalArgumentException(\"Missing week of year (iw) or day of week (id) tokens.\");\n    }\n    if (roundYearCount > 0 && yearCount > 0) {\n      throw new IllegalArgumentException(\"Invalid duplication of format element: Both year and\"\n          + \"round year are provided\");\n    }\n    for (TemporalField tokenType : temporalFields) {\n      if (Collections.frequency(temporalFields, tokenType) > 1) {\n        throw new IllegalArgumentException(\n            \"Invalid duplication of format element: multiple \" + tokenType\n                + \" tokens provided.\");\n      }\n    }\n    if (temporalFields.contains(ChronoField.AMPM_OF_DAY) &&\n        !(temporalFields.contains(ChronoField.HOUR_OF_DAY) ||\n            temporalFields.contains(ChronoField.HOUR_OF_AMPM))) {\n      throw new IllegalArgumentException(\"AM/PM provided but missing hour token.\");\n    }\n    if (temporalFields.contains(ChronoField.AMPM_OF_DAY) &&\n        temporalFields.contains(ChronoField.HOUR_OF_DAY)) {\n      throw new IllegalArgumentException(\"Conflict between median indicator and hour token.\");\n    }\n    if (temporalFields.contains(ChronoField.HOUR_OF_AMPM) &&\n        temporalFields.contains(ChronoField.HOUR_OF_DAY)) {\n      throw new IllegalArgumentException(\"Conflict between hour of day and hour of am/pm token.\");\n    }\n    if (temporalFields.contains(ChronoField.DAY_OF_YEAR) &&\n        (temporalFields.contains(ChronoField.DAY_OF_MONTH) ||\n            temporalFields.contains(ChronoField.MONTH_OF_YEAR))) {\n      throw new IllegalArgumentException(\"Day of year provided with day or month token.\");\n    }\n    if (temporalFields.contains(ChronoField.SECOND_OF_DAY) &&\n        (temporalFields.contains(ChronoField.HOUR_OF_DAY) ||\n            temporalFields.contains(ChronoField.HOUR_OF_AMPM) ||\n            temporalFields.contains(ChronoField.MINUTE_OF_HOUR) ||\n            temporalFields.contains(ChronoField.SECOND_OF_MINUTE))) {\n      throw new IllegalArgumentException(\n          \"Second of day token conflicts with other token(s).\");\n    }\n    if (timeZoneTemporalUnits.contains(ChronoUnit.MINUTES) &&\n        !timeZoneTemporalUnits.contains(ChronoUnit.HOURS)) {\n      throw new IllegalArgumentException(\"Time zone minute token provided without time zone hour token.\");\n    }\n  }\n\n  /**\n   * Make sure the generated list of tokens is valid for formatting datetime objects to strings.\n   */\n  private void verifyForFormat() {\n    for (Token token : tokens) {\n      if (token.type == TokenType.TIMEZONE) {\n        throw new IllegalArgumentException(token.string.toUpperCase() + \" not a valid format for \"\n            + \"timestamp or date.\");\n      }\n    }\n  }\n\n  public String format(Timestamp ts) {\n    StringBuilder fullOutputSb = new StringBuilder();\n    String outputString = null;\n    int value;\n    LocalDateTime localDateTime =\n        LocalDateTime.ofEpochSecond(ts.toEpochSecond(), ts.getNanos(), ZoneOffset.UTC);\n    for (Token token : tokens) {\n      switch (token.type) {\n      case NUMERIC_TEMPORAL:\n      case CHARACTER_TEMPORAL:\n        try {\n          value = localDateTime.get(token.temporalField);\n          if (token.type == TokenType.NUMERIC_TEMPORAL) {\n            outputString = formatNumericTemporal(value, token);\n          } else {\n            outputString = formatCharacterTemporal(value, token);\n          }\n        } catch (DateTimeException e) {\n          throw new IllegalArgumentException(token.temporalField + \" couldn't be obtained from \"\n              + \"LocalDateTime \" + localDateTime, e);\n        }\n        break;\n      case TIMEZONE: //invalid for timestamp and date\n        throw new IllegalArgumentException(token.string.toUpperCase() + \" not a valid format for \"\n            + \"timestamp or date.\");\n      case SEPARATOR:\n      case TEXT:\n        outputString = token.string;\n        break;\n      case ISO_8601_DELIMITER:\n        outputString = token.string.toUpperCase();\n        break;\n      default:\n        // won't happen\n      }\n      fullOutputSb.append(outputString);\n    }\n    return fullOutputSb.toString();\n  }\n\n  public String format(Date date) {\n    return format(Timestamp.ofEpochSecond(date.toEpochSecond()));\n  }\n\n  private String formatNumericTemporal(int value, Token token) {\n    String output;\n    if (token.temporalField == ChronoField.AMPM_OF_DAY) {\n      output = value == 0 ? \"a\" : \"p\";\n      output += token.string.length() == 2 ? \"m\" : \".m.\";\n      if (token.string.startsWith(\"A\") || token.string.startsWith(\"P\")) {\n        output = output.toUpperCase();\n      }\n    } else { // it's a numeric value\n\n      if (token.temporalField == ChronoField.HOUR_OF_AMPM && value == 0) {\n        value = 12;\n      }\n      try {\n        output = Integer.toString(value);\n        output = padOrTruncateNumericTemporal(token, output);\n      } catch (Exception e) {\n        throw new IllegalArgumentException(\"Value: \" + value + \" could not be cast to string.\", e);\n      }\n    }\n    return output;\n  }\n\n  private String formatCharacterTemporal(int value, Token token) {\n    String output = null;\n    if (token.temporalField == ChronoField.MONTH_OF_YEAR) {\n      output = Month.of(value).getDisplayName(TextStyle.FULL, Locale.US);\n    } else if (token.temporalField == ChronoField.DAY_OF_WEEK) {\n      output = DayOfWeek.of(value).getDisplayName(TextStyle.FULL, Locale.US);\n    }\n    if (output == null) {\n      throw new IllegalStateException(\"TemporalField: \" + token.temporalField + \" not valid for \"\n          + \"character formatting.\");\n    }\n\n    // set length\n    if (output.length() > token.length) {\n      output = output.substring(0, token.length); // truncate to length\n    } else if (!token.fillMode && output.length() < token.length) {\n      output = StringUtils.rightPad(output, token.length); //pad to size\n    }\n\n    // Set case.\n    // If the first letter is lowercase then the output is lowercase: 'mONTH' -> 'may'\n    // If the first two letters are uppercase then the output is uppercase: 'MOnth' -> 'MAY'\n    // If the first letter is uppercase and the second is lowercase then the output is capitalized:\n    //     'Month' -> 'May'.\n    if (Character.isLowerCase(token.string.charAt(0))) {\n      output = output.toLowerCase();\n    } else if (Character.isUpperCase(token.string.charAt(1))) {\n      output = output.toUpperCase();\n    } else {\n      output = capitalize(output);\n    }\n    return output;\n  }\n\n  /**\n   * To match token.length, pad left with zeroes or truncate.\n   * Omit padding if fill mode (FM) modifier on.\n   */\n  private String padOrTruncateNumericTemporal(Token token, String output) {\n    //exception\n    if (token.temporalField == ChronoField.NANO_OF_SECOND) {\n      output = StringUtils.leftPad(output, 9, '0'); // pad left to length 9\n      if (output.length() > token.length) {\n        output = output.substring(0, token.length); // truncate right to size\n      }\n      if (token.string.equalsIgnoreCase(\"ff\")) {\n        output = output.replaceAll(\"0*$\", \"\"); //truncate trailing 0's\n      }\n\n      // the rule\n    } else {\n      if (output.length() < token.length && !token.fillMode) {\n        output = StringUtils.leftPad(output, token.length, '0'); // pad left\n      } else if (output.length() > token.length) {\n        output = output.substring(output.length() - token.length); // truncate left\n      }\n      if (token.fillMode) {\n        output = output.replaceAll(\"^0*\", \"\"); //truncate leading 0's\n      }\n    }\n    if (output.isEmpty()) {\n      output = \"0\";\n    }\n    return output;\n  }\n\n  public Timestamp parseTimestamp(final String fullInput) {\n    String substring;\n    int index = 0;\n    int value;\n    int timeZoneHours = 0, timeZoneMinutes = 0;\n    int iyyy = 0, iw = 0;\n    List<Integer> temporalValues = new ArrayList<>();\n\n    for (Token token : tokens) {\n      switch (token.type) {\n      case NUMERIC_TEMPORAL:\n      case CHARACTER_TEMPORAL:\n        if (token.type == TokenType.NUMERIC_TEMPORAL) {\n          substring = getNextNumericSubstring(fullInput, index, token); // e.g. yy-m -> yy\n          value = parseNumericTemporal(substring, token); // e.g. 18->2018\n        } else {\n          substring = getNextCharacterSubstring(fullInput, index, token); //e.g. Marcharch -> March\n          value = parseCharacterTemporal(substring, token); // e.g. July->07\n        }\n        temporalValues.add(value);\n\n        //update IYYY and IW if necessary\n        if (token.temporalField == IsoFields.WEEK_BASED_YEAR) {\n          iyyy = value;\n        }\n        if (token.temporalField == IsoFields.WEEK_OF_WEEK_BASED_YEAR) {\n          iw = value;\n        }\n\n        index += substring.length();\n        break;\n      case TIMEZONE:\n        if (token.temporalUnit == ChronoUnit.HOURS) {\n          String nextCharacter = fullInput.substring(index, index + 1);\n          if (\"-\".equals(nextCharacter) || \"+\".equals(nextCharacter)) {\n            index++;\n          }\n          // parse next two digits\n          substring = getNextNumericSubstring(fullInput, index, index + 2, token);\n          try {\n            timeZoneHours = Integer.parseInt(substring);\n          } catch (NumberFormatException e) {\n            throw new IllegalArgumentException(\"Couldn't parse substring \\\"\" + substring +\n                \"\\\" with token \" + token + \" to int. Pattern is \" + pattern, e);\n          }\n          if (timeZoneHours < -15 || timeZoneHours > 15) {\n            throw new IllegalArgumentException(\"Couldn't parse substring \\\"\" + substring +\n                \"\\\" to TZH because TZH range is -15 to +15. Pattern is \" + pattern);\n          }\n        } else { // time zone minutes\n          substring = getNextNumericSubstring(fullInput, index, token);\n          try {\n            timeZoneMinutes = Integer.parseInt(substring);\n          } catch (NumberFormatException e) {\n            throw new IllegalArgumentException(\"Couldn't parse substring \\\"\" + substring +\n            \"\\\" with token \" + token + \" to int. Pattern is \" + pattern, e);\n          }\n          if (timeZoneMinutes < 0 || timeZoneMinutes > 59) {\n            throw new IllegalArgumentException(\"Couldn't parse substring \\\"\" + substring +\n                \"\\\" to TZM because TZM range is 0 to 59. Pattern is \" + pattern);\n          }\n        }\n        index += substring.length();\n        break;\n      case SEPARATOR:\n        index = parseSeparator(fullInput, index, token);\n        break;\n      case ISO_8601_DELIMITER:\n      case TEXT:\n        index = parseText(fullInput, index, token);\n      default:\n        //do nothing\n      }\n    }\n\n    checkForLeftoverInput(fullInput, index);\n\n    checkForInvalidIsoWeek(iyyy, iw);\n\n    return getTimestampFromValues(tokens, temporalValues);\n  }\n\n  /**\n   * Anything left unparsed at end of input string? Throw error.\n   * @param fullInput full input String\n   * @param index where we left off parsing\n   */\n  private void checkForLeftoverInput(String fullInput, int index) {\n    if (!fullInput.substring(index).isEmpty()) {\n      throw new IllegalArgumentException(\"Leftover input after parsing: \" +\n          fullInput.substring(index) + \" in string \" + fullInput);\n    }\n  }\n\n  /**\n   * Check for WEEK_OF_WEEK_BASED_YEAR (iw) value 53 when WEEK_BASED_YEAR (iyyy) does not have 53\n   * weeks.\n   */\n  private void checkForInvalidIsoWeek(int iyyy, int iw) {\n    if (iyyy == 0) {\n      return;\n    }\n\n    LocalDateTime ldt = LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC);\n    try {\n      ldt = ldt.with(IsoFields.WEEK_BASED_YEAR, iyyy);\n      ldt = ldt.with(IsoFields.WEEK_OF_WEEK_BASED_YEAR, iw);\n    } catch (DateTimeException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (ldt.getYear() != iyyy) {\n      throw new IllegalArgumentException(\"ISO year \" + iyyy + \" does not have \" + iw + \" weeks.\");\n    }\n  }\n\n  /**\n   * Create a timestamp from the list of values parsed from the input and the list of tokens\n   * parsed from the pattern input.\n   *\n   * We need to be able to parse input like \"29.02.2000\" (with pattern \"dd.mm.yyyy\")\n   * correctly \u2013 if we assigned the day value to the timestamp before the year value, then\n   * output would be 2000-02-28. So before creating the Timestamp we have to:\n   *\n   *  - Make a list of pairs.\n   *    Left value: only the Tokens that represent a temporal value\n   *    Right value: their corresponding int values parsed from the input\n   *  - Sort this list by length of base unit, in descending order (years before months, etc.).\n   *  - Then create the parsed output Timestamp object by creating a LocalDateTime object using the\n   *    token's TemporalField and the value.\n   *\n   * @param tokens list of tokens of any type, in order of pattern input\n   * @param temporalValues list of integer values parsed from the input, in order of input\n   * @return the parsed Timestamp\n   * @throws IllegalStateException if temporal values list and tokens in tokens list sizes are not\n   * equal\n   */\n  private Timestamp getTimestampFromValues(List<Token> tokens, List<Integer> temporalValues) {\n\n    // Get list of temporal Tokens\n    List<Token> temporalTokens = tokens.stream()\n        .filter(token-> token.type == TokenType.NUMERIC_TEMPORAL\n            || token.type == TokenType.CHARACTER_TEMPORAL)\n        .collect(Collectors.toList());\n    Preconditions.checkState(temporalTokens.size() == temporalValues.size(),\n        \"temporalTokens list length (\" + temporalTokens.size()\n        + \") differs from that of temporalValues (length: \" + temporalValues.size() + \")\");\n\n    // Get sorted list of temporal Token/value Pairs\n    List<ImmutablePair<Token, Integer>> tokenValueList = new ArrayList<>(temporalTokens.size());\n    for (int i = 0; i < temporalTokens.size(); i++) {\n      ImmutablePair<Token, Integer> pair =\n          new ImmutablePair<>(temporalTokens.get(i), temporalValues.get(i));\n      tokenValueList.add(pair);\n    }\n    tokenValueList.sort(((Comparator<ImmutablePair<Token, Integer>>) (o1, o2) -> {\n              Token token1 = o1.left;\n              Token token2 = o2.left;\n              return token1.temporalField.getBaseUnit().getDuration()\n                .compareTo(token2.temporalField.getBaseUnit().getDuration());\n            }).reversed());\n\n    // Create Timestamp\n    LocalDateTime ldt = LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC);\n    for (Pair<Token, Integer> pair : tokenValueList) {\n      TemporalField temporalField = pair.getLeft().temporalField;\n      int value = pair.getRight();\n      try {\n        ldt = ldt.with(temporalField, value);\n      } catch (DateTimeException e){\n        throw new IllegalArgumentException(\n            \"Value \" + value + \" not valid for token \" + temporalField);\n      }\n    }\n    return Timestamp.ofEpochSecond(ldt.toEpochSecond(ZoneOffset.UTC), ldt.getNano());\n  }\n\n  public Date parseDate(String input){\n    return Date.ofEpochMilli(parseTimestamp(input).toEpochMilli());\n  }\n\n  /**\n   * Return the next substring to parse. Length is either specified or token.length, but a\n   * separator or an ISO-8601 delimiter can cut the substring short. (e.g. if the token pattern is\n   * \"YYYY\" we expect the next 4 characters to be 4 numbers. However, if it is \"976/\" then we\n   * return \"976\" because a separator cuts it short.)\n   */\n  private String getNextNumericSubstring(String s, int begin, Token token) {\n    return getNextNumericSubstring(s, begin, begin + token.length, token);\n  }\n\n  private String getNextNumericSubstring(String s, int begin, int end, Token token) {\n    if (end > s.length()) {\n      end = s.length();\n    }\n    s = s.substring(begin, end);\n    if (token.temporalField == ChronoField.AMPM_OF_DAY) {\n      if (s.charAt(1) == 'm' || s.charAt(1) == 'M') { // length 2\n        return s.substring(0, 2);\n      } else {\n        return s;\n      }\n    }\n    // if it's a character temporal, the first non-letter character is a delimiter\n    if (token.type == TokenType.CHARACTER_TEMPORAL && s.matches(\".*[^A-Za-z].*\")) {\n      s = s.split(\"[^A-Za-z]\", 2)[0];\n\n    // if it's a numeric element, next non-numeric character is a delimiter. Don't worry about\n    // AM/PM since we've already handled that case.\n    } else if ((token.type == TokenType.NUMERIC_TEMPORAL || token.type == TokenType.TIMEZONE)\n        && s.matches(\".*\\\\D.*\")) {\n      s = s.split(\"\\\\D\", 2)[0];\n    }\n\n    return s;\n  }\n\n  /**\n   * Get the integer value of a numeric temporal substring.\n   *\n   * @param substring the next parseable substring of the input string\n   * @param token Token representing the next element of the pattern\n   * @return int value of temporal\n   * @throws IllegalArgumentException if substring is not parseable or if its value is outside of\n   * token's allowed range (e.g. token is hh/hh12 (range: 1-12) and substring is \"0\")\n   */\n  private int parseNumericTemporal(String substring, Token token) {\n    checkFormatExact(substring, token);\n\n    // exceptions to the rule\n    if (token.temporalField == ChronoField.AMPM_OF_DAY) {\n      return substring.toLowerCase().startsWith(\"a\") ? AM : PM;\n    }\n    if (token.temporalField == ChronoField.HOUR_OF_AMPM) {\n      if (\"12\".equals(substring)) {\n        return 0;\n      }\n      if (\"0\".equals(substring)) {\n        throw new IllegalArgumentException(\"Value of hour of day (hh/hh12) in input is 0. \"\n            + \"The value should be between 1 and 12.\");\n      }\n    }\n    if (token.temporalField == ChronoField.YEAR\n        || token.temporalField == IsoFields.WEEK_BASED_YEAR) {\n\n      String currentYearString;\n      if (token.temporalField == ChronoField.YEAR) {\n        currentYearString = Integer.toString(this.now.or(LocalDateTime.now()).getYear());\n      } else {\n        currentYearString = Integer.toString(this.now.or(LocalDateTime.now()).get(IsoFields.WEEK_BASED_YEAR));\n      }\n\n      //deal with round years\n      if (token.string.startsWith(\"r\") && substring.length() == 2) {\n        int currFirst2Digits = Integer.parseInt(currentYearString.substring(0, 2));\n        int currLast2Digits = Integer.parseInt(currentYearString.substring(2));\n        int valLast2Digits = Integer.parseInt(substring);\n        if (valLast2Digits < 50 && currLast2Digits >= 50) {\n          currFirst2Digits += 1;\n        } else if (valLast2Digits >= 50 && currLast2Digits < 50) {\n          currFirst2Digits -= 1;\n        }\n        substring = Integer.toString(currFirst2Digits) + substring;\n      } else { // fill in prefix digits with current date\n        substring = currentYearString.substring(0, 4 - substring.length()) + substring;\n      }\n\n    } else if (token.temporalField == ChronoField.NANO_OF_SECOND) {\n      int i = Integer.min(token.length, substring.length());\n      substring += StringUtils.repeat(\"0\", NANOS_MAX_LENGTH - i);\n    }\n\n    // the rule\n    try {\n      return Integer.parseInt(substring);\n    } catch (NumberFormatException e) {\n      throw new IllegalArgumentException(\"Couldn't parse substring \\\"\" + substring +\n          \"\\\" with token \" + token + \" to integer. Pattern is \" + pattern, e);\n    }\n  }\n\n  private static final String MONTH_REGEX;\n  private static final String DAY_OF_WEEK_REGEX;\n  static {\n    StringBuilder sb = new StringBuilder();\n    String or = \"\";\n    for (Month month : Month.values()) {\n      sb.append(or).append(month);\n      or = \"|\";\n    }\n    MONTH_REGEX = sb.toString();\n    sb = new StringBuilder();\n    or = \"\";\n    for (DayOfWeek dayOfWeek : DayOfWeek.values()) {\n      sb.append(or).append(dayOfWeek);\n      or = \"|\";\n    }\n    DAY_OF_WEEK_REGEX = sb.toString();\n  }\n\n  private String getNextCharacterSubstring(String fullInput, int index, Token token) {\n    int end = index + token.length;\n    if (end > fullInput.length()) {\n      end = fullInput.length();\n    }\n    String substring = fullInput.substring(index, end);\n    if (token.length == 3) { //dy, mon\n      return substring;\n    }\n\n    // patterns day, month\n    String regex;\n    if (token.temporalField == ChronoField.MONTH_OF_YEAR) {\n      regex = MONTH_REGEX;\n    } else if (token.temporalField == ChronoField.DAY_OF_WEEK) {\n      regex = DAY_OF_WEEK_REGEX;\n    } else {\n      throw new IllegalArgumentException(\"Error at index \" + index + \": \" + token + \" not a \"\n          + \"character temporal with length not 3\");\n    }\n    Matcher matcher = Pattern.compile(regex, Pattern.CASE_INSENSITIVE).matcher(substring);\n    if (matcher.find()) {\n      return substring.substring(0, matcher.end());\n    }\n    throw new IllegalArgumentException(\n        \"Couldn't find \" + token.string + \" in substring \" + substring + \" at index \" + index);\n  }\n\n  private int parseCharacterTemporal(String substring, Token token) {\n    try {\n      if (token.temporalField == ChronoField.MONTH_OF_YEAR) {\n        if (token.length == 3) {\n          return Month.from(MONTH_FORMATTER.parse(capitalize(substring))).getValue();\n        } else {\n          return Month.valueOf(substring.toUpperCase()).getValue();\n        }\n      } else if (token.temporalField == ChronoField.DAY_OF_WEEK) {\n        if (token.length == 3) {\n          return DayOfWeek.from(DAY_OF_WEEK_FORMATTER.parse(capitalize(substring))).getValue();\n        } else {\n          return DayOfWeek.valueOf(substring.toUpperCase()).getValue();\n        }\n      }\n    } catch (Exception e) {\n      throw new IllegalArgumentException(\n          \"Couldn't parse substring \\\"\" + substring + \"\\\" with token \" + token + \" to integer.\"\n              + \"Pattern is \" + pattern, e);\n    }\n    throw new IllegalArgumentException(\n        \"token: (\" + token + \") isn't a valid character temporal. Pattern is \" + pattern);\n  }\n\n  /**\n   * @throws IllegalArgumentException if input length doesn't match expected (token) length\n   */\n  private void checkFormatExact(String substring, Token token) {\n    // AM/PM defaults to length 4 but make it 2 for FX check if the pattern actually has length 2\n    if (formatExact && token.temporalField == ChronoField.AMPM_OF_DAY) {\n      token.length = token.string.length();\n    }\n    if (formatExact\n        && !(token.fillMode || token.temporalField == ChronoField.NANO_OF_SECOND)\n        && token.length != substring.length()) {\n      throw new IllegalArgumentException(\n          \"FX on and expected token length \" + token.length + \" for token \" + token\n              + \" does not match substring (\" + substring + \") length \" + substring.length());\n    }\n  }\n\n  /**\n   * Parse the next separator(s). At least one separator character is expected. Separator\n   * characters are interchangeable.\n   *\n   * Caveat: If the last separator character in the separator substring is \"-\" and is immediately\n   *     followed by a time zone hour (tzh) token, it's a negative sign and not counted as a\n   *     separator, UNLESS this is the only separator character in the separator substring (in\n   *     which case it is not counted as the negative sign).\n   *\n   * @throws IllegalArgumentException if separator is missing or if FX is on and separator doesn't\n   * match the expected separator pattern exactly\n   */\n  private int parseSeparator(String fullInput, int index, Token token) {\n    int begin = index;\n    String s;\n    StringBuilder separatorsFound = new StringBuilder();\n\n    while (index < fullInput.length() &&\n        VALID_SEPARATORS.contains(fullInput.substring(index, index + 1))) {\n      s = fullInput.substring(index, index + 1);\n      if (!isLastCharacterOfSeparator(index, fullInput)\n          || !(\"-\".equals(s) && (nextTokenIs(\"tzh\", token)))\n          || separatorsFound.length() == 0) {\n        separatorsFound.append(s);\n      }\n      index++;\n    }\n\n    if (separatorsFound.length() == 0) {\n      throw new IllegalArgumentException(\"Missing separator at index \" + index);\n    }\n    if (formatExact && !token.string.equals(separatorsFound.toString())) {\n      throw new IllegalArgumentException(\"FX on and separator found: \" + separatorsFound\n          + \" does not match expected separator: \" + token.string);\n    }\n\n    return begin + separatorsFound.length();\n  }\n\n  private int parseText(String fullInput, int index, Token token) {\n    String substring;\n    substring = fullInput.substring(index, index + token.length);\n    if (!token.string.equalsIgnoreCase(substring)) {\n      throw new IllegalArgumentException(\n          \"Wrong input at index \" + index + \": Expected: \\\"\" + token.string + \"\\\" but got: \\\"\"\n              + substring + \"\\\" for token: \" + token);\n    }\n    return index + token.length;\n  }\n\n  /**\n   * Is the next character something other than a separator?\n   */\n  private boolean isLastCharacterOfSeparator(int index, String string) {\n    if (index == string.length() - 1) { // if we're at the end of the string, yes\n      return true;\n    }\n    return !VALID_SEPARATORS.contains(string.substring(index + 1, index + 2));\n  }\n\n  /**\n   * Does the temporalUnit/temporalField of the next token match the pattern's?\n   */\n  private boolean nextTokenIs(String pattern, Token currentToken) {\n    // make sure currentToken isn't the last one\n    final int idx = tokens.indexOf(currentToken);\n    if (idx == tokens.size() - 1) {\n      return false;\n    }\n    Token nextToken = tokens.get(idx + 1);\n    pattern = pattern.toLowerCase();\n    return (isTimeZoneToken(pattern) && TIME_ZONE_TOKENS.get(pattern) == nextToken.temporalUnit\n        || isNumericTemporalToken(pattern) && NUMERIC_TEMPORAL_TOKENS.get(pattern) == nextToken.temporalField\n        || isCharacterTemporalToken(pattern) && CHARACTER_TEMPORAL_TOKENS.get(pattern) == nextToken.temporalField);\n  }\n\n  public String getPattern() {\n    return pattern;\n  }\n\n  /**\n   * @return a copy of token list\n   */\n  protected List<Token> getTokens() {\n    return new ArrayList<>(tokens);\n  }\n\n  private static String capitalize(String substring) {\n    return StringUtils.capitalize(substring.toLowerCase());\n  }\n",
    "testIntBinary": "\n    byte[] value = ByteBuffer.allocate(4).putInt(10).array();\n    byte[] encoded = new IntegerLexicoder().encode(10);\n\n    byte[] lex = AccumuloIndexLexicoder.encodeValue(value, serdeConstants.INT_TYPE_NAME, false);\n    assertArrayEquals(lex, encoded);\n\n    value = ByteBuffer.allocate(2).putShort((short) 10).array();\n    lex = AccumuloIndexLexicoder.encodeValue(value, serdeConstants.SMALLINT_TYPE_NAME, false);\n    assertArrayEquals(lex, encoded);\n\n    value = ByteBuffer.allocate(1).put((byte)10).array();\n    lex = AccumuloIndexLexicoder.encodeValue(value, serdeConstants.TINYINT_TYPE_NAME, false);\n    assertArrayEquals(lex, encoded);\n  ",
    "process_stage1": "\n    try {\n      commandLine = new GnuParser().parse(options, argv);\n      Properties confProps = commandLine.getOptionProperties(\"hiveconf\");\n      for (String propKey : confProps.stringPropertyNames()) {\n        // with HIVE-11304, hive.root.logger cannot have both logger name and log level.\n        // if we still see it, split logger and level separately for hive.root.logger\n        // and hive.log.level respectively\n        if (propKey.equalsIgnoreCase(\"hive.root.logger\")) {\n          CommonCliOptions.splitAndSetLogger(propKey, confProps);\n        } else {\n          System.setProperty(propKey, confProps.getProperty(propKey));\n        }\n      }\n\n      Properties hiveVars = commandLine.getOptionProperties(\"define\");\n      for (String propKey : hiveVars.stringPropertyNames()) {\n        hiveVariables.put(propKey, hiveVars.getProperty(propKey));\n      }\n\n      Properties hiveVars2 = commandLine.getOptionProperties(\"hivevar\");\n      for (String propKey : hiveVars2.stringPropertyNames()) {\n        hiveVariables.put(propKey, hiveVars2.getProperty(propKey));\n      }\n    } catch (ParseException e) {\n      System.err.println(e.getMessage());\n      printUsage();\n      return false;\n    }\n    return true;\n  ",
    "testFloatString": "\n    byte[] value = \"10.55\".getBytes(UTF_8);\n    byte[] encoded = new DoubleLexicoder().encode(10.55);\n\n    byte[] lex = AccumuloIndexLexicoder.encodeValue(value, serdeConstants.FLOAT_TYPE_NAME, true);\n    assertArrayEquals(lex, encoded);\n\n    lex = AccumuloIndexLexicoder.encodeValue(value, serdeConstants.DOUBLE_TYPE_NAME, true);\n    assertArrayEquals(lex, encoded);\n  ",
    "getFloatVar": "\n    return getFloatVar(this, var);\n  ",
    "checkTypeSafety": "\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY) ? null : NO_COMPARES_MSG;\n    ",
    "getStartTime": "\n    return startTimes.getOrDefault(method, 0L);\n  ",
    "getRowIdFactory": "\n    return rowIdFactory;\n  ",
    "incDeltaValue": "\n    if (delta == pool.length) return NO_DELTA; // The (pool-sized) list is being fully drained.\n    long result = delta + 1;\n    if (getArrayIndex(markerFrom, result) == getArrayIndex(otherMarker, 1)) {\n      return NO_DELTA; // The list is being drained, cannot increase the delta anymore.\n    }\n    return result;\n  ",
    "hasColumnEncoding": "\n    Preconditions.checkNotNull(columnMapping);\n\n    int offset = columnMapping.lastIndexOf(AccumuloHiveConstants.POUND);\n\n    // Make sure that the '#' wasn't escaped\n    if (0 < offset && AccumuloHiveConstants.ESCAPE == columnMapping.charAt(offset - 1)) {\n      // The encoding name/codes don't contain pound signs\n      return false;\n    }\n\n    return -1 != offset;\n  ",
    "populateParentPaths": "\n      return;\n    }\n    while(path != null) {\n      parents.add(path);\n      path = path.getParent();\n    }\n  ",
    "setZonedDateTime": "\n    this.zonedDateTime = zonedDateTime != null ? zonedDateTime : EPOCH;\n  ",
    "testManyRangesGeneratorOutput": "\n    // The AccumuloRangeGenerator produces an Object (due to the limitations of the\n    // traversal interface) which requires interpretation of that Object into Ranges.\n    // Changes in the return object from the AccumuloRangeGenerator must also represent\n    // a change in the AccumuloPredicateHandler.\n    AccumuloPredicateHandler mockHandler = Mockito.mock(AccumuloPredicateHandler.class);\n    ExprNodeDesc root = Mockito.mock(ExprNodeDesc.class);\n    String hiveRowIdColumnName = \"rid\";\n    Range r1 = new Range(\"a\"), r2 = new Range(\"z\");\n\n    Mockito.when(mockHandler.getRanges(conf, columnMapper)).thenCallRealMethod();\n    Mockito.when(mockHandler.generateRanges(conf, columnMapper, hiveRowIdColumnName, root))\n                 .thenReturn(Arrays.asList(r1, r2));\n    Mockito.when(mockHandler.getExpression(conf)).thenReturn(root);\n\n    // A null result from AccumuloRangeGenerator is all ranges\n    Assert.assertEquals(Arrays.asList(r1, r2), mockHandler.getRanges(conf, columnMapper));\n  ",
    "multiplyScaleDownTenDestructive": "\n    assert (tenScale >= 0);\n    if (this.fitsInt32() && right.fitsInt32()) {\n      multiplyDestructiveFitsInt32(right, (short) 0, tenScale);\n      return;\n    }\n    int[] z = multiplyArrays4And4To8(this.v, right.v);\n\n    // Then, scale back.\n    scaleDownTenArray8RoundUp(z, tenScale);\n    update(z[0], z[1], z[2], z[3]);\n  ",
    "preserveXAttr": "\n      dstFS.setXAttr(dst, attr.getKey(), attr.getValue());\n    }\n  ",
    "getCode": "\n    return code;\n  ",
    "setupCmdHistory": "\n    final String HISTORYFILE = \".hivehistory\";\n    String historyDirectory = System.getProperty(\"user.home\");\n    PersistentHistory history = null;\n    try {\n      if ((new File(historyDirectory)).exists()) {\n        String historyFile = historyDirectory + File.separator + HISTORYFILE;\n        history = new FileHistory(new File(historyFile));\n        reader.setHistory(history);\n      } else {\n        System.err.println(\"WARNING: Directory for Hive history file: \" + historyDirectory +\n                           \" does not exist.   History will not be available during this session.\");\n      }\n    } catch (Exception e) {\n      System.err.println(\"WARNING: Encountered an error while trying to initialize Hive's \" +\n                         \"history file.  History will not be available during this session.\");\n      System.err.println(e.getMessage());\n    }\n\n    // add shutdown hook to flush the history to history file\n    ShutdownHookManager.addShutdownHook(new Runnable() {\n      @Override\n      public void run() {\n        History h = reader.getHistory();\n        if (h instanceof FileHistory) {\n          try {\n            ((FileHistory) h).flush();\n          } catch (IOException e) {\n            System.err.println(\"WARNING: Failed to write command history file: \" + e.getMessage());\n          }\n        }\n      }\n    });\n  ",
    "addIndexCol": "\n    colMap.put(encode(cf, cq), colType);\n  ",
    "serializeTo128": "\n    assert (this.mag.getV3() >= 0);\n    buf.put(this.mag.getV0());\n    buf.put(this.mag.getV1());\n    buf.put(this.mag.getV2());\n    buf.put(this.mag.getV3()\n        | (this.negative ? SqlMathUtil.NEGATIVE_INT_MASK : 0));\n  ",
    "executeCmd": "\n    final Process p1 = Runtime.getRuntime().exec(cmdArr, null, dir);\n    new Thread(new Runnable() {\n      @Override\n      @SuppressFBWarnings(value = \"OS_OPEN_STREAM\", justification = \"Testing only\")\n      public void run() {\n        BufferedReader input = new BufferedReader(new InputStreamReader(p1.getErrorStream(), StandardCharsets.UTF_8));\n        String line;\n        try {\n          while ((line = input.readLine()) != null) {\n            System.out.println(line);\n          }\n        } catch (IOException e) {\n          LOG.error(\"Failed to execute the command due the exception \" + e);\n        }\n      }\n    }).start();\n    p1.waitFor();\n  ",
    "negate": "\n    result.update(left);\n    result.negateDestructive();\n  ",
    "findRegisteredDriver": "\n    for (Enumeration drivers = DriverManager.getDrivers(); drivers != null\n        && drivers.hasMoreElements();) {\n      Driver driver = (Driver) drivers.nextElement();\n      try {\n        if (driver.acceptsURL(url)) {\n          return driver;\n        }\n      } catch (Exception e) {\n      }\n    }\n    return null;\n  ",
    "testUseCurrentDB3": "\n    verifyCMD(\n        \"create database if not exists testDB; set hive.cli.print.current.db=true;use  testDB;\\n\"\n            + \"use default;drop if exists testDB;\", \"hive (testDB)>\", err, null, ERRNO_OTHER, true);\n  ",
    "getSQLStdAuthDefaultWhiteListPattern": "\n    // create the default white list from list of safe config params\n    // and regex list\n    String confVarPatternStr = Joiner.on(\"|\").join(convertVarsToRegex(SQL_STD_AUTH_SAFE_VAR_NAMES));\n    String regexPatternStr = Joiner.on(\"|\").join(sqlStdAuthSafeVarNameRegexes);\n    return regexPatternStr + \"|\" + confVarPatternStr;\n  ",
    "testSubtractDestructive": "\n    two.subtractDestructive(one);\n    assertEquals(1L, one.asLong());\n    assertEquals(1L, one.asLong());\n\n    try {\n      one.subtractDestructive(new UnsignedInt128(10L));\n      fail();\n    } catch (ArithmeticException ex) {\n      // ok\n    }\n\n    UnsignedInt128 big = new UnsignedInt128((1L << 62) + (3L << 34) + 3L);\n    big.shiftLeftDestructive(6);\n    UnsignedInt128 tmp = new UnsignedInt128((1L << 61) + 5L);\n    tmp.shiftLeftDestructive(6);\n\n    big.subtractDestructive(tmp);\n    big.subtractDestructive(tmp);\n\n    assertEquals((3 << 6) - 2 * (5 << 6), big.getV0());\n    assertEquals((3 << 8) - 1, big.getV1());\n    assertEquals(0, big.getV2());\n    assertEquals(0, big.getV3());\n  ",
    "setState": "\n    boolean result = state.compareAndSet(oldVal, newVal);\n    if (result && casLog != null) {\n      casLog.log(oldVal, newVal);\n    }\n    return result;\n  ",
    "deserializeWithTooFewHiveColumns": "\n    Properties properties = new Properties();\n    Configuration conf = new Configuration();\n    properties.setProperty(AccumuloSerDeParameters.COLUMN_MAPPINGS, \":rowID,cf:f1,cf:f2,cf:f3\");\n    properties.setProperty(serdeConstants.LIST_COLUMNS, \"row,col1,col2\");\n    properties.setProperty(serdeConstants.LIST_COLUMN_TYPES, \"string,string,string\");\n\n    serde.initialize(conf, properties, null);\n    serde.deserialize(new Text(\"fail\"));\n  ",
    "setDayOfMonth": "\n    localDate = localDate.withDayOfMonth(dayOfMonth);\n  ",
    "getpCompare": "\n    return pCompare;\n  ",
    "parseCharacterTemporal": "\n    try {\n      if (token.temporalField == ChronoField.MONTH_OF_YEAR) {\n        if (token.length == 3) {\n          return Month.from(MONTH_FORMATTER.parse(capitalize(substring))).getValue();\n        } else {\n          return Month.valueOf(substring.toUpperCase()).getValue();\n        }\n      } else if (token.temporalField == ChronoField.DAY_OF_WEEK) {\n        if (token.length == 3) {\n          return DayOfWeek.from(DAY_OF_WEEK_FORMATTER.parse(capitalize(substring))).getValue();\n        } else {\n          return DayOfWeek.valueOf(substring.toUpperCase()).getValue();\n        }\n      }\n    } catch (Exception e) {\n      throw new IllegalArgumentException(\n          \"Couldn't parse substring \\\"\" + substring + \"\\\" with token \" + token + \" to integer.\"\n              + \"Pattern is \" + pattern, e);\n    }\n    throw new IllegalArgumentException(\n        \"token: (\" + token + \") isn't a valid character temporal. Pattern is \" + pattern);\n  ",
    "equal": "\n    Equal equalObj = new Equal(strCompare);\n    byte[] val = \"aaa\".getBytes();\n    assertTrue(equalObj.accept(val));\n  ",
    "clear": "\n    if (interned != null) copyFromInternedToThis();\n    super.clear();\n  ",
    "testExtractOpNonJsonChildrenShouldThrow": "\n    String jsonString = \"{\\\"opName\\\":{\\\"children\\\":\\\"not-json\\\"}}\";\n    JSONObject operator = new JSONObject(jsonString);\n\n    Vertex uut = new Vertex(\"name\", null, null, tezJsonParser);\n\n    try {\n      uut.extractOp(operator, null);\n    } catch (Exception e) {\n      assertEquals(\"Unsupported operator name's children operator is neither a jsonobject nor a jsonarray\", e.getMessage());\n    }\n  ",
    "help": "\n    String[] parts = beeLine.split(line);\n    String cmd = parts.length > 1 ? parts[1] : \"\";\n    int count = 0;\n    TreeSet<ColorBuffer> clist = new TreeSet<ColorBuffer>();\n\n    for (int i = 0; i < beeLine.commandHandlers.length; i++) {\n      if (cmd.length() == 0 ||\n          Arrays.asList(beeLine.commandHandlers[i].getNames()).contains(cmd)) {\n        clist.add(beeLine.getColorBuffer().pad(\"!\" + beeLine.commandHandlers[i].getName(), 20)\n            .append(beeLine.wrap(beeLine.commandHandlers[i].getHelpText(), 60, 20)));\n      }\n    }\n\n    for (Iterator<ColorBuffer> i = clist.iterator(); i.hasNext();) {\n      beeLine.output(i.next());\n    }\n\n    if (cmd.length() == 0) {\n      beeLine.output(\"\");\n      beeLine.output(beeLine.loc(\"comments\", beeLine.getApplicationContactInformation()));\n    }\n\n    return true;\n  ",
    "testPrefixWithEscape": "\n    String mappingStr = \"cf:foo\\\\*bar*\";\n    ColumnMapping mapping = ColumnMappingFactory.get(mappingStr, ColumnEncoding.getDefault(),\n        \"col\", TypeInfoFactory.getMapTypeInfo(TypeInfoFactory.stringTypeInfo,\n            TypeInfoFactory.stringTypeInfo));\n\n    Assert.assertEquals(HiveAccumuloMapColumnMapping.class, mapping.getClass());\n    HiveAccumuloMapColumnMapping mapMapping = (HiveAccumuloMapColumnMapping) mapping;\n\n    Assert.assertEquals(\"cf\", mapMapping.getColumnFamily());\n    Assert.assertEquals(\"foo*bar\", mapMapping.getColumnQualifierPrefix());\n    Assert.assertEquals(ColumnEncoding.getDefault(), mapMapping.getKeyEncoding());\n    Assert.assertEquals(ColumnEncoding.getDefault(), mapMapping.getValueEncoding());\n  ",
    "update96": "\n    update96(buf.get(), buf.get(), buf.get());\n  ",
    "defaultBeelineConnect": "\n    String url;\n    try {\n      url = getDefaultConnectionUrl(cl);\n      if (url == null) {\n        debug(\"Default hs2 connection config file not found\");\n        return false;\n      }\n    } catch (BeelineConfFileParseException e) {\n      error(e);\n      return false;\n    }\n    return dispatch(\"!connect \" + url);\n  ",
    "testStripCodeWithEscapedPound": "\n    String mapping = \"foo:ba\\\\#r\";\n\n    Assert.assertEquals(\n        mapping,\n        ColumnEncoding.stripCode(mapping + AccumuloHiveConstants.POUND\n            + ColumnEncoding.BINARY.getCode()));\n  ",
    "testShutdownHook": "\n    ByteArrayOutputStream os = new ByteArrayOutputStream();\n    PrintStream ops = new PrintStream(os);\n    BeeLine beeline = new BeeLine();\n    DatabaseConnections dbConnections = beeline.getDatabaseConnections();\n    dbConnections.setConnection(new DatabaseConnection(beeline,null,null, null));\n    dbConnections.setConnection(new DatabaseConnection(beeline,null,null, null));\n    Assert.assertEquals(2, dbConnections.size());\n    beeline.setOutputStream(ops);\n    beeline.getShutdownHook().run();\n    Assert.assertEquals(0, dbConnections.size());\n  ",
    "testRangeDisjunction": "\n    // rowId >= 'f'\n    ExprNodeDesc column = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null, false);\n    ExprNodeDesc constant = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"f\");\n    List<ExprNodeDesc> children = Lists.newArrayList();\n    children.add(column);\n    children.add(constant);\n    ExprNodeDesc node = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrGreaterThan(), children);\n    assertNotNull(node);\n\n    // rowId <= 'm'\n    ExprNodeDesc column2 = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null,\n        false);\n    ExprNodeDesc constant2 = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"m\");\n    List<ExprNodeDesc> children2 = Lists.newArrayList();\n    children2.add(column2);\n    children2.add(constant2);\n    ExprNodeDesc node2 = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrLessThan(), children2);\n    assertNotNull(node2);\n\n    // Or UDF\n    List<ExprNodeDesc> bothFilters = Lists.newArrayList();\n    bothFilters.add(node);\n    bothFilters.add(node2);\n    ExprNodeGenericFuncDesc both = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPOr(), bothFilters);\n\n    // Should generate (-inf,+inf)\n    List<Range> expectedRanges = Arrays.asList(new Range());\n\n    AccumuloRangeGenerator rangeGenerator = new AccumuloRangeGenerator(conf, handler, rowIdMapping, \"rid\");\n    SemanticDispatcher disp = new DefaultRuleDispatcher(rangeGenerator,\n        Collections.<SemanticRule, SemanticNodeProcessor> emptyMap(), null);\n    SemanticGraphWalker ogw = new DefaultGraphWalker(disp);\n    ArrayList<Node> topNodes = new ArrayList<Node>();\n    topNodes.add(both);\n    HashMap<Node,Object> nodeOutput = new HashMap<Node,Object>();\n\n    try {\n      ogw.startWalking(topNodes, nodeOutput);\n    } catch (SemanticException ex) {\n      throw new RuntimeException(ex);\n    }\n\n    Object result = nodeOutput.get(both);\n    Assert.assertNotNull(result);\n    Assert.assertTrue(\"Result from graph walk was not a List\", result instanceof List);\n    @SuppressWarnings(\"unchecked\")\n    List<Range> actualRanges = (List<Range>) result;\n    Assert.assertEquals(expectedRanges, actualRanges);\n  ",
    "mergeTokenIntoJobConf": "\n    JobConf accumuloJobConf = new JobConf(jobConf);\n    accumuloJobConf.getCredentials().addToken(accumuloToken.getService(), accumuloToken);\n\n    // Merge them together.\n    ShimLoader.getHadoopShims().mergeCredentials(jobConf, accumuloJobConf);\n  ",
    "isolation": "\n    if (!(beeLine.assertConnection())) {\n      return false;\n    }\n\n    int i;\n\n    if (line.endsWith(\"TRANSACTION_NONE\")) {\n      i = Connection.TRANSACTION_NONE;\n    } else if (line.endsWith(\"TRANSACTION_READ_COMMITTED\")) {\n      i = Connection.TRANSACTION_READ_COMMITTED;\n    } else if (line.endsWith(\"TRANSACTION_READ_UNCOMMITTED\")) {\n      i = Connection.TRANSACTION_READ_UNCOMMITTED;\n    } else if (line.endsWith(\"TRANSACTION_REPEATABLE_READ\")) {\n      i = Connection.TRANSACTION_REPEATABLE_READ;\n    } else if (line.endsWith(\"TRANSACTION_SERIALIZABLE\")) {\n      i = Connection.TRANSACTION_SERIALIZABLE;\n    } else {\n      return beeLine.error(\"Usage: isolation <TRANSACTION_NONE \"\n          + \"| TRANSACTION_READ_COMMITTED \"\n          + \"| TRANSACTION_READ_UNCOMMITTED \"\n          + \"| TRANSACTION_REPEATABLE_READ \"\n          + \"| TRANSACTION_SERIALIZABLE>\");\n    }\n\n    beeLine.getDatabaseConnection().getConnection().setTransactionIsolation(i);\n\n    int isol = beeLine.getDatabaseConnection().getConnection().getTransactionIsolation();\n    final String isoldesc;\n    switch (i)\n    {\n    case Connection.TRANSACTION_NONE:\n      isoldesc = \"TRANSACTION_NONE\";\n      break;\n    case Connection.TRANSACTION_READ_COMMITTED:\n      isoldesc = \"TRANSACTION_READ_COMMITTED\";\n      break;\n    case Connection.TRANSACTION_READ_UNCOMMITTED:\n      isoldesc = \"TRANSACTION_READ_UNCOMMITTED\";\n      break;\n    case Connection.TRANSACTION_REPEATABLE_READ:\n      isoldesc = \"TRANSACTION_REPEATABLE_READ\";\n      break;\n    case Connection.TRANSACTION_SERIALIZABLE:\n      isoldesc = \"TRANSACTION_SERIALIZABLE\";\n      break;\n    default:\n      isoldesc = \"UNKNOWN\";\n    }\n\n    beeLine.info(beeLine.loc(\"isolation-status\", isoldesc));\n    return true;\n  ",
    "getTableVisibilityLabel": "\n    String visibilityLabel = tableProperties.getProperty(VISIBILITY_LABEL_KEY, null);\n    if (null == visibilityLabel || visibilityLabel.isEmpty()) {\n      return DEFAULT_VISIBILITY_LABEL;\n    }\n\n    return new ColumnVisibility(visibilityLabel);\n  ",
    "getCount": "\n    return count;\n  ",
    "saveDir": "\n    String dir = System.getProperty(\"beeline.rcfile\");\n    if (dir != null && dir.length() > 0) {\n      return new File(dir);\n    }\n\n    File f = new File(System.getProperty(\"user.home\"),\n        (System.getProperty(\"os.name\").toLowerCase()\n            .indexOf(\"windows\") != -1 ? \"\" : \".\") + \"beeline\")\n        .getAbsoluteFile();\n    try {\n      f.mkdirs();\n    } catch (Exception e) {\n    }\n    return f;\n  ",
    "getOpts": "\n    return opts;\n  ",
    "getFirstCmd": "\n    return cmd.substring(length).trim();\n  ",
    "isPurge": "\n    return !MetaStoreUtils.isExternalTable(table) || MetaStoreUtils.isExternalTablePurge(table);\n  ",
    "testMissingPassword": "\n    Configuration conf = new Configuration(false);\n    conf.set(AccumuloConnectionParameters.INSTANCE_NAME, \"accumulo\");\n    conf.set(AccumuloConnectionParameters.ZOOKEEPERS, \"localhost:2181\");\n    conf.set(AccumuloConnectionParameters.USER_NAME, \"user\");\n\n    Instance instance = Mockito.mock(Instance.class);\n\n    AccumuloConnectionParameters cnxnParams = new AccumuloConnectionParameters(conf);\n\n    // Provide an instance of the code doesn't try to make a real Instance\n    // We just want to test that we fail before trying to make a connector\n    // with null password\n    cnxnParams.getConnector(instance);\n  ",
    "toInstant": "\n    return zonedDateTime.toInstant();\n  ",
    "isNumericTemporalToken": "\n    return NUMERIC_TEMPORAL_TOKENS.containsKey(candidate);\n  ",
    "getStatus": "\n      return status;\n    }\n  }\n",
    "areOptimizationsEnabled": "\n        return conf.getBoolean(\n                HiveConf.ConfVars.HIVE_BLOBSTORE_OPTIMIZATIONS_ENABLED.varname,\n                HiveConf.ConfVars.HIVE_BLOBSTORE_OPTIMIZATIONS_ENABLED.defaultBoolVal\n        );\n    ",
    "setHost": "\n      this.host = host;\n      return this;\n    ",
    "setLastConnectedUrl": "\n    this.lastConnectedUrl = lastConnectedUrl;\n  ",
    "findSQLState": "\n    ErrorMsg error = getErrorMsg(mesg);\n    return error.getSQLState();\n  ",
    "checkNoPartitionFilter": "\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_NO_PARTITION_FILTER)\n          ? null : NO_PARTITIONLESS_MSG;\n    ",
    "verifyHighPrecisionSubtractSingle": "\n\n    Decimal128 a, b, r;\n    String sA, sB;\n\n    a = new Decimal128();\n    sA = makeNumericString(37);\n    a.update(sA, (short) 0);\n    b = new Decimal128();\n    sB = makeNumericString(37);\n    b.update(sB, (short) 0);\n\n    r = new Decimal128();\n    r.addDestructive(a, (short) 0);\n    r.subtractDestructive(b, (short) 0);\n\n    String res1 = r.toFormalString();\n\n    // Now do the add with Java BigDecimal\n    BigDecimal bdA = new BigDecimal(sA);\n    BigDecimal bdB = new BigDecimal(sB);\n    BigDecimal bdR = bdA.subtract(bdB);\n\n    String res2 = bdR.toPlainString();\n\n    // Compare the results\n    String message = \"For operation \" + a.toFormalString() + \" - \" + b.toFormalString();\n    assertEquals(message, res2, res1);\n  ",
    "setMaxWidth": "\n    this.maxWidth = maxWidth;\n  ",
    "map": "\n    Map<Object, Object> m = new LinkedHashMap<Object, Object>();\n    for (int i = 0; i < obs.length - 1; i += 2) {\n      m.put(obs[i], obs[i + 1]);\n    }\n    return Collections.unmodifiableMap(m);\n  ",
    "testConfigureAccumuloInputFormatWithAuthorizations": "\n    AccumuloConnectionParameters accumuloParams = new AccumuloConnectionParameters(conf);\n    conf.set(AccumuloSerDeParameters.AUTHORIZATIONS_KEY, \"foo,bar\");\n    ColumnMapper columnMapper = new ColumnMapper(conf.get(AccumuloSerDeParameters.COLUMN_MAPPINGS),\n        conf.get(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE), columnNames, columnTypes);\n    Set<Pair<Text,Text>> cfCqPairs = inputformat\n        .getPairCollection(columnMapper.getColumnMappings());\n    List<IteratorSetting> iterators = Collections.emptyList();\n    Set<Range> ranges = Collections.singleton(new Range());\n    String instanceName = \"realInstance\";\n    String zookeepers = \"host1:2181,host2:2181,host3:2181\";\n\n    ZooKeeperInstance zkInstance = Mockito.mock(ZooKeeperInstance.class);\n    HiveAccumuloTableInputFormat mockInputFormat = Mockito.mock(HiveAccumuloTableInputFormat.class);\n    HiveAccumuloHelper helper = Mockito.mock(HiveAccumuloHelper.class);\n\n    // Stub out the ZKI mock\n    Mockito.when(zkInstance.getInstanceName()).thenReturn(instanceName);\n    Mockito.when(zkInstance.getZooKeepers()).thenReturn(zookeepers);\n    // Stub out a mocked Helper instance\n    Mockito.when(mockInputFormat.getHelper()).thenReturn(helper);\n\n    // Call out to the real configure method\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .configure(conf, zkInstance, con, accumuloParams, columnMapper, iterators, ranges);\n\n    // Also compute the correct cf:cq pairs so we can assert the right argument was passed\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .getPairCollection(columnMapper.getColumnMappings());\n\n    mockInputFormat.configure(conf, zkInstance, con, accumuloParams, columnMapper, iterators,\n        ranges);\n\n    // Verify that the correct methods are invoked on AccumuloInputFormat\n    Mockito.verify(helper).setInputFormatZooKeeperInstance(conf, instanceName, zookeepers, false);\n    Mockito.verify(helper).setInputFormatConnectorInfo(conf, USER, new PasswordToken(PASS));\n    Mockito.verify(mockInputFormat).setInputTableName(conf, TEST_TABLE);\n    Mockito.verify(mockInputFormat).setScanAuthorizations(conf, new Authorizations(\"foo,bar\"));\n    Mockito.verify(mockInputFormat).addIterators(conf, iterators);\n    Mockito.verify(mockInputFormat).setRanges(conf, ranges);\n    Mockito.verify(mockInputFormat).fetchColumns(conf, cfCqPairs);\n  ",
    "processLocalCmd": "\n    boolean escapeCRLF = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_ESCAPE_CRLF);\n    CommandProcessorResponse response = new CommandProcessorResponse();\n\n    if (proc != null) {\n      if (proc instanceof IDriver) {\n        IDriver qp = (IDriver) proc;\n        PrintStream out = ss.out;\n        long start = System.currentTimeMillis();\n        if (ss.getIsVerbose()) {\n          out.println(cmd);\n        }\n\n        // Set HDFS CallerContext to queryId and reset back to sessionId after the query is done\n        ShimLoader.getHadoopShims()\n            .setHadoopQueryContext(String.format(USER_ID, qp.getQueryState().getQueryId(), ss.getUserName()));\n        try {\n          response = qp.run(cmd);\n        } catch (CommandProcessorException e) {\n          qp.close();\n          ShimLoader.getHadoopShims()\n              .setHadoopSessionContext(String.format(USER_ID, ss.getSessionId(), ss.getUserName()));\n          throw e;\n        }\n\n        // query has run capture the time\n        long end = System.currentTimeMillis();\n        double timeTaken = (end - start) / 1000.0;\n\n        ArrayList<String> res = new ArrayList<String>();\n\n        printHeader(qp, out);\n\n        // print the results\n        int counter = 0;\n        try {\n          if (out instanceof FetchConverter) {\n            ((FetchConverter) out).fetchStarted();\n          }\n          while (qp.getResults(res)) {\n            for (String r : res) {\n                  if (escapeCRLF) {\n                    r = EscapeCRLFHelper.escapeCRLF(r);\n                  }\n              out.println(r);\n            }\n            counter += res.size();\n            res.clear();\n            if (out.checkError()) {\n              break;\n            }\n          }\n        } catch (IOException e) {\n          console.printError(\"Failed with exception \" + e.getClass().getName() + \":\" + e.getMessage(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          throw new CommandProcessorException(1);\n        } finally {\n          qp.close();\n          ShimLoader.getHadoopShims()\n              .setHadoopSessionContext(String.format(USER_ID, ss.getSessionId(), ss.getUserName()));\n\n          if (out instanceof FetchConverter) {\n            ((FetchConverter) out).fetchFinished();\n          }\n\n          console.printInfo(\n              \"Time taken: \" + timeTaken + \" seconds\" + (counter == 0 ? \"\" : \", Fetched: \" + counter + \" row(s)\"));\n        }\n      } else {\n        String firstToken = tokenizeCmd(cmd.trim())[0];\n        String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length());\n\n        if (ss.getIsVerbose()) {\n          ss.out.println(firstToken + \" \" + cmd_1);\n        }\n\n        try {\n          CommandProcessorResponse res = proc.run(cmd_1);\n          if (res.getMessage() != null) {\n            console.printInfo(res.getMessage());\n          }\n          return res;\n        } catch (CommandProcessorException e) {\n          ss.out.println(\"Query returned non-zero code: \" + e.getResponseCode() + \", cause: \" + e.getMessage());\n          throw e;\n        }\n      }\n    }\n    return response;\n  ",
    "stringFor": "\n    switch (timeunit) {\n      case DAYS: return \"day\";\n      case HOURS: return \"hour\";\n      case MINUTES: return \"min\";\n      case SECONDS: return \"sec\";\n      case MILLISECONDS: return \"msec\";\n      case MICROSECONDS: return \"usec\";\n      case NANOSECONDS: return \"nsec\";\n    }\n    throw new IllegalArgumentException(\"Invalid timeunit \" + timeunit);\n  ",
    "testToString": "\n    assertEquals(\"0-0\", HiveIntervalYearMonth.valueOf(\"0-0\").toString());\n    assertEquals(\"1-2\", HiveIntervalYearMonth.valueOf(\"1-2\").toString());\n    assertEquals(\"-1-2\", HiveIntervalYearMonth.valueOf(\"-1-2\").toString());\n    assertEquals(\"1-0\", HiveIntervalYearMonth.valueOf(\"1-0\").toString());\n    assertEquals(\"-1-0\", HiveIntervalYearMonth.valueOf(\"-1-0\").toString());\n    assertEquals(\"0-0\", HiveIntervalYearMonth.valueOf(\"-0-0\").toString());\n  ",
    "createStatement": "\n    Statement stmnt = getDatabaseConnection().getConnection().createStatement();\n    if (getOpts().timeout > -1) {\n      stmnt.setQueryTimeout(getOpts().timeout);\n    }\n    if (signalHandler != null) {\n      signalHandler.setStatement(stmnt);\n    }\n\n    // If no fetch size is specified in beeline, let the driver figure it out\n    final int fetchSize = getOpts().getFetchSize();\n    if (fetchSize >= 0) {\n      stmnt.setFetchSize(fetchSize);\n    }\n\n    return stmnt;\n  ",
    "setValues": "\n      this.tsNanos = tsNanos;\n      this.gcPauseNanos = gcPauseNanos;\n    }\n  ",
    "testBooleanString": "\n    byte[] value = Boolean.TRUE.toString().getBytes(UTF_8);\n    assertArrayEquals(AccumuloIndexLexicoder.encodeValue(value, serdeConstants.BOOLEAN_TYPE_NAME,\n        true), value);\n  ",
    "multiplyArrays4And4To8": "\n    assert (left.length == 4);\n    assert (right.length == 4);\n    long product;\n\n    // this method could go beyond the integer ranges until we scale back\n    // so, we need twice more variables.\n    int[] z = new int[8];\n\n    product = (right[0] & SqlMathUtil.LONG_MASK)\n        * (left[0] & SqlMathUtil.LONG_MASK);\n    z[0] = (int) product;\n\n    product = (right[0] & SqlMathUtil.LONG_MASK)\n        * (left[1] & SqlMathUtil.LONG_MASK)\n        + (right[1] & SqlMathUtil.LONG_MASK)\n        * (left[0] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[1] = (int) product;\n\n    product = (right[0] & SqlMathUtil.LONG_MASK)\n        * (left[2] & SqlMathUtil.LONG_MASK)\n        + (right[1] & SqlMathUtil.LONG_MASK)\n        * (left[1] & SqlMathUtil.LONG_MASK)\n        + (right[2] & SqlMathUtil.LONG_MASK)\n        * (left[0] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[2] = (int) product;\n\n    product = (right[0] & SqlMathUtil.LONG_MASK)\n        * (left[3] & SqlMathUtil.LONG_MASK)\n        + (right[1] & SqlMathUtil.LONG_MASK)\n        * (left[2] & SqlMathUtil.LONG_MASK)\n        + (right[2] & SqlMathUtil.LONG_MASK)\n        * (left[1] & SqlMathUtil.LONG_MASK)\n        + (right[3] & SqlMathUtil.LONG_MASK)\n        * (left[0] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[3] = (int) product;\n\n    product = (right[1] & SqlMathUtil.LONG_MASK)\n        * (left[3] & SqlMathUtil.LONG_MASK)\n        + (right[2] & SqlMathUtil.LONG_MASK)\n        * (left[2] & SqlMathUtil.LONG_MASK)\n        + (right[3] & SqlMathUtil.LONG_MASK)\n        * (left[1] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[4] = (int) product;\n\n    product = (right[2] & SqlMathUtil.LONG_MASK)\n        * (left[3] & SqlMathUtil.LONG_MASK)\n        + (right[3] & SqlMathUtil.LONG_MASK)\n        * (left[2] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[5] = (int) product;\n\n    // v[1], v[0]\n    product = (right[3] & SqlMathUtil.LONG_MASK)\n        * (left[3] & SqlMathUtil.LONG_MASK) + (product >>> 32);\n    z[6] = (int) product;\n    z[7] = (int) (product >>> 32);\n\n    return z;\n  ",
    "updateCount": "\n    if (v[3] != 0) {\n      this.count = (byte) 4;\n    } else if (v[2] != 0) {\n      this.count = (byte) 3;\n    } else if (v[1] != 0) {\n      this.count = (byte) 2;\n    } else if (v[0] != 0) {\n      this.count = (byte) 1;\n    } else {\n      this.count = (byte) 0;\n    }\n  }\n  \n   /*(non-Javadoc)\n   * Serializes one int part into the given @{link #ByteBuffer",
    "setTimeInMillis": "\n    localDateTime = LocalDateTime\n        .ofInstant(Instant.ofEpochMilli(epochMilli), ZoneOffset.UTC)\n        .withNano(nanos);\n  ",
    "testConvertToUTC": "\n    String s = \"2017-04-14 18:00:00 Asia/Shanghai\";\n    TimestampTZ timestampTZ = TimestampTZUtil.parse(s, ZoneId.of(\"UTC\"));\n    Assert.assertEquals(\"2017-04-14 10:00:00.0 UTC\", timestampTZ.toString());\n  ",
    "getAuthorizationsFromValue": "\n    if (null == authorizationStr) {\n      return null;\n    }\n\n    return new Authorizations(authorizationStr);\n  ",
    "copy": "\n    boolean gotException = false;\n    boolean returnVal = true;\n    StringBuilder exceptions = new StringBuilder();\n    if (srcs.length == 1) {\n      return doIOUtilsCopyBytes(srcFS, srcFS.getFileStatus(srcs[0]), dstFS, dst, deleteSource, overwrite, preserveXAttr, conf, copyStatistics);\n    } else {\n      try {\n        FileStatus sdst = dstFS.getFileStatus(dst);\n        if (!sdst.isDirectory()) {\n          throw new IOException(\"copying multiple files, but last argument `\" + dst + \"' is not a directory\");\n        }\n      } catch (FileNotFoundException var16) {\n        throw new IOException(\"`\" + dst + \"': specified destination directory does not exist\", var16);\n      }\n\n      Path[] var17 = srcs;\n      int var11 = srcs.length;\n\n      for(int var12 = 0; var12 < var11; ++var12) {\n        Path src = var17[var12];\n\n        try {\n          if (!doIOUtilsCopyBytes(srcFS, srcFS.getFileStatus(src), dstFS, dst, deleteSource, overwrite, preserveXAttr, conf, copyStatistics)) {\n            returnVal = false;\n          }\n        } catch (IOException var15) {\n          gotException = true;\n          exceptions.append(var15.getMessage());\n          exceptions.append(\"\\n\");\n        }\n      }\n\n      if (gotException) {\n        throw new IOException(exceptions.toString());\n      } else {\n        return returnVal;\n      }\n    }\n  ",
    "rollback": "\n    if (!(beeLine.assertConnection())) {\n      return false;\n    }\n    if (!(beeLine.assertAutoCommit())) {\n      return false;\n    }\n    try {\n      long start = System.currentTimeMillis();\n      beeLine.getDatabaseConnection().getConnection().rollback();\n      long end = System.currentTimeMillis();\n      beeLine.showWarnings();\n      beeLine.info(beeLine.loc(\"rollback-complete\")\n          + \" \" + beeLine.locElapsedTime(end - start));\n      return true;\n    } catch (Exception e) {\n      return beeLine.error(e);\n    }\n  ",
    "applySystemProperties": "\n    Map<String, String> systemProperties = getConfSystemProperties();\n    for (Entry<String, String> systemProperty : systemProperties.entrySet()) {\n      this.set(systemProperty.getKey(), systemProperty.getValue());\n    }\n  ",
    "testCopyWithDistCpAs": "\n    Path copySrc = new Path(\"copySrc\");\n    Path copyDst = new Path(\"copyDst\");\n    HiveConf conf = new HiveConf(TestFileUtils.class);\n\n    FileSystem fs = copySrc.getFileSystem(conf);\n\n    String doAsUser = conf.getVar(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER);\n    UserGroupInformation proxyUser = UserGroupInformation.createProxyUser(\n            doAsUser, UserGroupInformation.getLoginUser());\n\n    HadoopShims shims = mock(HadoopShims.class);\n    when(shims.runDistCpAs(Collections.singletonList(copySrc), copyDst, conf, proxyUser)).thenReturn(true);\n    when(shims.runDistCp(Collections.singletonList(copySrc), copyDst, conf)).thenReturn(false);\n\n    // doAs when asked\n    Assert.assertTrue(FileUtils.distCp(fs, Collections.singletonList(copySrc), copyDst, false, proxyUser, conf, shims));\n    verify(shims).runDistCpAs(Collections.singletonList(copySrc), copyDst, conf, proxyUser);\n    // don't doAs when not asked\n    Assert.assertFalse(FileUtils.distCp(fs, Collections.singletonList(copySrc), copyDst, true, null, conf, shims));\n    verify(shims).runDistCp(Collections.singletonList(copySrc), copyDst, conf);\n\n    // When distcp is done with doAs, the delete should also be done as doAs. But in current code its broken. This\n    // should be fixed. For now check is added to avoid wrong usage. So if doAs is set, delete source should be false.\n    try {\n      FileUtils.distCp(fs, Collections.singletonList(copySrc), copyDst, true, proxyUser, conf, shims);\n      Assert.assertTrue(\"Should throw IOException as doAs is called with delete source set to true\".equals(\"\"));\n    } catch (IOException e) {\n      Assert.assertTrue(e.getMessage().\n              equalsIgnoreCase(\"Distcp is called with doAsUser and delete source set as true\"));\n    }\n  ",
    "testRowRangeIntersection": "\n    // rowId >= 'f'\n    ExprNodeDesc column = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null, false);\n    ExprNodeDesc constant = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"f\");\n    List<ExprNodeDesc> children = Lists.newArrayList();\n    children.add(column);\n    children.add(constant);\n    ExprNodeDesc node = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrGreaterThan(), children);\n    assertNotNull(node);\n\n    // rowId <= 'm'\n    ExprNodeDesc column2 = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null,\n        false);\n    ExprNodeDesc constant2 = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"m\");\n    List<ExprNodeDesc> children2 = Lists.newArrayList();\n    children2.add(column2);\n    children2.add(constant2);\n    ExprNodeDesc node2 = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrLessThan(), children2);\n    assertNotNull(node2);\n\n    List<ExprNodeDesc> bothFilters = Lists.newArrayList();\n    bothFilters.add(node);\n    bothFilters.add(node2);\n    ExprNodeGenericFuncDesc both = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPAnd(), bothFilters);\n\n    String filterExpr = SerializationUtilities.serializeExpression(both);\n    conf.set(TableScanDesc.FILTER_EXPR_CONF_STR, filterExpr);\n\n    // Should make ['f', 'm\\0')\n    List<Range> ranges = handler.getRanges(conf, columnMapper);\n    assertEquals(1, ranges.size());\n    assertEquals(new Range(new Key(\"f\"), true, new Key(\"m\\0\"), false), ranges.get(0));\n  ",
    "checkForInvalidIsoWeek": "\n    if (iyyy == 0) {\n      return;\n    }\n\n    LocalDateTime ldt = LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC);\n    try {\n      ldt = ldt.with(IsoFields.WEEK_BASED_YEAR, iyyy);\n      ldt = ldt.with(IsoFields.WEEK_OF_WEEK_BASED_YEAR, iw);\n    } catch (DateTimeException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (ldt.getYear() != iyyy) {\n      throw new IllegalArgumentException(\"ISO year \" + iyyy + \" does not have \" + iw + \" weeks.\");\n    }\n  ",
    "setCount": "\n    this.count = count;\n  ",
    "testRandomMultiplyDivideInverse": "\n    final int N = 100000;\n    final long MASK56 = 0x00FFFFFFFFFFFFL; // 56 bit mask to generate positive 56 bit longs\n                                           // from random signed longs\n    int seed = 897089790;\n    Random rand = new Random(seed);\n    long l1, l2;\n    for (int i = 1; i <= N; i++) {\n      l1 = rand.nextLong() & MASK56;\n      l2 = rand.nextLong() & MASK56;\n      verifyMultiplyDivideInverse(l1, l2);\n    }\n  ",
    "getJoinRSOp": "\n    if (outputOps.size() == 0) {\n      return null;\n    } else if (outputOps.size() == 1) {\n      if (outputOps.get(0).type == OpType.RS) {\n        return outputOps.get(0);\n      } else {\n        return null;\n      }\n    } else {\n      for (Op op : outputOps) {\n        if (op.type == OpType.RS) {\n          if (op.outputVertexName.equals(joinVertex.name)) {\n            return op;\n          }\n        }\n      }\n      return null;\n    }\n  ",
    "getTrimmedVar": "\n    assert (var.valClass == String.class) : var.varname;\n    if (var.altName != null) {\n      return conf.getTrimmed(var.varname, conf.getTrimmed(var.altName, var.defaultStrVal));\n    }\n    return conf.getTrimmed(var.varname, var.defaultStrVal);\n  ",
    "testHiveLogging": "\n    // customized log4j config log file to be: /${test.tmp.dir}/TestHiveLogging/hiveLog4jTest.log\n    File customLogPath = new File(new File(System.getProperty(\"test.tmp.dir\")),\n        System.getProperty(\"user.name\") + \"-TestHiveLogging/\");\n    String customLogName = \"hiveLog4j2Test.log\";\n    File customLogFile = new File(customLogPath, customLogName);\n    RunTest(customLogFile,\n      \"hive-log4j2-test.properties\", \"hive-exec-log4j2-test.properties\");\n  ",
    "setResponseHeader": "\n    response.setHeader(ACCESS_CONTROL_ALLOW_METHODS, ALLOWED_METHODS);\n    response.setHeader(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n    response.setContentType(CONTENT_TYPE_TEXT);\n  ",
    "testDegreesAndMillis": "\n    Connector con = mockInstance.getConnector(USER, new PasswordToken(PASS.getBytes()));\n    Scanner scan = con.createScanner(TEST_TABLE, new Authorizations(\"blah\"));\n    IteratorSetting is = new IteratorSetting(1, PrimitiveComparisonFilter.FILTER_PREFIX + 1,\n        PrimitiveComparisonFilter.class);\n\n    is.addOption(PrimitiveComparisonFilter.P_COMPARE_CLASS, DoubleCompare.class.getName());\n    is.addOption(PrimitiveComparisonFilter.COMPARE_OPT_CLASS, GreaterThanOrEqual.class.getName());\n    is.addOption(PrimitiveComparisonFilter.CONST_VAL, Base64.getEncoder().encodeToString(parseDoubleBytes(\"55.6\")));\n    is.addOption(PrimitiveComparisonFilter.COLUMN, \"cf:dgrs\");\n    scan.addScanIterator(is);\n\n    IteratorSetting is2 = new IteratorSetting(2, PrimitiveComparisonFilter.FILTER_PREFIX + 2,\n        PrimitiveComparisonFilter.class);\n\n    is2.addOption(PrimitiveComparisonFilter.P_COMPARE_CLASS, LongCompare.class.getName());\n    is2.addOption(PrimitiveComparisonFilter.COMPARE_OPT_CLASS, LessThan.class.getName());\n    is2.addOption(PrimitiveComparisonFilter.CONST_VAL, Base64.getEncoder().encodeToString(parseLongBytes(\"778\")));\n    is2.addOption(PrimitiveComparisonFilter.COLUMN, \"cf:mills\");\n\n    scan.addScanIterator(is2);\n\n    boolean foundDennis = false;\n    int totalCount = 0;\n    for (Map.Entry<Key,Value> kv : scan) {\n      boolean foundName = false;\n      boolean foundSid = false;\n      boolean foundDegrees = false;\n      boolean foundMillis = false;\n      SortedMap<Key,Value> items = PrimitiveComparisonFilter.decodeRow(kv.getKey(), kv.getValue());\n      for (Map.Entry<Key,Value> item : items.entrySet()) {\n        SortedMap<Key,Value> nestedItems = PrimitiveComparisonFilter.decodeRow(item.getKey(),\n            item.getValue());\n        for (Map.Entry<Key,Value> nested : nestedItems.entrySet()) {\n          if (nested.getKey().getRow().toString().equals(\"r3\")) {\n            foundDennis = true;\n          }\n          if (nested.getKey().getColumnQualifier().equals(NAME)) {\n            foundName = true;\n          } else if (nested.getKey().getColumnQualifier().equals(SID)) {\n            foundSid = true;\n          } else if (nested.getKey().getColumnQualifier().equals(DEGREES)) {\n            foundDegrees = true;\n          } else if (nested.getKey().getColumnQualifier().equals(MILLIS)) {\n            foundMillis = true;\n          }\n        }\n      }\n      totalCount++;\n      assertTrue(foundDegrees & foundMillis & foundName & foundSid);\n    }\n    assertTrue(foundDennis);\n    assertEquals(totalCount, 1);\n  ",
    "shutdown": "\n    shouldRun = false;\n  ",
    "doTestSerializationUtilsRead": "\n\n    // System.out.println(\"TEST_SERIALIZATION_UTILS_READ bigInteger \" + bigInteger);\n\n    HiveDecimalV1 oldDec = HiveDecimalV1.create(bigInteger);\n    if (oldDec != null && isTenPowerBug(oldDec.toString())) {\n      return;\n    }\n    HiveDecimal dec = HiveDecimal.create(bigInteger);\n    if (oldDec == null) {\n      assertTrue(dec == null);\n      return;\n    }\n    assertTrue(dec != null);\n    dec.validate();\n    // System.out.println(\"TEST_SERIALIZATION_UTILS_READ oldDec \" + oldDec);\n    // System.out.println(\"TEST_SERIALIZATION_UTILS_READ dec \" + dec);\n\n    Assert.assertEquals(bigInteger, oldDec.unscaledValue());\n    Assert.assertEquals(bigInteger, dec.unscaledValue());\n\n    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\n    SerializationUtils.writeBigInteger(outputStream, bigInteger);\n    byte[] bytes = outputStream.toByteArray();\n\n    ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);\n    BigInteger deserializedBigInteger =\n        SerializationUtils.readBigInteger(byteArrayInputStream);\n\n    // Verify SerializationUtils first.\n    Assert.assertEquals(bigInteger, deserializedBigInteger);\n\n    // Now HiveDecimal\n    byte[] scratchBytes = new byte[HiveDecimal.SCRATCH_BUFFER_LEN_SERIALIZATION_UTILS_READ];\n\n    byteArrayInputStream = new ByteArrayInputStream(bytes);\n    HiveDecimal resultDec =\n        dec.serializationUtilsRead(\n            byteArrayInputStream, dec.scale(),\n            scratchBytes);\n    assertTrue(resultDec != null);\n    resultDec.validate();\n\n    Assert.assertEquals(dec.toString(), resultDec.toString());\n\n    //----------------------------------------------------------------------------------------------\n\n    // Add scale.\n\n    int scale = 0 + r.nextInt(38 + 1);\n    BigDecimal bigDecimal = new BigDecimal(bigInteger, scale);\n\n    oldDec = HiveDecimalV1.create(bigDecimal);\n    dec = HiveDecimal.create(bigDecimal);\n    if (oldDec == null) {\n      assertTrue(dec == null);\n      return;\n    }\n    assertTrue(dec != null);\n    dec.validate();\n    // System.out.println(\"TEST_SERIALIZATION_UTILS_READ with scale oldDec \" + oldDec);\n    // System.out.println(\"TEST_SERIALIZATION_UTILS_READ with scale dec \" + dec);\n\n    outputStream = new ByteArrayOutputStream();\n    SerializationUtils.writeBigInteger(outputStream, dec.unscaledValue());\n    bytes = outputStream.toByteArray();\n\n    // Now HiveDecimal\n    byteArrayInputStream = new ByteArrayInputStream(bytes);\n    resultDec =\n        dec.serializationUtilsRead(\n            byteArrayInputStream, dec.scale(),\n            scratchBytes);\n    assertTrue(resultDec != null);\n    resultDec.validate();\n\n    Assert.assertEquals(dec.toString(), resultDec.toString());\n  ",
    "updateFixedPoint": "\n    this.scale = scale;\n    if (val < 0L) {\n      this.unscaledValue.update(-val);\n      this.signum = -1;\n    } else if (val == 0L) {\n      zeroClear();\n    } else {\n      this.unscaledValue.update(val);\n      this.signum = 1;\n    }\n  ",
    "getFromMapping": "\n    Preconditions.checkNotNull(columnMapping);\n\n    String encoding = getColumnEncoding(columnMapping);\n\n    return get(encoding);\n  ",
    "getSQLState": "\n    return sqlState;\n  ",
    "getBatch": "\n    return batch;\n  ",
    "testCompareTo": "\n    assertTrue(one.compareTo(two) < 0);\n    assertTrue(two.compareTo(one) > 0);\n    assertTrue(one.compareTo(zero) > 0);\n    assertTrue(zero.compareTo(two) < 0);\n  ",
    "enLargeBuffer": "\n    final int requestCapacity = Math.addExact(count, increment);\n    final int currentCapacity = buf.length;\n\n    if (requestCapacity > currentCapacity) {\n      // Increase size by a factor of 1.5x\n      int newCapacity = currentCapacity + (currentCapacity >> 1);\n\n      // Check for overflow scenarios\n      if (newCapacity < 0 || newCapacity > MAX_ARRAY_SIZE) {\n        newCapacity = MAX_ARRAY_SIZE;\n      } else if (newCapacity < requestCapacity) {\n        newCapacity = requestCapacity;\n      }\n      buf = Arrays.copyOf(buf, newCapacity);\n    }\n  }\n\n  /**\n   * {@inheritDoc",
    "getVarWithoutType": "\n    return var.altName != null ? conf.get(var.varname, conf.get(var.altName, var.defaultExpr))\n      : conf.get(var.varname, var.defaultExpr);\n  ",
    "restoreOutputStream": "\n    System.out.flush();\n    System.setOut(origOut);\n\n    System.err.flush();\n    System.setErr(origErr);\n  ",
    "doFilter": "\n      HttpServletRequestWrapper quoted =\n              new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      headerMap.forEach((k, v) -> httpResponse.addHeader(k, v));\n      chain.doFilter(quoted, httpResponse);\n    ",
    "update": "\n    if (length == 0) {\n      this.zeroClear();\n      return;\n    }\n    this.negative = false;\n    if (str[offset] == '-') {\n      this.negative = true;\n      ++offset;\n      --length;\n    } else if (str[offset] == '+') {\n      ++offset;\n      --length;\n    }\n    this.mag.update(str, offset, length);\n    if (this.mag.isZero()) {\n      this.negative = false;\n    }\n  ",
    "processOption": "\n      if (isBeeLineOpt(arg)) {\n        processBeeLineOpt(arg);\n      } else {\n        //-p with the next argument being for BeeLineOpts\n        if (\"-p\".equals(arg)) {\n          isPasswordOptionSet = true;\n          if(iter.hasNext()) {\n            String next = (String) iter.next();\n            if(isBeeLineOpt(next)) {\n              processBeeLineOpt(next);\n              return;\n            } else {\n              iter.previous();\n            }\n          }\n        }\n        super.processOption(arg, iter);\n      }\n    ",
    "getAccumuloToken": "\n    checkNotNull(user, \"Provided UGI was null\");\n    Collection<Token<? extends TokenIdentifier>> tokens = user.getTokens();\n    for (Token<?> token : tokens) {\n      if (ACCUMULO_SERVICE.equals(token.getKind())) {\n        return token;\n      }\n    }\n    return null;\n  ",
    "setShowElapsedTime": "\n    this.showElapsedTime = showElapsedTime;\n  ",
    "getConfInternal": "\n    Statement stmnt = null;\n    BufferedRows rows = null;\n    ResultSet rs = null;\n    try {\n      boolean hasResults = false;\n      DatabaseConnection dbconn = beeLine.getDatabaseConnection();\n      Connection conn = null;\n      if (dbconn != null)\n        conn = dbconn.getConnection();\n      if (conn != null) {\n        if (call) {\n          stmnt = conn.prepareCall(\"set\");\n          hasResults = ((CallableStatement) stmnt).execute();\n        } else {\n          stmnt = beeLine.createStatement();\n          hasResults = stmnt.execute(\"set\");\n        }\n      }\n      if (hasResults) {\n        rs = stmnt.getResultSet();\n        rows = new BufferedRows(beeLine, rs);\n      }\n    } catch (SQLException e) {\n      beeLine.error(e);\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e1) {\n          beeLine.error(e1);\n        }\n      }\n      if (stmnt != null) {\n        try {\n          stmnt.close();\n        } catch (SQLException e2) {\n          beeLine.error(e2);\n        }\n      }\n    }\n    return rows;\n  ",
    "calculateBytesHashCode": "\n    // Don't use this for ReduceSinkOperators\n    return murmurHash(keyBytes, keyStart, keyLength);\n  ",
    "byteDesc": "\n    double val = 0.0;\n    String ending = \"\";\n    if (len < 1024 * 1024) {\n      val = (1.0 * len) / 1024;\n      ending = \" KB\";\n    } else if (len < 1024 * 1024 * 1024) {\n      val = (1.0 * len) / (1024 * 1024);\n      ending = \" MB\";\n    } else if (len < 1024L * 1024 * 1024 * 1024) {\n      val = (1.0 * len) / (1024 * 1024 * 1024);\n      ending = \" GB\";\n    } else if (len < 1024L * 1024 * 1024 * 1024 * 1024) {\n      val = (1.0 * len) / (1024L * 1024 * 1024 * 1024);\n      ending = \" TB\";\n    } else {\n      val = (1.0 * len) / (1024L * 1024 * 1024 * 1024 * 1024);\n      ending = \" PB\";\n    }\n    return limitDecimalTo2(val) + ending;\n  ",
    "getScriptOutputFile": "\n    return scriptOutputFile;\n  ",
    "setOutputFormatZooKeeperInstance": "\n    try {\n      ClientConfiguration clientConf = getClientConfiguration(zookeepers, instanceName, isSasl);\n      AccumuloOutputFormat.setZooKeeperInstance(conf, clientConf);\n    } catch (IllegalStateException ise) {\n      // AccumuloOutputFormat complains if you re-set an already set value. We just don't care.\n      log.debug(\"Ignoring exception setting ZooKeeper instance of \" + instanceName + \" at \"\n          + zookeepers, ise);\n    }\n  }\n\n  /**\n   * Calls {@link AccumuloInputFormat#setMockInstance(JobConf, String)",
    "isBlobStorageAsScratchDir": "\n        return conf.getBoolean(\n                HiveConf.ConfVars.HIVE_BLOBSTORE_USE_BLOBSTORE_AS_SCRATCHDIR.varname,\n                DISABLE_BLOBSTORAGE_AS_SCRATCHDIR\n        );\n    }\n\n    /**\n     * Returns true if {@link HiveConf.ConfVars#HIVE_BLOBSTORE_OPTIMIZATIONS_ENABLED",
    "getIteratorPushdown": "\n    return conf.getBoolean(ITERATOR_PUSHDOWN_KEY, ITERATOR_PUSHDOWN_DEFAULT);\n  ",
    "render": "\n    if (monitor == null) return;\n    // position the cursor to line 0\n    repositionCursor();\n\n    // print header\n    // -------------------------------------------------------------------------------\n    //         VERTICES     STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED\n    // -------------------------------------------------------------------------------\n    reprintLine(SEPARATOR);\n    reprintLineWithColorAsBold(String.format(HEADER_FORMAT, monitor.headers().toArray()),\n      Ansi.Color.CYAN);\n    reprintLine(SEPARATOR);\n\n\n    // Map 1 .......... container  SUCCEEDED      7          7        0        0       0       0\n    List<String> printReady =\n        monitor.rows().stream().map(row -> String.format(VERTEX_FORMAT, row.toArray())).collect(Collectors.toList());\n    reprintMultiLine(StringUtils.join(printReady, \"\\n\"));\n\n    // -------------------------------------------------------------------------------\n    // VERTICES: 03/04            [=================>>-----] 86%  ELAPSED TIME: 1.71 s\n    // -------------------------------------------------------------------------------\n    String progressStr = \"\" + (int) (monitor.progressedPercentage() * 100) + \"%\";\n    float et = (float) (System.currentTimeMillis() - monitor.startTime()) / (float) 1000;\n    String elapsedTime = \"ELAPSED TIME: \" + secondsFormatter.format(et) + \" s\";\n    String footer = String.format(\n      FOOTER_FORMAT,\n      monitor.footerSummary(),\n      getInPlaceProgressBar(monitor.progressedPercentage()),\n      progressStr,\n      elapsedTime);\n\n    reprintLine(SEPARATOR);\n    reprintLineWithColorAsBold(footer, Ansi.Color.RED);\n    reprintLine(SEPARATOR);\n  ",
    "getColType": "\n    return colMap.get(encode(cf, cq));\n  ",
    "checkHiveConf": "\n    Assert.assertEquals(expectedHiveVal, new HiveConf().get(name));\n  ",
    "testDivideDestructiveSignedInt128Again": "\n    SignedInt128 complicated1 = new SignedInt128(0xF9892FCA, 0x59D109AD, 0, 0);\n    SignedInt128 complicated2 = new SignedInt128(0xF09DC19A, 3, 0, 0);\n    BigInteger bigInteger1 = complicated1.toBigIntegerSlow();\n    BigInteger bigInteger2 = complicated2.toBigIntegerSlow();\n    complicated1.divideDestructive(complicated2, new SignedInt128());\n    BigInteger ans = bigInteger1.divide(bigInteger2);\n    assertEquals(ans, complicated1.toBigIntegerSlow());\n  ",
    "addBeelineShutdownHook": "\n    // add shutdown hook to flush the history to history file and it also close all open connections\n    ShutdownHookManager.addShutdownHook(getShutdownHook());\n  ",
    "testPiArcsine": "\n\n    // This one uses the arcsin method. Involves more multiplications/divisions.\n    // pi=Sum (3 * 2n!/(16^n * (2n+1) * n! * n!))\n    // =Sum (3 * ((n+1)(n+2)...2n)/n!*16^n/(2n+1))\n    // =Sum (3 / (2n+1) * (n+1)/16 * (n+2)/32... * 2n/16(n+1))\n    // (note that it is split so that each term is not overflown)\n    final int LOOPS = 50;\n    final short SCALE = 30;\n    Decimal128 total = new Decimal128(0);\n    Decimal128 multiplier = new Decimal128();\n    Decimal128 divisor = new Decimal128();\n    Decimal128 current = new Decimal128();\n    for (int i = 0; i < LOOPS; ++i) {\n      current.update(3, SCALE);\n      divisor.update(2 * i + 1, SCALE);\n      current.divideDestructive(divisor, SCALE);\n      for (int j = 1; j <= i; ++j) {\n        multiplier.update(i + j, SCALE);\n        divisor.update(16 * j, SCALE);\n        current.multiplyDestructive(multiplier, SCALE);\n        current.divideDestructive(divisor, SCALE);\n      }\n\n      total.addDestructive(current, SCALE);\n    }\n\n    assertTrue(total.toFormalString().startsWith(\"3.141592653589793238462\"));\n  ",
    "getDayOfWeek": "\n    return localDateTime.getDayOfWeek().plus(1).getValue();\n  ",
    "testUnsignedInt128Count": "\n    UnsignedInt128 minValue = UnsignedInt128.MIN_VALUE;\n    assertEquals((byte) 0, minValue.getCount());\n    UnsignedInt128 maxValue = UnsignedInt128.MAX_VALUE;\n    assertEquals((byte) 4, maxValue.getCount());\n    UnsignedInt128 unsignedInt128 = new UnsignedInt128(0, 0, 0, 0);\n    assertEquals((byte) 0, unsignedInt128.getCount());\n\n    // Let count equals 4. updateCount() will not be called if update v0, v1, v2, v3.\n    unsignedInt128.setV3(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.setV3(200);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.setV2(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.setV1(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.setV0(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.zeroClear();\n    assertEquals((byte) 0, unsignedInt128.getCount());\n\n    // Let count equals 3. updateCount() will not be called if update v0, v1, v2.\n    unsignedInt128.setV2(100);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV2(200);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV1(100);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV0(100);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV3(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.zeroClear();\n    assertEquals((byte) 0, unsignedInt128.getCount());\n\n    // Let count equals 2. updateCount() will not be called if update v0, v1.\n    unsignedInt128.setV1(100);\n    assertEquals((byte) 2, unsignedInt128.getCount());\n    unsignedInt128.setV1(100);\n    assertEquals((byte) 2, unsignedInt128.getCount());\n    unsignedInt128.setV0(1);\n    assertEquals((byte) 2, unsignedInt128.getCount());\n    unsignedInt128.setV2(100);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV3(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.zeroClear();\n    assertEquals((byte) 0, unsignedInt128.getCount());\n\n    // Let count equals 2. updateCount() will not be called if update v0.\n    unsignedInt128.setV0(2);\n    assertEquals((byte) 1, unsignedInt128.getCount());\n    unsignedInt128.setV0(2);\n    assertEquals((byte) 1, unsignedInt128.getCount());\n    unsignedInt128.setV1(100);\n    assertEquals((byte) 2, unsignedInt128.getCount());\n    unsignedInt128.setV2(100);\n    assertEquals((byte) 3, unsignedInt128.getCount());\n    unsignedInt128.setV3(100);\n    assertEquals((byte) 4, unsignedInt128.getCount());\n    unsignedInt128.zeroClear();\n    assertEquals((byte) 0, unsignedInt128.getCount());\n  ",
    "green": "\n    return append(GREEN, str);\n  ",
    "testAddDependencyNonRoot": "\n    Map<String, Stage> children = new LinkedHashMap<>();\n    children.put(\"a\", stageA);\n    children.put(\"b\", stageB);\n\n\n    String jsonString = \"{\\\"DEPENDENT STAGES\\\":\\\"a,b\\\"}\";\n    JSONObject names = new JSONObject(jsonString);\n\n    uut.addDependency(names, children);\n\n    assertEquals(2, uut.parentStages.size());\n    assertEquals(stageA, uut.parentStages.get(0));\n    assertEquals(stageB, uut.parentStages.get(1));\n\n    assertEquals(1, stageA.childStages.size());\n    assertEquals(uut, stageA.childStages.get(0));\n\n    assertEquals(1, stageB.childStages.size());\n    assertEquals(uut, stageB.childStages.get(0));\n  ",
    "isAscii": "\n    return (b & 0x80) == 0;\n  ",
    "debug": "\n    beeLine.debug(message);\n  ",
    "setupBufferedOutput": "\n    OutputStream pdataOut;\n    if (test) {\n      pdataOut = System.out;\n    } else {\n      pdataOut = new FileOutputStream(FileDescriptor.out);\n    }\n    BufferedOutputStream bos =\n        new BufferedOutputStream(pdataOut, STDOUT_BUFFER_SIZE);\n    PrintStream ps =\n        new PrintStream(bos, false);\n    System.setOut(ps);\n  ",
    "beforeTests": "\n    PrintWriter writer = new PrintWriter(fileName);\n    writer.println(\"select 1;\");\n    writer.println(\"select 2;\");\n    writer.println(\"select 3;\");\n    writer.println(\"select 4;\");\n    writer.println(\"select 5;\");\n    writer.println(\"select 6;\");\n    writer.println(\"select 7;\");\n    writer.println(\"select 8;\");\n    writer.println(\"select 9;\");\n    writer.println(\"select 10;\");\n    writer.close();\n  ",
    "testAddDependencyRoot": "\n    Map<String, Stage> children = new LinkedHashMap<>();\n    children.put(\"a\", stageA);\n    children.put(\"b\", stageB);\n\n    String jsonString = \"{\\\"ROOT STAGE\\\":\\\"X\\\",\\\"DEPENDENT STAGES\\\":\\\"a,b\\\"}\";\n    JSONObject names = new JSONObject(jsonString);\n\n    uut.addDependency(names, children);\n\n    assertEquals(2, uut.parentStages.size());\n    assertEquals(1, stageA.childStages.size());\n    assertEquals(1, stageB.childStages.size());\n  ",
    "runQuery": "\n    Statement stmt = con.createStatement();\n    ResultSet res = stmt.executeQuery(sqlStmt);\n\n    ResultSetMetaData meta = res.getMetaData();\n    System.out.println(\"Resultset has \" + meta.getColumnCount() + \" columns\");\n    for (int i = 1; i <= meta.getColumnCount(); i++) {\n      System.out.println(\"Column #\" + i + \" Name: \" + meta.getColumnName(i) +\n            \" Type: \" + meta.getColumnType(i));\n    }\n\n    while (res.next()) {\n      for (int i = 1; i <= meta.getColumnCount(); i++) {\n        System.out.println(\"Column #\" + i + \": \" + res.getString(i));\n      }\n    }\n    res.close();\n    stmt.close();\n  ",
    "getURI": "\n      return null;\n    }\n\n    URI uri = new URI(path);\n    if (uri.getScheme() == null) {\n      // if no scheme in the path, we assume it's file on local fs.\n      uri = new File(path).toURI();\n    }\n\n    return uri;\n  ",
    "makeDefaultListBucketingDirName": "\n    String lbDirName;\n    String defaultDir = FileUtils.escapePathName(name);\n    StringBuilder defaultDirPath = new StringBuilder();\n    for (int i = 0; i < skewedCols.size(); i++) {\n      if (i > 0) {\n        defaultDirPath.append(Path.SEPARATOR);\n      }\n      defaultDirPath.append(defaultDir);\n    }\n    lbDirName = defaultDirPath.toString();\n    return lbDirName;\n  ",
    "getTimestampFromValues": "\n\n    // Get list of temporal Tokens\n    List<Token> temporalTokens = tokens.stream()\n        .filter(token-> token.type == TokenType.NUMERIC_TEMPORAL\n            || token.type == TokenType.CHARACTER_TEMPORAL)\n        .collect(Collectors.toList());\n    Preconditions.checkState(temporalTokens.size() == temporalValues.size(),\n        \"temporalTokens list length (\" + temporalTokens.size()\n        + \") differs from that of temporalValues (length: \" + temporalValues.size() + \")\");\n\n    // Get sorted list of temporal Token/value Pairs\n    List<ImmutablePair<Token, Integer>> tokenValueList = new ArrayList<>(temporalTokens.size());\n    for (int i = 0; i < temporalTokens.size(); i++) {\n      ImmutablePair<Token, Integer> pair =\n          new ImmutablePair<>(temporalTokens.get(i), temporalValues.get(i));\n      tokenValueList.add(pair);\n    }\n    tokenValueList.sort(((Comparator<ImmutablePair<Token, Integer>>) (o1, o2) -> {\n              Token token1 = o1.left;\n              Token token2 = o2.left;\n              return token1.temporalField.getBaseUnit().getDuration()\n                .compareTo(token2.temporalField.getBaseUnit().getDuration());\n            }).reversed());\n\n    // Create Timestamp\n    LocalDateTime ldt = LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC);\n    for (Pair<Token, Integer> pair : tokenValueList) {\n      TemporalField temporalField = pair.getLeft().temporalField;\n      int value = pair.getRight();\n      try {\n        ldt = ldt.with(temporalField, value);\n      } catch (DateTimeException e){\n        throw new IllegalArgumentException(\n            \"Value \" + value + \" not valid for token \" + temporalField);\n      }\n    }\n    return Timestamp.ofEpochSecond(ldt.toEpochSecond(ZoneOffset.UTC), ldt.getNano());\n  ",
    "testMapSerialization": "\n    Properties properties = new Properties();\n    Configuration conf = new Configuration();\n\n    properties.setProperty(AccumuloSerDeParameters.COLUMN_MAPPINGS, \":rowID,cf:vals\");\n    properties.setProperty(serdeConstants.LIST_COLUMNS, \"row,values\");\n    properties.setProperty(serdeConstants.LIST_COLUMN_TYPES, \"string,map<string,string>\");\n    properties.setProperty(serdeConstants.COLLECTION_DELIM, \":\");\n    properties.setProperty(serdeConstants.MAPKEY_DELIM, \"=\");\n\n    // Get one of the default separators to avoid having to set a custom separator\n    char collectionSeparator = ':', kvSeparator = '=';\n\n    serde.initialize(conf, properties, null);\n\n    AccumuloHiveRow row = new AccumuloHiveRow();\n    row.setRowId(\"r1\");\n    row.add(\"cf\", \"vals\", (\"k1\" + kvSeparator + \"v1\" + collectionSeparator + \"k2\" + kvSeparator\n        + \"v2\" + collectionSeparator + \"k3\" + kvSeparator + \"v3\").getBytes());\n\n    Object obj = serde.deserialize(row);\n\n    assertNotNull(obj);\n    assertTrue(obj instanceof LazyAccumuloRow);\n\n    LazyAccumuloRow lazyRow = (LazyAccumuloRow) obj;\n    Object field0 = lazyRow.getField(0);\n    assertNotNull(field0);\n    assertTrue(field0 instanceof LazyString);\n    assertEquals(row.getRowId(), ((LazyString) field0).getWritableObject().toString());\n\n    Object field1 = lazyRow.getField(1);\n    assertNotNull(field1);\n    assertTrue(field1 instanceof LazyMap);\n    LazyMap map = (LazyMap) field1;\n\n    Map<Object,Object> untypedMap = map.getMap();\n    assertEquals(3, map.getMapSize());\n    Set<String> expectedKeys = new HashSet<String>();\n    expectedKeys.add(\"k1\");\n    expectedKeys.add(\"k2\");\n    expectedKeys.add(\"k3\");\n    for (Entry<Object,Object> entry : untypedMap.entrySet()) {\n      assertNotNull(entry.getKey());\n      assertTrue(entry.getKey() instanceof LazyString);\n      LazyString key = (LazyString) entry.getKey();\n\n      assertNotNull(entry.getValue());\n      assertTrue(entry.getValue() instanceof LazyString);\n      LazyString value = (LazyString) entry.getValue();\n\n      String strKey = key.getWritableObject().toString(), strValue = value.getWritableObject()\n          .toString();\n\n      assertTrue(expectedKeys.remove(strKey));\n\n      assertEquals(2, strValue.length());\n      assertTrue(strValue.startsWith(\"v\"));\n      assertTrue(strValue.endsWith(Character.toString(strKey.charAt(1))));\n    }\n\n    assertTrue(\"Did not find expected keys: \" + expectedKeys, expectedKeys.isEmpty());\n  ",
    "setV": "\n    this.v[0] = v[0];\n    this.v[1] = v[1];\n    this.v[2] = v[2];\n    this.v[3] = v[3];\n    updateCount();\n  ",
    "setDelimiterForDSV": "\n    this.delimiterForDSV = delimiterForDSV;\n  ",
    "checkParseTimestampIso": "\n    formatter =\n        new HiveSqlDateTimeFormatter(parsePattern, true,\n            Optional.of(LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC)));\n    Timestamp ts = formatter.parseTimestamp(input);\n    formatter =\n        new HiveSqlDateTimeFormatter(formatPattern, false,\n            Optional.of(LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC)));\n    assertEquals(expectedOutput, formatter.format(ts));\n  ",
    "setModifiableWhiteListRegex": "\n    if (paramNameRegex == null) {\n      return;\n    }\n    modWhiteListPattern = Pattern.compile(paramNameRegex);\n  ",
    "getAuthType": "\n    return authType;\n  ",
    "stop": "\n    shouldRun = false;\n    if (isStarted()) {\n      monitorThread.interrupt();\n      try {\n        monitorThread.join();\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n    }\n  ",
    "checkBucketing": "\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_BUCKETING) ? null : NO_BUCKETING_MSG;\n    ",
    "getManifest": "\n    URL base = BeeLine.class.getResource(\"/META-INF/MANIFEST.MF\");\n    URLConnection c = base.openConnection();\n    if (c instanceof JarURLConnection) {\n      return ((JarURLConnection) c).getManifest();\n    }\n    return null;\n  ",
    "quoteComments": "\n    char[] chars = value.toCharArray();\n    if (!commentProvided(chars)) {\n      return null;\n    }\n    StringBuilder builder = new StringBuilder();\n    int prev = 0;\n    for (int i = 0; i < chars.length; i++) {\n      if (chars[i] == 0x00) {\n        if (builder.length() > 0) {\n          builder.append(',');\n        }\n        builder.append('\\'').append(chars, prev, i - prev).append('\\'');\n        prev = i + 1;\n      }\n    }\n    builder.append(\",\\'\").append(chars, prev, chars.length - prev).append('\\'');\n    return builder.toString();\n  ",
    "testSetPattern": "\n    verifyPatternParsing(\" ---yyyy-\\'-:-  -,.;/MM-dd--\", Arrays.asList(\n        null, // represents separator, which has no temporal field\n        ChronoField.YEAR,\n        null,\n        ChronoField.MONTH_OF_YEAR,\n        null,\n        ChronoField.DAY_OF_MONTH,\n        null\n        ));\n\n    verifyPatternParsing(\"ymmdddhh24::mi:ss A.M. pm\", 25, \"ymmdddhh24::mi:ss A.M. pm\",\n        Arrays.asList(\n        ChronoField.YEAR,\n        ChronoField.MONTH_OF_YEAR,\n        ChronoField.DAY_OF_YEAR,\n        ChronoField.HOUR_OF_DAY,\n        null, ChronoField.MINUTE_OF_HOUR,\n        null, ChronoField.SECOND_OF_MINUTE,\n        null, ChronoField.AMPM_OF_DAY,\n        null, ChronoField.AMPM_OF_DAY\n    ));\n  ",
    "putAll": "\n    if (interned != null) copyFromInternedToThis();\n    super.putAll(t);\n  ",
    "testUnitFor": "\n    Assert.assertEquals(TimeUnit.SECONDS, HiveConf.unitFor(\"L\", TimeUnit.SECONDS));\n    Assert.assertEquals(TimeUnit.MICROSECONDS, HiveConf.unitFor(\"\", TimeUnit.MICROSECONDS));\n    Assert.assertEquals(TimeUnit.DAYS, HiveConf.unitFor(\"d\", null));\n    Assert.assertEquals(TimeUnit.DAYS, HiveConf.unitFor(\"days\", null));\n    Assert.assertEquals(TimeUnit.HOURS, HiveConf.unitFor(\"h\", null));\n    Assert.assertEquals(TimeUnit.HOURS, HiveConf.unitFor(\"hours\", null));\n    Assert.assertEquals(TimeUnit.MINUTES, HiveConf.unitFor(\"m\", null));\n    Assert.assertEquals(TimeUnit.MINUTES, HiveConf.unitFor(\"minutes\", null));\n    Assert.assertEquals(TimeUnit.SECONDS, HiveConf.unitFor(\"s\", null));\n    Assert.assertEquals(TimeUnit.SECONDS, HiveConf.unitFor(\"seconds\", null));\n    Assert.assertEquals(TimeUnit.MILLISECONDS, HiveConf.unitFor(\"ms\", null));\n    Assert.assertEquals(TimeUnit.MILLISECONDS, HiveConf.unitFor(\"msecs\", null));\n    Assert.assertEquals(TimeUnit.MICROSECONDS, HiveConf.unitFor(\"us\", null));\n    Assert.assertEquals(TimeUnit.MICROSECONDS, HiveConf.unitFor(\"usecs\", null));\n    Assert.assertEquals(TimeUnit.NANOSECONDS, HiveConf.unitFor(\"ns\", null));\n    Assert.assertEquals(TimeUnit.NANOSECONDS, HiveConf.unitFor(\"nsecs\", null));\n  ",
    "connectUsingArgs": "\n    String driver = null, user = null, pass = \"\", url = null;\n    String auth = null;\n\n\n    if (cl.hasOption(\"help\")) {\n      usage();\n      getOpts().setHelpAsked(true);\n      return true;\n    }\n    \n    if (cl.hasOption(\"getUrlsFromBeelineSite\")) {\n      printBeelineSiteUrls();\n      getOpts().setBeelineSiteUrlsAsked(true);\n      return true;\n    }\n\n    Properties hiveVars = cl.getOptionProperties(\"hivevar\");\n    for (String key : hiveVars.stringPropertyNames()) {\n      getOpts().getHiveVariables().put(key, hiveVars.getProperty(key));\n    }\n\n    Properties hiveConfs = cl.getOptionProperties(\"hiveconf\");\n    for (String key : hiveConfs.stringPropertyNames()) {\n      setHiveConfVar(key, hiveConfs.getProperty(key));\n    }\n\n    driver = cl.getOptionValue(\"d\");\n    auth = cl.getOptionValue(\"a\");\n    user = cl.getOptionValue(\"n\");\n    getOpts().setAuthType(auth);\n    if (cl.hasOption(\"w\")) {\n      pass = obtainPasswordFromFile(cl.getOptionValue(\"w\"));\n    } else {\n      if (beelineParser.isPasswordOptionSet) {\n        pass = cl.getOptionValue(\"p\");\n      }\n    }\n    url = cl.getOptionValue(\"u\");\n    if ((url == null) && cl.hasOption(\"reconnect\")){\n      // If url was not specified with -u, but -r was present, use that.\n      url = getOpts().getLastConnectedUrl();\n    }\n    getOpts().setInitFiles(cl.getOptionValues(\"i\"));\n    getOpts().setScriptFile(cl.getOptionValue(\"f\"));\n\n    if (url != null) {\n      String hplSqlMode = Utils.parsePropertyFromUrl(url, Constants.MODE);\n      if (\"HPLSQL\".equalsIgnoreCase(hplSqlMode)) {\n        getOpts().setDelimiter(\"/\");\n        getOpts().setEntireLineAsCommand(true);\n      }\n      // Specifying username/password/driver explicitly will override the values from the url;\n      // make sure we don't override the values present in the url with empty values.\n      if (user == null) {\n        user = Utils.parsePropertyFromUrl(url, JdbcConnectionParams.AUTH_USER);\n      }\n      if (pass == null) {\n        pass = Utils.parsePropertyFromUrl(url, JdbcConnectionParams.AUTH_PASSWD);\n      }\n      if (driver == null) {\n        driver = Utils.parsePropertyFromUrl(url, JdbcConnectionParams.PROPERTY_DRIVER);\n      }\n\n      String com;\n      String comForDebug;\n      if(pass != null) {\n        com = constructCmd(url, user, pass, driver, false);\n        comForDebug = constructCmd(url, user, pass, driver, true);\n      } else {\n        com = constructCmdUrl(url, user, driver, false);\n        comForDebug = constructCmdUrl(url, user, driver, true);\n      }\n      debug(comForDebug);\n      if (!dispatch(com)) {\n        exit = true;\n        return false;\n      }\n      return true;\n    }\n    // load property file\n    String propertyFile = cl.getOptionValue(\"property-file\");\n    if (propertyFile != null) {\n      try {\n        this.consoleReader = new ConsoleReader();\n      } catch (IOException e) {\n        handleException(e);\n      }\n      if (!dispatch(\"!properties \" + propertyFile)) {\n        exit = true;\n        return false;\n      } else {\n        return true;\n      }\n    }\n    return false;\n  ",
    "data": "\n    // generate the dummy driver by using txt file\n    String u = HiveTestUtils.getFileFromClasspath(\"DummyDriver.txt\");\n    Map<File, String> extraContent=new HashMap<>();\n    extraContent.put(new File(\"META-INF/services/java.sql.Driver\"), dummyDriverClazzName);\n    File jarFile = HiveTestUtils.genLocalJarForTest(u, dummyDriverClazzName, extraContent);\n    String pathToDummyDriver = jarFile.getAbsolutePath();\n    String pathToPostgresJar = System.getProperty(\"maven.local.repository\")\n        + File.separator + \"org\"\n        + File.separator + \"postgresql\"\n        + File.separator + \"postgresql\"\n        + File.separator + \"42.5.1\"\n        + File.separator\n        + \"postgresql-42.5.1.jar\";\n    return Arrays.asList(new Object[][] {\n        { \"jdbc:postgresql://host:5432/testdb\", \"org.postgresql.Driver\", pathToPostgresJar, true },\n        { \"jdbc:dummy://host:5432/testdb\", dummyDriverClazzName, pathToDummyDriver, false } });\n  ",
    "parseTextToken": "\n    checkFillModeOff(fillMode, begin);\n    int end = begin;\n    do {\n      end = fullPattern.indexOf('\\\"', end + 1);\n      if (end == -1) {\n        throw new IllegalArgumentException(\n            \"Missing closing double quote (\\\") opened at index \" + begin);\n      }\n    // if double quote is escaped with a backslash, keep looking for the closing quotation mark\n    } while (\"\\\\\".equals(fullPattern.substring(end - 1, end)));\n    Token lastAddedToken = new Token(TokenType.TEXT, fullPattern.substring(begin + 1, end));\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  ",
    "isBeelineSiteUrlsAsked": "\n    return beelineSiteUrlsAsked;\n  ",
    "fetchStarted": "\n    fetchStarted = true;\n  ",
    "testBeelineOpts": "\n    TestBeeline bl = new TestBeeline();\n    String args[] =\n        new String[] { \"-u\", \"url\", \"-n\", \"name\", \"-p\", \"password\", \"-d\", \"driver\",\n            \"--autoCommit=true\", \"--verbose\", \"--truncateTable\" };\n    Assert.assertEquals(0, bl.initArgs(args));\n    Assert.assertTrue(bl.connectArgs.equals(\"url name password driver\"));\n    Assert.assertTrue(bl.getOpts().getAutoCommit());\n    Assert.assertTrue(bl.getOpts().getVerbose());\n    Assert.assertTrue(bl.getOpts().getTruncateTable());\n  ",
    "listStatusIterator": "\n    return RemoteIterators.filteringRemoteIterator(fs.listStatusIterator(path),\n        status -> filter.accept(status.getPath()));\n  ",
    "getTimeout": "\n    return timeout;\n  ",
    "all": "\n    int index = beeLine.getDatabaseConnections().getIndex();\n    boolean success = true;\n\n    for (int i = 0; i < beeLine.getDatabaseConnections().size(); i++) {\n      beeLine.getDatabaseConnections().setIndex(i);\n      beeLine.output(beeLine.loc(\"executing-con\", beeLine.getDatabaseConnection()));\n      // ### FIXME: this is broken for multi-line SQL\n      success = sql(line.substring(\"all \".length())) && success;\n    }\n\n    // restore index\n    beeLine.getDatabaseConnections().setIndex(index);\n    return success;\n  ",
    "getSignum": "\n    return signum;\n  }\n\n  /**\n   * Returns the <i>scale</i> of this {@code Decimal128}. The scale is the\n   * positive number of digits to the right of the decimal point.\n   *\n   * @return the scale of this {@code Decimal128",
    "getJobCredentialProviderPassword": "\n    String jobKeyStoreLocation =\n        conf.get(HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH.varname);\n    String password = null;\n    if(StringUtils.isNotBlank(jobKeyStoreLocation)) {\n      password = System.getenv(Constants.HIVE_SERVER2_JOB_CREDSTORE_PASSWORD_ENVVAR);\n      if (StringUtils.isNotBlank(password)) {\n        return password;\n      }\n    }\n    password = System.getenv(Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR);\n    if (StringUtils.isNotBlank(password)) {\n      return password;\n    }\n    return null;\n  ",
    "loadProperties": "\n    for (Object element : props.keySet()) {\n      String key = element.toString();\n      if (key.equals(PROPERTY_NAME_EXIT)) {\n        // fix for sf.net bug 879422\n        continue;\n      }\n      if (key.startsWith(PROPERTY_PREFIX)) {\n        set(key.substring(PROPERTY_PREFIX.length()),\n            props.getProperty(key));\n      }\n    }\n  ",
    "setConnectorInfoForInputAndOutput": "\n    // Obtain a delegation token from Accumulo\n    AuthenticationToken token = getDelegationToken(conn);\n\n    // Make sure the Accumulo token is set in the Configuration (only a stub of the Accumulo\n    // AuthenticationToken is serialized, not the entire token). configureJobConf may be\n    // called multiple times with the same JobConf which results in an error from Accumulo\n    // MapReduce API. Catch the error, log a debug message and just keep going\n    try {\n      InputConfigurator.setConnectorInfo(AccumuloInputFormat.class, conf,\n          params.getAccumuloUserName(), token);\n    } catch (IllegalStateException e) {\n      // The implementation balks when this method is invoked multiple times\n      log.debug(\"Ignoring IllegalArgumentException about re-setting connector information\");\n    }\n    try {\n      OutputConfigurator.setConnectorInfo(AccumuloOutputFormat.class, conf,\n          params.getAccumuloUserName(), token);\n    } catch (IllegalStateException e) {\n      // The implementation balks when this method is invoked multiple times\n      log.debug(\"Ignoring IllegalArgumentException about re-setting connector information\");\n    }\n\n    return getHadoopToken(token);\n  ",
    "getUrlToUse": "\n    boolean useIndirectUrl = false;\n    // If the url passed to us is a valid url with a protocol, we use it as-is\n    // Otherwise, we assume it is a name of parameter that we have to get the url from\n    try {\n      URI tryParse = new URI(urlParam);\n      if (tryParse.getScheme() == null){\n        // param had no scheme, so not a URL\n        useIndirectUrl = true;\n      }\n    } catch (URISyntaxException e){\n      // param did not parse as a URL, so not a URL\n      useIndirectUrl = true;\n    }\n    if (useIndirectUrl){\n      // Use url param indirectly - as the name of an env var that contains the url\n      // If the urlParam is \"default\", we would look for a BEELINE_URL_DEFAULT url\n      String envUrl = beeLine.getOpts().getEnv().get(\n          BeeLineOpts.URL_ENV_PREFIX + urlParam.toUpperCase());\n      if (envUrl != null){\n        return envUrl;\n      }\n    }\n    return urlParam; // default return the urlParam passed in as-is.\n  ",
    "testMakeRelative": "\n    Path parentPath = new Path(\"/user/hive/database\");\n    Path childPath = new Path(parentPath, \"table/dir/subdir\");\n    Path relativePath = FileUtils.makeRelative(parentPath, childPath);\n    assertEquals(\"table/dir/subdir\", relativePath.toString());\n\n    // try with parent as Root.\n    relativePath = FileUtils.makeRelative(new Path(Path.SEPARATOR), childPath);\n    assertEquals(\"user/hive/database/table/dir/subdir\", relativePath.toString());\n\n    // try with non child path, it should return the child path as is.\n    childPath = new Path(\"/user/hive/database1/table/dir/subdir\");\n    relativePath = FileUtils.makeRelative(parentPath, childPath);\n    assertEquals(childPath.toString(), relativePath.toString());\n  ",
    "storeTokenInJobConf": "\n    SessionUtils.setTokenStr(Utils.getUGI(),\n          tokenStr, HiveAuthConstants.HS2_CLIENT_TOKEN);\n    System.out.println(\"Stored token \" + tokenStr);\n  ",
    "testMultiplyDestructiveSignedInt128": "\n    two.multiplyDestructive(one);\n    assertEquals(2L, two.longValue());\n    assertEquals(1L, one.longValue());\n    two.multiplyDestructive(two);\n    assertEquals(4L, two.longValue());\n\n    SignedInt128 five = new SignedInt128(5);\n    five.multiplyDestructive(new SignedInt128(6432346));\n    assertEquals(6432346 * 5, five.getV0());\n    assertEquals(0, five.getV1());\n    assertEquals(0, five.getV2());\n    assertEquals(0, five.getV3());\n\n    SignedInt128 big = new SignedInt128((1L << 62) + (3L << 34) + 3L);\n    big.multiplyDestructive(new SignedInt128(96));\n\n    assertEquals(3 * 96, big.getV0());\n    assertEquals(96 * (3 << 2), big.getV1());\n    assertEquals(96 / 4, big.getV2());\n    assertEquals(0, big.getV3());\n\n    SignedInt128 tmp = new SignedInt128(1);\n    tmp.shiftLeftDestructive(126);\n    try {\n      tmp.multiplyDestructive(new SignedInt128(2));\n      fail();\n    } catch (ArithmeticException ex) {\n      // ok\n    }\n\n    SignedInt128 complicated1 = new SignedInt128(0xF9892FCA, 0x59D109AD,\n        0x0534AB4C, 0);\n    BigInteger bigInteger1 = complicated1.toBigIntegerSlow();\n    SignedInt128 complicated2 = new SignedInt128(54234234, 9, 0, 0);\n    BigInteger bigInteger2 = complicated2.toBigIntegerSlow();\n    complicated1.multiplyDestructive(complicated2);\n    BigInteger ans = bigInteger1.multiply(bigInteger2);\n    assertEquals(ans, complicated1.toBigIntegerSlow());\n\n    try {\n      SignedInt128 complicated3 = new SignedInt128(0xF9892FCA, 0x59D109AD,\n          0x0534AB4C, 0);\n      complicated3.multiplyDestructive(new SignedInt128(54234234, 9845, 0, 0));\n      fail();\n    } catch (ArithmeticException ex) {\n      // ok\n    }\n  ",
    "getDescription": "\n      String validator = validatorDescription();\n      if (validator != null) {\n        return validator + \".\\n\" + description;\n      }\n      return description;\n    ",
    "validateRequest": "\n    HttpServletRequest request = (HttpServletRequest) req;\n    HttpServletResponse response = (HttpServletResponse) res;\n    String credentials = request.getHeader(HttpHeader.AUTHORIZATION.asString());\n\n    try {\n      if (!mandatory)\n        return new DeferredAuthentication(this);\n\n      if (credentials != null) {\n        int space = credentials.indexOf(' ');\n        if (space > 0) {\n          String method = credentials.substring(0, space);\n          if (\"basic\".equalsIgnoreCase(method)) {\n            credentials = credentials.substring(space + 1);\n            credentials = B64Code.decode(credentials, StandardCharsets.ISO_8859_1);\n            int i = credentials.indexOf(':');\n            if (i > 0) {\n              String username = credentials.substring(0, i);\n              String password = credentials.substring(i + 1);\n\n              UserIdentity user = login(username, password);\n              if (user != null) {\n                return new UserAuthentication(getAuthMethod(), user);\n              }\n            }\n          }\n        }\n      }\n\n      if (DeferredAuthentication.isDeferred(response))\n        return Authentication.UNAUTHENTICATED;\n\n      response.setHeader(HttpHeader.WWW_AUTHENTICATE.asString(), \"basic realm=\\\"\" + _loginService.getName() + '\"');\n      response.sendError(HttpServletResponse.SC_UNAUTHORIZED);\n      return Authentication.SEND_CONTINUE;\n    } catch (IOException e) {\n      throw new ServerAuthException(e);\n    }\n  ",
    "testParseWithKerberosNoSSL": "\n    String url = getParsedUrlFromConfigFile(\"test-hs2-conn-conf-kerberos-nossl.xml\");\n    String expectedUrl =\n        \"jdbc:hive2://localhost:10000/default;principal=hive/dummy-hostname@domain.com;ssl=false\";\n    Assert.assertTrue(\"Expected \" + expectedUrl + \" got \" + url, expectedUrl.equals(url));\n  ",
    "getConstVal": "\n    return constVal;\n  ",
    "testMixedSerializationMap": "\n    AccumuloHiveRow row = new AccumuloHiveRow(\"row\");\n\n    row.add(new Text(\"cf1\"), new Text(toBytes(1)), \"2\".getBytes());\n    row.add(new Text(\"cf1\"), new Text(toBytes(2)), \"4\".getBytes());\n    row.add(new Text(\"cf1\"), new Text(toBytes(3)), \"6\".getBytes());\n\n    HiveAccumuloMapColumnMapping mapping = new HiveAccumuloMapColumnMapping(\"cf1\", null,\n        ColumnEncoding.BINARY, ColumnEncoding.STRING, \"column\", TypeInfoFactory.getMapTypeInfo(\n            TypeInfoFactory.intTypeInfo, TypeInfoFactory.intTypeInfo).toString());\n\n    // Map of Integer to String\n    Text nullSequence = new Text(\"\\\\N\");\n    ObjectInspector oi = LazyFactory.createLazyObjectInspector(TypeInfoUtils\n        .getTypeInfosFromTypeString(\"map<int,int>\").get(0), new byte[] {(byte) 1, (byte) 2}, 0,\n        nullSequence, false, (byte) 0);\n\n    LazyAccumuloMap map = new LazyAccumuloMap((LazyMapObjectInspector) oi);\n    map.init(row, mapping);\n\n    Assert.assertEquals(3, map.getMapSize());\n\n    Object o = map.getMapValueElement(new IntWritable(1));\n    Assert.assertNotNull(o);\n    Assert.assertEquals(new IntWritable(2), ((LazyInteger) o).getWritableObject());\n\n    o = map.getMapValueElement(new IntWritable(2));\n    Assert.assertNotNull(o);\n    Assert.assertEquals(new IntWritable(4), ((LazyInteger) o).getWritableObject());\n\n    o = map.getMapValueElement(new IntWritable(3));\n    Assert.assertNotNull(o);\n    Assert.assertEquals(new IntWritable(6), ((LazyInteger) o).getWritableObject());\n  ",
    "getNameWithOpIdStats": "\n    StringBuilder sb = new StringBuilder();\n    sb.append(DagJsonParserUtils.renameReduceOutputOperator(name, vertex));\n    if (operatorId != null) {\n      sb.append(\" [\" + operatorId + \"]\");\n    }\n    if (!DagJsonParserUtils.getOperatorNoStats().contains(name) && attrs.containsKey(\"Statistics:\")) {\n      sb.append(\" (\" + attrs.get(\"Statistics:\") + \")\");\n    }\n    attrs.remove(\"Statistics:\");\n    return sb.toString();\n  ",
    "blue": "\n    return append(BLUE, str);\n  ",
    "powAsDouble": "\n    if (this.signum == 0) {\n      return 0;\n    }\n    double val = doubleValue();\n    double result = Math.pow(val, n);\n    if (Double.isInfinite(result) || Double.isNaN(result)) {\n      SqlMathUtil.throwOverflowException();\n    }\n    return result;\n  }\n\n  /**\n   * Returns the signum of this {@code Decimal128}.\n   *\n   * @return -1, 0, or 1 as the value of this {@code Decimal128",
    "rename": "\n    LOG.info(\"Renaming \" + sourcePath + \" to \" + destPath);\n\n    // If destPath directory exists, rename call will move the sourcePath\n    // into destPath without failing. So check it before renaming.\n    if (fs.exists(destPath)) {\n      throw new IOException(\"Cannot rename the source path. The destination \"\n          + \"path already exists.\");\n    }\n    return fs.rename(sourcePath, destPath);\n  ",
    "getConstant": "\n    String b64Const = options.get(CONST_VAL);\n    return Base64.getDecoder().decode(b64Const.getBytes());\n  ",
    "uncheckedGetField": "\n    if (getFieldInited()[id]) {\n      return getFields()[id].getObject();\n    }\n    getFieldInited()[id] = true;\n\n    ColumnMapping columnMapping = columnMappings.get(id);\n\n    LazyObjectBase field = getFields()[id];\n\n    if (columnMapping instanceof HiveAccumuloMapColumnMapping) {\n      HiveAccumuloMapColumnMapping mapColumnMapping = (HiveAccumuloMapColumnMapping) columnMapping;\n\n      LazyAccumuloMap map = (LazyAccumuloMap) field;\n      map.init(row, mapColumnMapping);\n    } else {\n      byte[] value;\n      if (columnMapping instanceof HiveAccumuloRowIdColumnMapping) {\n        // Use the rowID directly\n        value = row.getRowId().getBytes();\n      } else if (columnMapping instanceof HiveAccumuloColumnMapping) {\n        HiveAccumuloColumnMapping accumuloColumnMapping = (HiveAccumuloColumnMapping) columnMapping;\n\n        // Use the colfam and colqual to get the value\n        value = row.getValue(\n            new Text(accumuloColumnMapping.getColumnFamilyBytes()),\n            new Text(accumuloColumnMapping.getColumnQualifierBytes()));\n      } else {\n        log.error(\"Could not process ColumnMapping of type \" + columnMapping.getClass()\n            + \" at offset \" + id + \" in column mapping: \" + columnMapping.getMappingSpec());\n        throw new IllegalArgumentException(\"Cannot process ColumnMapping of type \"\n            + columnMapping.getClass());\n      }\n      if (value == null || isNull(oi.getNullSequence(), value, 0, value.length)) {\n        field.setNull();\n      } else {\n        ByteArrayRef ref = new ByteArrayRef();\n        ref.setData(value);\n        field.init(ref, 0, value.length);\n      }\n    }\n\n    return field.getObject();\n  ",
    "stringToTimestamp": "\n    final String s = Objects.requireNonNull(text).trim();\n    // Handle simpler cases directly avoiding exceptions\n    if (s.length() == DATE_LENGTH) {\n      Date d = DateParser.parseDate(s);\n      if (d == null) {\n        throw new IllegalArgumentException(\"Cannot parse date: \" + text);\n      }\n      return Timestamp.ofEpochMilli(d.toEpochMilli());\n    }\n    try {\n      return Timestamp.valueOf(s);\n    } catch (IllegalArgumentException eT) {\n      // Try zoned timestamp\n      try {\n        return Timestamp.valueOf(\n            TimestampTZUtil.parse(s).getZonedDateTime().toLocalDateTime().toString());\n      } catch (IllegalArgumentException | DateTimeException eTZ) {\n        try {\n          // Try HH:mm:ss format (For Hour, Minute & Second UDF).\n          return Timestamp.getTimestampFromTime(s);\n        } catch(DateTimeException e) {\n          // Last attempt\n          return Timestamp.ofEpochMilli(Date.valueOf(s).toEpochMilli());\n        }\n      }\n    }\n  ",
    "uriToString": "\n    if (uris == null) {\n      return null;\n    }\n    StringBuilder ret = new StringBuilder(uris[0].toString());\n    for(int i = 1; i < uris.length;i++){\n      ret.append(\",\");\n      ret.append(uris[i].toString());\n    }\n    return ret.toString();\n  ",
    "setPort": "\n      this.port = port;\n      return this;\n    ",
    "testHiveAccumuloRecord": "\n    FileInputFormat.addInputPath(conf, new Path(\"unused\"));\n    InputSplit[] splits = inputformat.getSplits(conf, 0);\n    assertEquals(splits.length, 1);\n    RecordReader<Text,AccumuloHiveRow> reader = inputformat.getRecordReader(splits[0], conf, null);\n    Text rowId = new Text(\"r1\");\n    AccumuloHiveRow row = new AccumuloHiveRow();\n    row.add(COLUMN_FAMILY.toString(), NAME.toString(), \"brian\".getBytes());\n    row.add(COLUMN_FAMILY.toString(), SID.toString(), parseIntBytes(\"1\"));\n    row.add(COLUMN_FAMILY.toString(), DEGREES.toString(), parseDoubleBytes(\"44.5\"));\n    row.add(COLUMN_FAMILY.toString(), MILLIS.toString(), parseLongBytes(\"555\"));\n    assertTrue(reader.next(rowId, row));\n    assertEquals(rowId.toString(), row.getRowId());\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, NAME));\n    assertArrayEquals(\"brian\".getBytes(), row.getValue(COLUMN_FAMILY, NAME));\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, SID));\n    assertArrayEquals(parseIntBytes(\"1\"), row.getValue(COLUMN_FAMILY, SID));\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, DEGREES));\n    assertArrayEquals(parseDoubleBytes(\"44.5\"), row.getValue(COLUMN_FAMILY, DEGREES));\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, MILLIS));\n    assertArrayEquals(parseLongBytes(\"555\"), row.getValue(COLUMN_FAMILY, MILLIS));\n  ",
    "getTableNames": "\n    Schema.Table[] t = getSchema().getTables();\n    Set<String> names = new TreeSet<String>();\n    for (int i = 0; t != null && i < t.length; i++) {\n      names.add(t[i].getName());\n    }\n    return names.toArray(new String[names.size()]);\n  ",
    "createIndexDefinition": "\n      AccumuloIndexDefinition def = new AccumuloIndexDefinition(tname, iname);\n      String cols = AccumuloIndexedOutputFormat.getIndexColumns(job);\n      LOG.info(\"Index Cols = {}\", cols);\n      def.setColumnTuples(cols);\n      return def;\n    ",
    "initArgsFromCliVars": "\n    List<String> commands = Collections.emptyList();\n\n    CliOptionsProcessor optionsProcessor = new CliOptionsProcessor();\n    if (!optionsProcessor.process(args)) {\n      return 1;\n    }\n    CommandLine commandLine = optionsProcessor.getCommandLine();\n\n\n    Properties confProps = commandLine.getOptionProperties(\"hiveconf\");\n    for (String propKey : confProps.stringPropertyNames()) {\n      setHiveConfVar(propKey, confProps.getProperty(propKey));\n    }\n\n    Properties hiveVars = commandLine.getOptionProperties(\"define\");\n    for (String propKey : hiveVars.stringPropertyNames()) {\n      getOpts().getHiveConfVariables().put(propKey, hiveVars.getProperty(propKey));\n    }\n\n    Properties hiveVars2 = commandLine.getOptionProperties(\"hivevar\");\n    for (String propKey : hiveVars2.stringPropertyNames()) {\n      getOpts().getHiveConfVariables().put(propKey, hiveVars2.getProperty(propKey));\n    }\n\n    getOpts().setScriptFile(commandLine.getOptionValue(\"f\"));\n\n    if (commandLine.getOptionValues(\"i\") != null) {\n      getOpts().setInitFiles(commandLine.getOptionValues(\"i\"));\n    }\n\n    dbName = commandLine.getOptionValue(\"database\");\n    getOpts().setVerbose(Boolean.parseBoolean(commandLine.getOptionValue(\"verbose\")));\n    getOpts().setSilent(Boolean.parseBoolean(commandLine.getOptionValue(\"silent\")));\n\n    int code = 0;\n    if (commandLine.getOptionValues(\"e\") != null) {\n      commands = Arrays.asList(commandLine.getOptionValues(\"e\"));\n    }\n\n    if (!commands.isEmpty() && getOpts().getScriptFile() != null) {\n      System.err.println(\"The '-e' and '-f' options cannot be specified simultaneously\");\n      optionsProcessor.printCliUsage();\n      return 1;\n    }\n\n    if (!commands.isEmpty()) {\n      embeddedConnect();\n      connectDBInEmbededMode();\n      for (Iterator<String> i = commands.iterator(); i.hasNext(); ) {\n        String command = i.next().toString();\n        debug(loc(\"executing-command\", command));\n        if (!dispatch(command)) {\n          code++;\n        }\n      }\n      exit = true; // execute and exit\n    }\n    return code;\n  ",
    "testCompare": "\n    HiveIntervalYearMonth i1 = new HiveIntervalYearMonth(1, 2);\n    HiveIntervalYearMonth i2 = new HiveIntervalYearMonth(1, 2);\n    HiveIntervalYearMonth i3 = new HiveIntervalYearMonth(1, 3);\n\n    // compareTo()\n    assertEquals(i1 + \" compareTo \" + i1, 0, i1.compareTo(i1));\n    assertEquals(i1 + \" compareTo \" + i2, 0, i1.compareTo(i2));\n    assertEquals(i2 + \" compareTo \" + i1, 0, i2.compareTo(i1));\n    assertEquals(i3 + \" compareTo \" + i3, 0, i3.compareTo(i3));\n\n    assertTrue(i1 + \" compareTo \" + i3, 0 > i1.compareTo(i3));\n    assertTrue(i3 + \" compareTo \" + i1, 0 < i3.compareTo(i1));\n\n    // equals()\n    assertTrue(i1 + \" equals \" + i1, i1.equals(i1));\n    assertTrue(i1 + \" equals \" + i2, i1.equals(i2));\n    assertFalse(i1 + \" equals \" + i3, i1.equals(i3));\n    assertFalse(i3 + \" equals \" + i1, i3.equals(i1));\n\n    // hashCode()\n    assertEquals(i1 + \" hashCode \" + i1, i1.hashCode(), i1.hashCode());\n    assertEquals(i1 + \" hashCode \" + i1, i1.hashCode(), i2.hashCode());\n  ",
    "testGetOnlyName": "\n    FileInputFormat.addInputPath(conf, new Path(\"unused\"));\n\n    InputSplit[] splits = inputformat.getSplits(conf, 0);\n    assertEquals(splits.length, 1);\n    RecordReader<Text,AccumuloHiveRow> reader = inputformat.getRecordReader(splits[0], conf, null);\n    Text rowId = new Text(\"r1\");\n    AccumuloHiveRow row = new AccumuloHiveRow();\n    assertTrue(reader.next(rowId, row));\n    assertEquals(row.getRowId(), rowId.toString());\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, NAME));\n    assertArrayEquals(row.getValue(COLUMN_FAMILY, NAME), \"brian\".getBytes());\n\n    rowId = new Text(\"r2\");\n    assertTrue(reader.next(rowId, row));\n    assertEquals(row.getRowId(), rowId.toString());\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, NAME));\n    assertArrayEquals(row.getValue(COLUMN_FAMILY, NAME), \"mark\".getBytes());\n\n    rowId = new Text(\"r3\");\n    assertTrue(reader.next(rowId, row));\n    assertEquals(row.getRowId(), rowId.toString());\n    assertTrue(row.hasFamAndQual(COLUMN_FAMILY, NAME));\n    assertArrayEquals(row.getValue(COLUMN_FAMILY, NAME), \"dennis\".getBytes());\n\n    assertFalse(reader.next(rowId, row));\n  ",
    "testSasl": "\n    Configuration conf = new Configuration(false);\n\n    // Default is false\n    AccumuloConnectionParameters cnxnParams = new AccumuloConnectionParameters(conf);\n    assertFalse(cnxnParams.useSasl());\n\n    conf.set(AccumuloConnectionParameters.SASL_ENABLED, \"true\");\n\n    cnxnParams = new AccumuloConnectionParameters(conf);\n\n    assertTrue(cnxnParams.useSasl());\n\n    conf.set(AccumuloConnectionParameters.SASL_ENABLED, \"false\");\n    assertFalse(cnxnParams.useSasl());\n  ",
    "extractStagesAndPlans": "\n    // extract stages\n    JSONObject dependency = inputObject.getJSONObject(\"STAGE DEPENDENCIES\");\n    if (dependency != null && dependency.length() > 0) {\n      // iterate for the first time to get all the names of stages.\n      for (String stageName : JSONObject.getNames(dependency)) {\n        this.stages.put(stageName, new Stage(stageName, this));\n      }\n      // iterate for the second time to get all the dependency.\n      for (String stageName : JSONObject.getNames(dependency)) {\n        JSONObject dependentStageNames = dependency.getJSONObject(stageName);\n        this.stages.get(stageName).addDependency(dependentStageNames, this.stages);\n      }\n    }\n    // extract stage plans\n    JSONObject stagePlans = inputObject.getJSONObject(\"STAGE PLANS\");\n    if (stagePlans != null && stagePlans.length() > 0) {\n      for (String stageName : JSONObject.getNames(stagePlans)) {\n        JSONObject stagePlan = stagePlans.getJSONObject(stageName);\n        this.stages.get(stageName).extractVertex(stagePlan);\n      }\n    }\n  ",
    "extractHiInt": "\n    return (int) (val >> 32);\n  ",
    "getConnectionProperties": "\n    Properties props = new Properties();\n    String fileLocation = getFileLocation();\n    if (fileLocation == null) {\n      log.debug(\"User connection configuration file not found\");\n      return props;\n    }\n    log.info(\"Using connection configuration file at \" + fileLocation);\n    props.setProperty(HS2ConnectionFileParser.URL_PREFIX_PROPERTY_KEY, \"jdbc:hive2://\");\n    // load the properties from config file\n    Configuration conf = new Configuration(false);\n    conf.addResource(new Path(new File(fileLocation).toURI()));\n    try {\n      for (Entry<String, String> kv : conf) {\n        String key = kv.getKey();\n        if (key.startsWith(BEELINE_CONNECTION_PROPERTY_PREFIX)) {\n          props.setProperty(key.substring(BEELINE_CONNECTION_PROPERTY_PREFIX.length()),\n              kv.getValue());\n        }\n      }\n    } catch (Exception ex) {\n      throw new BeelineHS2ConnectionFileParseException(ex.getMessage(), ex);\n    }\n\n    return props;\n  ",
    "printUsage": "\n    new HelpFormatter().printHelp(cliname, OPTIONS);\n  ",
    "setOutputFormatConnectorInfo": "\n    try {\n      AccumuloOutputFormat.setConnectorInfo(conf, username, token);\n    } catch (IllegalStateException e) {\n      // AccumuloOutputFormat complains if you re-set an already set value. We just don't care.\n      log.debug(\"Ignoring exception setting Accumulo Connector instance for user \" + username, e);\n    }\n  }\n\n  /**\n   * Calls {@link AccumuloInputFormat#setZooKeeperInstance(JobConf, ClientConfiguration)",
    "testThatCliDriverDoesNotStripComments": "\n    // We need to overwrite System.out and System.err as that is what is used in ShellCmdExecutor\n    // So save old values...\n    PrintStream oldOut = System.out;\n    PrintStream oldErr = System.err;\n\n    // Capture stdout and stderr\n    ByteArrayOutputStream dataOut = new ByteArrayOutputStream();\n    SessionStream out = new SessionStream(dataOut);\n    System.setOut(out);\n    ByteArrayOutputStream dataErr = new ByteArrayOutputStream();\n    SessionStream err = new SessionStream(dataErr);\n    System.setErr(err);\n\n    CliSessionState ss = new CliSessionState(new HiveConf());\n    ss.out = out;\n    ss.err = err;\n\n    // Save output as yo cannot print it while System.out and System.err are weird\n    String message;\n    String errors;\n    try {\n      CliSessionState.start(ss);\n      CliDriver cliDriver = new CliDriver();\n      // issue a command with bad options\n      cliDriver.processCmd(\"!ls --abcdefghijklmnopqrstuvwxyz123456789\");\n      assertTrue(\"Comments with '--; should not have been stripped, so command should fail\", false);\n    } catch (CommandProcessorException e) {\n      // this is expected to happen\n    } finally {\n      // restore System.out and System.err\n      System.setOut(oldOut);\n      System.setErr(oldErr);\n    }\n    message = dataOut.toString(\"UTF-8\");\n    errors = dataErr.toString(\"UTF-8\");\n    assertTrue(\"Comments with '--; should not have been stripped,\"\n        + \" so we should have got an error in the output: '\" + errors + \"'.\",\n        errors.contains(\"option\"));\n    assertNotNull(message); // message kept around in for debugging\n  ",
    "setIsolation": "\n    this.isolation = isolation;\n  ",
    "getShutdownHook": "\n    return shutdownHook;\n  ",
    "testString": "\n    String strVal = \"The quick brown fox\";\n    byte[] value = strVal.getBytes(UTF_8);\n    assertArrayEquals(AccumuloIndexLexicoder.encodeValue(value, serdeConstants.STRING_TYPE_NAME,\n                                                        true), value);\n    assertArrayEquals(AccumuloIndexLexicoder.encodeValue(value, \"varChar(20)\",\n                                                        true), value);\n    assertArrayEquals(AccumuloIndexLexicoder.encodeValue(value, \"CHAR (20)\",\n                                                        true), value);\n  ",
    "createLogRunnable": "\n    if (statement instanceof HiveStatement) {\n      return new LogRunnable(this, (HiveStatement) statement, DEFAULT_QUERY_PROGRESS_INTERVAL,\n          eventNotifier);\n    } else {\n      beeLine.debug(\n          \"The statement instance is not HiveStatement type: \" + statement\n              .getClass());\n      return new Runnable() {\n        @Override\n        public void run() {\n          // do nothing.\n        }\n      };\n    }\n  ",
    "setAutoCommit": "\n    this.autoCommit = autoCommit;\n  ",
    "generateTemplate": "\n    DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\n    DocumentBuilder docBuilder = dbf.newDocumentBuilder();\n    Document doc = docBuilder.newDocument();\n    doc.appendChild(doc.createProcessingInstruction(\n        \"xml-stylesheet\", \"type=\\\"text/xsl\\\" href=\\\"configuration.xsl\\\"\"));\n\n    doc.appendChild(doc.createComment(\"\\n\" +\n        \"   Licensed to the Apache Software Foundation (ASF) under one or more\\n\" +\n        \"   contributor license agreements.  See the NOTICE file distributed with\\n\" +\n        \"   this work for additional information regarding copyright ownership.\\n\" +\n        \"   The ASF licenses this file to You under the Apache License, Version 2.0\\n\" +\n        \"   (the \\\"License\\\"); you may not use this file except in compliance with\\n\" +\n        \"   the License.  You may obtain a copy of the License at\\n\" +\n        \"\\n\" +\n        \"       http://www.apache.org/licenses/LICENSE-2.0\\n\" +\n        \"\\n\" +\n        \"   Unless required by applicable law or agreed to in writing, software\\n\" +\n        \"   distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\\n\" +\n        \"   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n\" +\n        \"   See the License for the specific language governing permissions and\\n\" +\n        \"   limitations under the License.\\n\"));\n\n    Element root = doc.createElement(\"configuration\");\n    doc.appendChild(root);\n\n    root.appendChild(doc.createComment(\n        \" WARNING!!! This file is auto generated for documentation purposes ONLY! \"));\n    root.appendChild(doc.createComment(\n        \" WARNING!!! Any changes you make to this file will be ignored by Hive.   \"));\n    root.appendChild(doc.createComment(\n        \" WARNING!!! You must make your changes in hive-site.xml instead.         \"));\n\n    root.appendChild(doc.createComment(\" Hive Execution Parameters \"));\n\n    Thread.currentThread().setContextClassLoader(ShimLoader.class.getClassLoader());\n    for (HiveConf.ConfVars confVars : HiveConf.ConfVars.values()) {\n      if (confVars.isExcluded()) {\n        // thought of creating template for each shims, but I couldn't generate proper mvn script\n        continue;\n      }\n      Element property = appendElement(root, \"property\", null);\n      appendElement(property, \"name\", confVars.varname);\n      appendElement(property, \"value\", confVars.getDefaultExpr());\n      appendElement(property, \"description\", normalize(confVars.getDescription()));\n      // wish to add new line here.\n    }\n    return doc;\n  ",
    "getColumnObjectInspectors": "\n    ArrayList<ObjectInspector> columnObjectInspectors = new ArrayList<ObjectInspector>(\n        columnTypes.size());\n    for (int i = 0; i < columnTypes.size(); i++) {\n      TypeInfo type = columnTypes.get(i);\n      ColumnMapping mapping = mappings.get(i);\n      if (mapping instanceof HiveAccumuloRowIdColumnMapping) {\n        columnObjectInspectors.add(factory.createRowIdObjectInspector(type));\n      } else {\n        columnObjectInspectors.add(LazyFactory.createLazyObjectInspector(type,\n            serDeParams.getSeparators(), 1, serDeParams.getNullSequence(), serDeParams.isEscaped(),\n            serDeParams.getEscapeChar()));\n      }\n    }\n\n    return columnObjectInspectors;\n  ",
    "getAllProperties": "\n    return getProperties(this);\n  ",
    "getVerbose": "\n    return verbose;\n  ",
    "processCmd": "protected abstract int processCmd(String cmd)",
    "inlineJoinOp": "\n    // inline map join operator\n    if (this.type == OpType.MAPJOIN) {\n      // get the map for posToVertex\n      Map<String, Vertex> posToVertex = new LinkedHashMap<>();\n      if (opObject.has(\"input vertices:\")) {\n        JSONObject vertexObj = opObject.getJSONObject(\"input vertices:\");\n        for (String pos : JSONObject.getNames(vertexObj)) {\n          String vertexName = vertexObj.getString(pos);\n          // update the connection\n          Connection c = null;\n          for (Connection connection : vertex.parentConnections) {\n            if (connection.from.name.equals(vertexName)) {\n              posToVertex.put(pos, connection.from);\n              c = connection;\n              break;\n            }\n          }\n          if (c != null) {\n            parser.addInline(this, c);\n          }\n        }\n        // update the attrs\n        this.attrs.remove(\"input vertices:\");\n      }\n      // update the keys to use operator name\n      JSONObject keys = opObject.getJSONObject(\"keys:\");\n      // find out the vertex for the big table\n      Set<Vertex> parentVertexes = new HashSet<>();\n      for (Connection connection : vertex.parentConnections) {\n        parentVertexes.add(connection.from);\n      }\n      parentVertexes.removeAll(posToVertex.values());\n      Map<String, String> posToOpId = new LinkedHashMap<>();\n      if (keys.length() != 0) {\n        for (String key : JSONObject.getNames(keys)) {\n          // first search from the posToVertex\n          if (posToVertex.containsKey(key)) {\n            Vertex v = posToVertex.get(key);\n            if (v.outputOps.size() == 1) {\n              posToOpId.put(key, v.outputOps.get(0).operatorId);\n            } else if ((v.outputOps.size() == 0 && v.vertexType == VertexType.UNION)) {\n              posToOpId.put(key, v.name);\n            } else {\n              Op joinRSOp = v.getJoinRSOp(vertex);\n              if (joinRSOp != null) {\n                posToOpId.put(key, joinRSOp.operatorId);\n              } else {\n                throw new Exception(\n                    \"Can not find join reduceSinkOp for \" + v.name + \" to join \" + vertex.name\n                        + \" when hive explain user is trying to identify the operator id.\");\n              }\n            }\n          }\n          // then search from parent\n          else if (parent != null) {\n            posToOpId.put(key, parent.operatorId);\n          }\n          // then assume it is from its own vertex\n          else if (parentVertexes.size() == 1) {\n            Vertex v = parentVertexes.iterator().next();\n            parentVertexes.clear();\n            if (v.outputOps.size() == 1) {\n              posToOpId.put(key, v.outputOps.get(0).operatorId);\n            } else if ((v.outputOps.size() == 0 && v.vertexType == VertexType.UNION)) {\n              posToOpId.put(key, v.name);\n            } else {\n              Op joinRSOp = v.getJoinRSOp(vertex);\n              if (joinRSOp != null) {\n                posToOpId.put(key, joinRSOp.operatorId);\n              } else {\n                throw new Exception(\n                    \"Can not find join reduceSinkOp for \" + v.name + \" to join \" + vertex.name\n                        + \" when hive explain user is trying to identify the operator id.\");\n              }\n            }\n          }\n          // finally throw an exception\n          else {\n            throw new Exception(\n                \"Can not find the source operator on one of the branches of map join.\");\n          }\n        }\n      }\n      this.attrs.remove(\"keys:\");\n      StringBuilder sb = new StringBuilder();\n      JSONArray conditionMap = opObject.getJSONArray(\"condition map:\");\n      for (int index = 0; index < conditionMap.length(); index++) {\n        JSONObject cond = conditionMap.getJSONObject(index);\n        String k = (String) cond.keys().next();\n        JSONObject condObject = new JSONObject((String)cond.get(k));\n        String type = condObject.getString(\"type\");\n        String left = condObject.getString(\"left\");\n        String right = condObject.getString(\"right\");\n        if (keys.length() != 0) {\n          sb.append(posToOpId.get(left) + \".\" + keys.get(left) + \"=\" + posToOpId.get(right) + \".\"\n              + keys.get(right) + \"(\" + type + \"),\");\n        } else {\n          // probably a cross product\n          sb.append(\"(\" + type + \"),\");\n        }\n      }\n      this.attrs.remove(\"condition map:\");\n      this.attrs.put(\"Conds:\", sb.substring(0, sb.length() - 1));\n    }\n    // should be merge join\n    else {\n      Map<String, String> posToOpId = new LinkedHashMap<>();\n      if (vertex.mergeJoinDummyVertices.isEmpty()) {\n        for (Entry<String, String> entry : vertex.tagToInput.entrySet()) {\n          Connection c = null;\n          for (Connection connection : vertex.parentConnections) {\n            if (connection.from.name.equals(entry.getValue())) {\n              Vertex v = connection.from;\n              if (v.outputOps.size() == 1) {\n                posToOpId.put(entry.getKey(), v.outputOps.get(0).operatorId);\n              } else if ((v.outputOps.size() == 0 && v.vertexType == VertexType.UNION)) {\n                posToOpId.put(entry.getKey(), v.name);\n              } else {\n                Op joinRSOp = v.getJoinRSOp(vertex);\n                if (joinRSOp != null) {\n                  posToOpId.put(entry.getKey(), joinRSOp.operatorId);\n                } else {\n                  throw new Exception(\n                      \"Can not find join reduceSinkOp for \" + v.name + \" to join \" + vertex.name\n                          + \" when hive explain user is trying to identify the operator id.\");\n                }\n              }\n              c = connection;\n              break;\n            }\n          }\n          if (c == null) {\n            throw new Exception(\"Can not find \" + entry.getValue()\n                + \" while parsing keys of merge join operator\");\n          }\n        }\n      } else {\n        posToOpId.put(vertex.tag, this.parent.operatorId);\n        for (Vertex v : vertex.mergeJoinDummyVertices) {\n          if (v.outputOps.size() != 1) {\n            throw new Exception(\"Can not find a single root operators in a single vertex \" + v.name\n                + \" when hive explain user is trying to identify the operator id.\");\n          }\n          posToOpId.put(v.tag, v.outputOps.get(0).operatorId);\n        }\n      }\n      // update the keys to use operator name\n      JSONObject keys = opObject.getJSONObject(\"keys:\");\n      if (keys.length() != 0) {\n        for (String key : JSONObject.getNames(keys)) {\n          if (!posToOpId.containsKey(key)) {\n            throw new Exception(\n                \"Can not find the source operator on one of the branches of merge join.\");\n          }\n        }\n        // inline merge join operator in a self-join\n        for (Vertex v : this.vertex.mergeJoinDummyVertices) {\n            parser.addInline(this, new Connection(null, v));\n        }\n      }\n      // update the attrs\n      this.attrs.remove(\"keys:\");\n      StringBuilder sb = new StringBuilder();\n      JSONArray conditionMap = opObject.getJSONArray(\"condition map:\");\n      for (int index = 0; index < conditionMap.length(); index++) {\n        JSONObject cond = conditionMap.getJSONObject(index);\n        String k = (String) cond.keys().next();\n        JSONObject condObject = new JSONObject((String)cond.get(k));\n        String type = condObject.getString(\"type\");\n        String left = condObject.getString(\"left\");\n        String right = condObject.getString(\"right\");\n        if (keys.length() != 0) {\n          sb.append(posToOpId.get(left) + \".\" + keys.get(left) + \"=\" + posToOpId.get(right) + \".\"\n              + keys.get(right) + \"(\" + type + \"),\");\n        } else {\n          // probably a cross product\n          sb.append(\"(\" + type + \"),\");\n        }\n      }\n      this.attrs.remove(\"condition map:\");\n      this.attrs.put(\"Conds:\", sb.substring(0, sb.length() - 1));\n    }\n  ",
    "testConfigureAccumuloInputFormat": "\n    AccumuloConnectionParameters accumuloParams = new AccumuloConnectionParameters(conf);\n    ColumnMapper columnMapper = new ColumnMapper(conf.get(AccumuloSerDeParameters.COLUMN_MAPPINGS),\n        conf.get(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE), columnNames, columnTypes);\n    Set<Pair<Text,Text>> cfCqPairs = inputformat\n        .getPairCollection(columnMapper.getColumnMappings());\n    List<IteratorSetting> iterators = Collections.emptyList();\n    Set<Range> ranges = Collections.singleton(new Range());\n    String instanceName = \"realInstance\";\n    String zookeepers = \"host1:2181,host2:2181,host3:2181\";\n\n    ZooKeeperInstance zkInstance = Mockito.mock(ZooKeeperInstance.class);\n    HiveAccumuloTableInputFormat mockInputFormat = Mockito.mock(HiveAccumuloTableInputFormat.class);\n    HiveAccumuloHelper helper = Mockito.mock(HiveAccumuloHelper.class);\n\n    // Stub out the ZKI mock\n    Mockito.when(zkInstance.getInstanceName()).thenReturn(instanceName);\n    Mockito.when(zkInstance.getZooKeepers()).thenReturn(zookeepers);\n    // Stub out a mocked Helper instance\n    Mockito.when(mockInputFormat.getHelper()).thenReturn(helper);\n\n    // Call out to the real configure method\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .configure(conf, zkInstance, con, accumuloParams, columnMapper, iterators, ranges);\n\n    // Also compute the correct cf:cq pairs so we can assert the right argument was passed\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .getPairCollection(columnMapper.getColumnMappings());\n\n    mockInputFormat.configure(conf, zkInstance, con, accumuloParams, columnMapper, iterators,\n        ranges);\n\n    // Verify that the correct methods are invoked on AccumuloInputFormat\n    Mockito.verify(helper).setInputFormatZooKeeperInstance(conf, instanceName, zookeepers, false);\n    Mockito.verify(helper).setInputFormatConnectorInfo(conf, USER, new PasswordToken(PASS));\n    Mockito.verify(mockInputFormat).setInputTableName(conf, TEST_TABLE);\n    Mockito.verify(mockInputFormat).setScanAuthorizations(conf,\n        con.securityOperations().getUserAuthorizations(USER));\n    Mockito.verify(mockInputFormat).addIterators(conf, iterators);\n    Mockito.verify(mockInputFormat).setRanges(conf, ranges);\n    Mockito.verify(mockInputFormat).fetchColumns(conf, cfCqPairs);\n  ",
    "testInvalidRowIdOffset": "\n    ArrayList<ColumnMapping> mappings = new ArrayList<ColumnMapping>();\n\n    // Should fail because of the -1\n    new AccumuloRowSerializer(-1, null, mappings, new ColumnVisibility(), null);\n  ",
    "getCmdList": "\n    if (entireLineAsCommand) {\n      return Stream.of(line).collect(Collectors.toList());\n    }\n    List<String> cmdList = new ArrayList<String>();\n    StringBuilder command = new StringBuilder();\n\n    // Marker to track if there is a special section open\n    SectionType sectionType = null;\n\n    // Index of the last seen delimiter in the given line\n    int lastDelimiterIndex = 0;\n\n    // Marker to track if the previous character was an escape character\n    boolean wasPrevEscape = false;\n\n    int index = 0;\n\n    // Iterate through the line and invoke the addCmdPart method whenever the delimiter is seen that is not inside a\n    // quoted string\n    for (; index < line.length();) {\n      if (!wasPrevEscape && sectionType == null && line.startsWith(\"'\", index)) {\n        // Opening non-escaped single quote\n        sectionType = SectionType.SINGLE_QUOTED;\n        index++;\n      } else if (!wasPrevEscape && sectionType == SectionType.SINGLE_QUOTED && line.startsWith(\"'\", index)) {\n        // Closing non-escaped single quote\n        sectionType = null;\n        index++;\n      } else if (!wasPrevEscape && sectionType == null && line.startsWith(\"\\\"\", index)) {\n        // Opening non-escaped double quote\n        sectionType = SectionType.DOUBLE_QUOTED;\n        index++;\n      } else if (!wasPrevEscape && sectionType == SectionType.DOUBLE_QUOTED && line.startsWith(\"\\\"\", index)) {\n        // Closing non-escaped double quote\n        sectionType = null;\n        index++;\n      } else if (sectionType == null && line.startsWith(\"--\", index)) {\n        // Opening line comment with (non-escapable?) double-dash\n        sectionType = SectionType.LINE_COMMENT;\n        wasPrevEscape = false;\n        index += 2;\n      } else if (sectionType == SectionType.LINE_COMMENT && line.startsWith(\"\\n\", index)) {\n        // Closing line comment with (non-escapable?) newline\n        sectionType = null;\n        wasPrevEscape = false;\n        index++;\n      } else if (sectionType == null && line.startsWith(\"/*\", index)) {\n        // Opening block comment with (non-escapable?) /*\n        sectionType = SectionType.BLOCK_COMMENT;\n        wasPrevEscape = false;\n        index += 2;\n      } else if (sectionType == SectionType.BLOCK_COMMENT && line.startsWith(\"*/\", index)) {\n        // Closing line comment with (non-escapable?) newline\n        sectionType = null;\n        wasPrevEscape = false;\n        index += 2;\n      } else if (line.startsWith(\"\\\\\", index)) {\n        // Escape character seen (anywhere)\n        wasPrevEscape = !wasPrevEscape;\n        index++;\n      } else if (sectionType == null && line.startsWith(beeLine.getOpts().getDelimiter(), index)) {\n        // If the delimiter is seen, and the line isn't inside a section, then treat\n        // line[lastDelimiterIndex] to line[index] as a single command\n        addCmdPart(cmdList, command, line.substring(lastDelimiterIndex, index));\n        index += beeLine.getOpts().getDelimiter().length();\n        lastDelimiterIndex = index;\n        wasPrevEscape = false;\n      } else {\n        wasPrevEscape = false;\n        index++;\n      }\n    }\n    // If the line doesn't end with the delimiter or if the line is empty, add the cmd part\n    if (lastDelimiterIndex != index || line.length() == 0) {\n      addCmdPart(cmdList, command, line.substring(lastDelimiterIndex, index));\n    }\n    return cmdList;\n  }\n\n  /**\n   * Given a cmdpart (e.g. if a command spans multiple lines), add to the current command, and if\n   * applicable add that command to the {@link List",
    "testAppendRestriction": "\n    String appendListStr = ConfVars.SCRATCH_DIR.varname + \",\" +\n        ConfVars.LOCAL_SCRATCH_DIR.varname + \",\" +\n        ConfVars.METASTORE_URIS.varname;\n\n    conf.addToRestrictList(appendListStr);\n    // check if the new configs are added to HIVE_CONF_RESTRICTED_LIST\n    String newRestrictList = conf.getVar(ConfVars.HIVE_CONF_RESTRICTED_LIST);\n    assertTrue(newRestrictList.contains(ConfVars.SCRATCH_DIR.varname));\n    assertTrue(newRestrictList.contains(ConfVars.LOCAL_SCRATCH_DIR.varname));\n    assertTrue(newRestrictList.contains(ConfVars.METASTORE_URIS.varname));\n\n    // check if the old values are still there in HIVE_CONF_RESTRICTED_LIST\n    assertTrue(newRestrictList.contains(ConfVars.HIVE_TEST_MODE_PREFIX.varname));\n\n    // verify that the new configs are in effect\n    verifyRestriction(ConfVars.HIVE_TEST_MODE_PREFIX.varname, \"foo\");\n    verifyRestriction(ConfVars.HIVE_CONF_RESTRICTED_LIST.varname, \"foo\");\n    verifyRestriction(ConfVars.LOCAL_SCRATCH_DIR.varname, \"foo\");\n    verifyRestriction(ConfVars.METASTORE_URIS.varname, \"foo\");\n  ",
    "beginMetrics": "\n    Metrics metrics = MetricsFactory.getInstance();\n    if (metrics != null) {\n      MetricsScope scope = metrics.createScope(MetricsConstant.API_PREFIX + method);\n      synchronized (openScopes) {\n        openScopes.put(method, scope);\n      }\n    }\n\n  ",
    "getPathOrParentThatExists": "\n      return stat;\n    }\n    Path parentPath = path.getParent();\n    return getPathOrParentThatExists(fs, parentPath);\n  ",
    "setupSpnegoFilter": "\n    Map<String, String> params = new HashMap<String, String>();\n    params.put(\"kerberos.principal\",\n      SecurityUtil.getServerPrincipal(b.spnegoPrincipal, b.host));\n    params.put(\"kerberos.keytab\", b.spnegoKeytab);\n    params.put(AuthenticationFilter.AUTH_TYPE, \"kerberos\");\n    FilterHolder holder = new FilterHolder();\n    holder.setClassName(AuthenticationFilter.class.getName());\n    holder.setInitParameters(params);\n    ServletHandler handler = ctx.getServletHandler();\n    handler.addFilterWithMapping(\n      holder, \"/*\", FilterMapping.ALL);\n  ",
    "getIsolation": "\n    return isolation;\n  ",
    "testParseZookeeper": "\n    String url = getParsedUrlFromConfigFile(\"test-hs2-connection-zookeeper-config.xml\");\n    String expectedUrl =\n        \"jdbc:hive2://zk-node-1:10000,zk-node-2:10001,zk-node-3:10004/default;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2\";\n    Assert.assertTrue(\"Expected \" + expectedUrl + \" got \" + url, expectedUrl.equals(url));\n  ",
    "wrap": "\n    StringBuilder buff = new StringBuilder();\n    StringBuilder line = new StringBuilder();\n\n    char[] head = new char[start];\n    Arrays.fill(head, ' ');\n\n    for (StringTokenizer tok = new StringTokenizer(toWrap, \" \"); tok.hasMoreTokens();) {\n      String next = tok.nextToken();\n      if (line.length() + next.length() > len) {\n        buff.append(line).append(separator).append(head);\n        line.setLength(0);\n      }\n\n      line.append(line.length() == 0 ? \"\" : \" \").append(next);\n    }\n\n    buff.append(line);\n    return buff.toString();\n  ",
    "getObjectInspector": "\n    return cachedObjectInspector;\n  ",
    "setExcludeCiphersuites": "\n      this.excludeCiphersuites = excludeCiphersuites;\n      return this;\n    ",
    "checkException": "\n    BeeLine testBeeLine = new TestBeeline(loc);\n    TTransportException tTransportException = new TTransportException(type);\n    SQLException sqlException = new SQLException(tTransportException);\n    testBeeLine.handleSQLException(sqlException);\n  ",
    "createRowId": "\n    LazyObjectBase lazyObj = LazyFactory.createLazyObject(inspector,\n        ColumnEncoding.BINARY == rowIdMapping.getEncoding());\n    log.info(\"Created \" + lazyObj.getClass() + \" for rowId with inspector \" + inspector.getClass());\n    return lazyObj;\n  ",
    "getNextCharacterSubstring": "\n    int end = index + token.length;\n    if (end > fullInput.length()) {\n      end = fullInput.length();\n    }\n    String substring = fullInput.substring(index, end);\n    if (token.length == 3) { //dy, mon\n      return substring;\n    }\n\n    // patterns day, month\n    String regex;\n    if (token.temporalField == ChronoField.MONTH_OF_YEAR) {\n      regex = MONTH_REGEX;\n    } else if (token.temporalField == ChronoField.DAY_OF_WEEK) {\n      regex = DAY_OF_WEEK_REGEX;\n    } else {\n      throw new IllegalArgumentException(\"Error at index \" + index + \": \" + token + \" not a \"\n          + \"character temporal with length not 3\");\n    }\n    Matcher matcher = Pattern.compile(regex, Pattern.CASE_INSENSITIVE).matcher(substring);\n    if (matcher.find()) {\n      return substring.substring(0, matcher.end());\n    }\n    throw new IllegalArgumentException(\n        \"Couldn't find \" + token.string + \" in substring \" + substring + \" at index \" + index);\n  ",
    "testSerializationUtilsWriteSpecial": "\n    Random r = new Random(998737);\n    for (BigDecimal bigDecimal : specialBigDecimals) {\n      doTestSerializationUtilsWrite(r, bigDecimal.unscaledValue());\n    }\n  ",
    "pad": "\n    if (str == null) {\n      str = \"\";\n    }\n    return pad(new ColorBuffer(str, false), len);\n  ",
    "isPathWithinSubtree": "\n      if (subtreeDepth > path.depth()) {\n        return false;\n      }\n      if(subtree.equals(path)){\n        return true;\n      }\n      path = path.getParent();\n    }\n    return false;\n  ",
    "compareTo": "\n    return zonedDateTime.toInstant().compareTo(o.zonedDateTime.toInstant());\n  ",
    "setInitFiles": "\n    this.initFiles = initFiles;\n  ",
    "processSelectDatabase": "\n    String database = ss.database;\n    if (database != null) {\n      processLineExitOnFailure(\"use \" + database + \";\");\n    }\n  ",
    "getServerName": "\n        return HtmlQuoting.quoteHtmlChars(rawRequest.getServerName());\n      }\n    }\n\n    @Override\n    public void init(FilterConfig config) throws ServletException {\n      this.config = config;\n      initHttpHeaderMap();\n    }\n\n    @Override\n    public void destroy() {\n    }\n\n    @Override\n    public void doFilter(ServletRequest request,\n                         ServletResponse response,\n                         FilterChain chain\n    ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted =\n              new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n      headerMap.forEach((k, v) -> httpResponse.addHeader(k, v));\n      chain.doFilter(quoted, httpResponse);\n    }\n\n    /**\n     * Infer the mime type for the response based on the extension of the request\n     * URI. Returns null if unknown.\n     */\n    private String inferMimeType(ServletRequest request) {\n      String path = ((HttpServletRequest)request).getRequestURI();\n      ServletContextHandler.Context sContext =\n              (ServletContextHandler.Context)config.getServletContext();\n      String mime = sContext.getMimeType(path);\n      return (mime == null) ? null : mime;\n    }\n\n    private void initHttpHeaderMap() {\n      Enumeration<String> params = this.config.getInitParameterNames();\n      headerMap = new HashMap<>();\n      while (params.hasMoreElements()) {\n        String key = params.nextElement();\n        Matcher m = PATTERN_HTTP_HEADER_REGEX.matcher(key);\n        if (m.matches()) {\n          String headerKey = m.group(1);\n          headerMap.put(headerKey, config.getInitParameter(key));\n        }\n      }\n    }\n  }\n\n",
    "testCommandSplits": "\n    // Test double quote in the string\n    String cmd1 = \"insert into escape1 partition (ds='1', part='\\\"') values (\\\"!\\\")\";\n    assertEquals(cmd1, CliDriver.splitSemiColon(cmd1).get(0));\n    assertEquals(cmd1, CliDriver.splitSemiColon(cmd1 + \";\").get(0));\n\n    // Test escape\n    String cmd2 = \"insert into escape1 partition (ds='1', part='\\\"\\\\'') values (\\\"!\\\")\";\n    assertEquals(cmd2, CliDriver.splitSemiColon(cmd2).get(0));\n    assertEquals(cmd2, CliDriver.splitSemiColon(cmd2 + \";\").get(0));\n\n    // Test multiple commands\n    List<String> results = CliDriver.splitSemiColon(cmd1 + \";\" + cmd2);\n    assertEquals(cmd1, results.get(0));\n    assertEquals(cmd2, results.get(1));\n\n    results = CliDriver.splitSemiColon(cmd1 + \";\" + cmd2 + \";\");\n    assertEquals(cmd1, results.get(0));\n    assertEquals(cmd2, results.get(1));\n  ",
    "getAuthorizationProvider": "\n    return null;\n  ",
    "parseColumnNames": "\n    // Are calculated in AccumuloSerDeParameters\n    return Collections.emptyList();\n  ",
    "testEmptyIteratorPushdownValue": "\n    TableDesc tableDesc = Mockito.mock(TableDesc.class);\n    Properties props = new Properties();\n    Map<String,String> jobProperties = new HashMap<String,String>();\n\n    props.setProperty(AccumuloSerDeParameters.COLUMN_MAPPINGS, \"cf:cq1,cf:cq2,cf:cq3\");\n    props.setProperty(AccumuloSerDeParameters.TABLE_NAME, \"table\");\n    props.setProperty(AccumuloSerDeParameters.ITERATOR_PUSHDOWN_KEY, \"\");\n\n    Mockito.when(tableDesc.getProperties()).thenReturn(props);\n\n    storageHandler.configureInputJobProperties(tableDesc, jobProperties);\n  ",
    "testGetMappingFromHiveColumn": "\n    List<String> hiveColumns = Arrays.asList(\"rowid\", \"col1\", \"col2\", \"col3\");\n    List<TypeInfo> columnTypes = Arrays.<TypeInfo> asList(TypeInfoFactory.stringTypeInfo,\n        TypeInfoFactory.stringTypeInfo, TypeInfoFactory.stringTypeInfo,\n        TypeInfoFactory.stringTypeInfo);\n    List<String> rawMappings = Arrays.asList(AccumuloHiveConstants.ROWID, \"cf:cq\", \"cf:_\",\n        \"cf:qual\");\n    ColumnMapper mapper = new ColumnMapper(\n        Joiner.on(AccumuloHiveConstants.COMMA).join(rawMappings), null, hiveColumns, columnTypes);\n\n    for (int i = 0; i < hiveColumns.size(); i++) {\n      String hiveColumn = hiveColumns.get(i), accumuloMapping = rawMappings.get(i);\n      ColumnMapping mapping = mapper.getColumnMappingForHiveColumn(hiveColumns, hiveColumn);\n\n      Assert.assertEquals(accumuloMapping, mapping.getMappingSpec());\n    }\n  ",
    "setDelta": "\n      return setValue(dest, val, deltaShift, DELTA_MASK);\n    ",
    "getTrimmedStrings": "\n    if (null == str || \"\".equals(str.trim())) {\n      return emptyStringArray;\n    }\n\n    return str.trim().split(\"\\\\s*,\\\\s*\");\n  ",
    "interrupt": "\n    synchronized (interruptCallbacks) {\n      for (HiveInterruptCallback resource : new ArrayList<HiveInterruptCallback>(interruptCallbacks)) {\n        resource.interrupt();\n      }\n    }\n  ",
    "getVersion": "\n    return version != null ? version.version() : \"Unknown\";\n  ",
    "stripHiddenConfigurations": "\n    HiveConfUtil.stripConfigurations(conf, hiddenSet);\n  ",
    "getCompareOpClass": "\n    if (!compareOps.containsKey(udfType)) {\n      throw new NoSuchCompareOpException(\"Null compare op for specified key: \" + udfType);\n    }\n    return compareOps.get(udfType);\n  ",
    "getNumGcWarnThresholdExceeded": "\n    return numGcWarnThresholdExceeded;\n  ",
    "getMetrics": "\n    MetricsRecordBuilder rb = collector.addRecord(JvmMetrics)\n        .setContext(\"jvm\").tag(ProcessName, processName)\n        .tag(SessionId, sessionId);\n    getMemoryUsage(rb);\n    getGcUsage(rb);\n    getThreadUsage(rb);\n    getEventCounters(rb);\n  ",
    "isPathWithinSubtree_samePrefix": "\n    Path path = new Path(\"/somedir1\");\n    Path subtree = new Path(\"/somedir\");\n\n    assertFalse(FileUtils.isPathWithinSubtree(path, subtree));\n  ",
    "getRevision": "\n    return version != null ? version.revision() : \"Unknown\";\n  ",
    "testSqlFromCmdWithEmbeddedQuotes": "\n    verifyCMD(null, \"hive\", out,\n        new String[] { \"-e\", \"select \\\"hive\\\"\" }, ERRNO_OK, true);\n  ",
    "getColumnMappingValue": "\n    return tableProperties.getProperty(COLUMN_MAPPINGS);\n  ",
    "getUser": "\n    return version != null ? version.user() : \"Unknown\";\n  ",
    "verifyAndSet": "\n    if (modWhiteListPattern != null) {\n      Matcher wlMatcher = modWhiteListPattern.matcher(name);\n      if (!wlMatcher.matches()) {\n        throw new IllegalArgumentException(\"Cannot modify \" + name + \" at runtime. \"\n            + \"It is not in list of params that are allowed to be modified at runtime\");\n      }\n    }\n    if (Iterables.any(restrictList,\n        restrictedVar -> name != null && name.startsWith(restrictedVar))) {\n      throw new IllegalArgumentException(\"Cannot modify \" + name + \" at runtime. It is in the list\"\n          + \" of parameters that can't be modified at runtime or is prefixed by a restricted variable\");\n    }\n    if (isLockedConfig(name)) {\n      return;\n    }\n    String oldValue = name != null ? get(name) : null;\n    if (name == null || value == null || !value.equals(oldValue)) {\n      // When either name or value is null, the set method below will fail,\n      // and throw IllegalArgumentException\n      set(name, value);\n    }\n  ",
    "getConnect": "\n    return connect;\n  ",
    "getErrorCode": "\n    return errorCode;\n  ",
    "testHistory": "\n    ByteArrayOutputStream os = new ByteArrayOutputStream();\n    PrintStream ops = new PrintStream(os);\n    BeeLine beeline = new BeeLine();\n    beeline.getOpts().setHistoryFile(fileName);\n    beeline.setOutputStream(ops);\n    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n    method.setAccessible(true);\n    method.invoke(beeline);\n    beeline.initializeConsoleReader(null);\n    beeline.dispatch(\"!history\");\n    String output = os.toString(\"UTF-8\");\n    String[] tmp = output.split(\"\\n\");\n    Assert.assertTrue(tmp[0].equals(\"1     : select 1;\"));\n    Assert.assertTrue(tmp[9].equals(\"10    : select 10;\"));\n    beeline.close();\n  ",
    "configureJobConf": "\n    helper.loadDependentJars(jobConf);\n\n    Properties tblProperties = tableDesc.getProperties();\n    AccumuloSerDeParameters serDeParams = null;\n    try {\n      serDeParams =\n          new AccumuloSerDeParameters(jobConf, tblProperties, AccumuloSerDe.class.getName());\n    } catch (SerDeException e) {\n      LOG.error(\"Could not instantiate AccumuloSerDeParameters\", e);\n      return;\n    }\n\n    try {\n      serDeParams.getRowIdFactory().addDependencyJars(jobConf);\n    } catch (IOException e) {\n      LOG.error(\"Could not add necessary dependencies for \"\n          + serDeParams.getRowIdFactory().getClass(), e);\n    }\n\n    // When Kerberos is enabled, we have to add the Accumulo delegation token to the\n    // Job so that it gets passed down to the YARN/Tez task.\n    if (connectionParams.useSasl()) {\n      try {\n        // Open an accumulo connection\n        Connector conn = connectionParams.getConnector();\n\n        // Convert the Accumulo token in a Hadoop token\n        Token<? extends TokenIdentifier> accumuloToken = helper.setConnectorInfoForInputAndOutput(connectionParams, conn, jobConf);\n\n        LOG.debug(\"Adding Hadoop Token for Accumulo to Job's Credentials\");\n\n        // Add the Hadoop token to the JobConf\n        helper.mergeTokenIntoJobConf(jobConf, accumuloToken);\n        LOG.debug(\"All job tokens: \" + jobConf.getCredentials().getAllTokens());\n      } catch (Exception e) {\n        throw new RuntimeException(\"Failed to obtain DelegationToken for \"\n            + connectionParams.getAccumuloUserName(), e);\n      }\n    }\n  ",
    "testConfigureMockAccumuloInputFormat": "\n    AccumuloConnectionParameters accumuloParams = new AccumuloConnectionParameters(conf);\n    ColumnMapper columnMapper = new ColumnMapper(conf.get(AccumuloSerDeParameters.COLUMN_MAPPINGS),\n        conf.get(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE), columnNames, columnTypes);\n    Set<Pair<Text,Text>> cfCqPairs = inputformat\n        .getPairCollection(columnMapper.getColumnMappings());\n    List<IteratorSetting> iterators = Collections.emptyList();\n    Set<Range> ranges = Collections.singleton(new Range());\n\n    HiveAccumuloTableInputFormat mockInputFormat = Mockito.mock(HiveAccumuloTableInputFormat.class);\n    HiveAccumuloHelper helper = Mockito.mock(HiveAccumuloHelper.class);\n\n    // Stub out a mocked Helper instance\n    Mockito.when(mockInputFormat.getHelper()).thenReturn(helper);\n\n    // Call out to the real configure method\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .configure(conf, mockInstance, con, accumuloParams, columnMapper, iterators, ranges);\n\n    // Also compute the correct cf:cq pairs so we can assert the right argument was passed\n    Mockito.doCallRealMethod().when(mockInputFormat)\n        .getPairCollection(columnMapper.getColumnMappings());\n\n    mockInputFormat.configure(conf, mockInstance, con, accumuloParams, columnMapper, iterators,\n        ranges);\n\n    // Verify that the correct methods are invoked on AccumuloInputFormat\n    Mockito.verify(helper).setInputFormatMockInstance(conf, mockInstance.getInstanceName());\n    Mockito.verify(helper).setInputFormatConnectorInfo(conf, USER, new PasswordToken(PASS));\n    Mockito.verify(mockInputFormat).setInputTableName(conf, TEST_TABLE);\n    Mockito.verify(mockInputFormat).setScanAuthorizations(conf,\n        con.securityOperations().getUserAuthorizations(USER));\n    Mockito.verify(mockInputFormat).addIterators(conf, iterators);\n    Mockito.verify(mockInputFormat).setRanges(conf, ranges);\n    Mockito.verify(mockInputFormat).fetchColumns(conf, cfCqPairs);\n  ",
    "processAndOpNode": "\n    // We might have multiple ranges coming from children\n    List<Range> andRanges = null;\n\n    for (Object nodeOutput : nodeOutputs) {\n      // null signifies nodes that are irrelevant to the generation\n      // of Accumulo Ranges\n      if (null == nodeOutput) {\n        continue;\n      }\n\n      // When an AND has no children (some conjunction over a field that isn't the column\n      // mapped to the Accumulo rowid) and when a conjunction generates Ranges which are empty\n      // (the children of the conjunction are disjoint), these two cases need to be kept separate.\n      //\n      // A null `andRanges` implies that ranges couldn't be computed, while an empty List\n      // of Ranges implies that there are no possible Ranges to lookup.\n      if (null == andRanges) {\n        andRanges = new ArrayList<Range>();\n      }\n\n      // The child is a single Range\n      if (nodeOutput instanceof Range) {\n        Range childRange = (Range) nodeOutput;\n\n        // No existing ranges, just accept the current\n        if (andRanges.isEmpty()) {\n          andRanges.add(childRange);\n        } else {\n          // For each range we have, intersect them. If they don't overlap\n          // the range can be discarded\n          List<Range> newRanges = new ArrayList<Range>();\n          for (Range andRange : andRanges) {\n            Range intersectedRange = andRange.clip(childRange, true);\n            if (null != intersectedRange) {\n              newRanges.add(intersectedRange);\n            }\n          }\n\n          // Set the newly-constructed ranges as the current state\n          andRanges = newRanges;\n        }\n      } else if (nodeOutput instanceof List) {\n        @SuppressWarnings(\"unchecked\")\n        List<Range> childRanges = (List<Range>) nodeOutput;\n\n        // No ranges, use the ranges from the child\n        if (andRanges.isEmpty()) {\n          andRanges.addAll(childRanges);\n        } else {\n          List<Range> newRanges = new ArrayList<Range>();\n\n          // Cartesian product of our ranges, to the child ranges\n          for (Range andRange : andRanges) {\n            for (Range childRange : childRanges) {\n              Range intersectedRange = andRange.clip(childRange, true);\n\n              // Retain only valid intersections (discard disjoint ranges)\n              if (null != intersectedRange) {\n                newRanges.add(intersectedRange);\n              }\n            }\n          }\n\n          // Set the newly-constructed ranges as the current state\n          andRanges = newRanges;\n        }\n      } else {\n        LOG.error(\"Expected Range from {} but got {}\", nd, nodeOutput);\n        throw new IllegalArgumentException(\"Expected Range but got \"\n            + nodeOutput.getClass().getName());\n      }\n    }\n\n    return andRanges;\n  ",
    "randHiveBigDecimalPair": "\n    BigDecimal[] pair = new BigDecimal[2];\n    BigDecimal bigDecimal1 = randHiveBigDecimal(r, digitAlphabet, bigDecimalFlavor);\n    pair[0] = bigDecimal1;\n\n    BigDecimal bigDecimal2;\n    switch (bigDecimalPairFlavor) {\n    case RANDOM:\n      bigDecimal2 = randHiveBigDecimal(r, digitAlphabet, bigDecimalFlavor);\n      break;\n    case NEAR:\n      bigDecimal2 = randHiveBigDecimalNear(r, bigDecimal1);\n      break;\n    case INVERSE:\n      bigDecimal2 = randHiveBigDecimalNear(r, bigDecimal1);\n      break;\n    default:\n      throw new RuntimeException(\"Unexpected big decimal pair flavor \" + bigDecimalPairFlavor);\n    }\n    pair[1] = bigDecimal2;\n    return pair;\n  ",
    "getV1": "\n    return this.mag.getV1();\n  ",
    "testHighPrecisionDecimal128Divide": "\n    final int N = 10000;\n    for (int i = 0; i < N; i++) {\n      verifyHighPrecisionDivideSingle();\n    }\n  ",
    "testScriptFile": "\n    TestBeeline bl = new TestBeeline();\n    String args[] = new String[] {\"-u\", \"url\", \"-n\", \"name\",\n      \"-p\", \"password\", \"-d\", \"driver\", \"-f\", \"myscript\"};\n    Assert.assertEquals(0, bl.initArgs(args));\n    Assert.assertTrue(bl.connectArgs.equals(\"url name password driver\"));\n    Assert.assertTrue(bl.getOpts().getScriptFile().equals(\"myscript\"));\n  ",
    "testNumHistories": "\n    ByteArrayOutputStream os = new ByteArrayOutputStream();\n    PrintStream ops = new PrintStream(os);\n    BeeLine beeline = new BeeLine();\n    beeline.getOpts().setHistoryFile(fileName);\n    beeline.setOutputStream(ops);\n    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n    method.setAccessible(true);\n    method.invoke(beeline);\n    beeline.initializeConsoleReader(null);\n    beeline.dispatch(\"!history\");\n    String output = os.toString(\"UTF-8\");\n    int numHistories = output.split(\"\\n\").length;\n    Assert.assertEquals(10, numHistories);\n    beeline.close();\n  ",
    "addZKServiceDiscoveryHosts": "\n    props.setProperty(\"serviceDiscoveryMode\", \"zooKeeper\");\n    props.setProperty(\"zooKeeperNamespace\",\n        HiveConf.getVar(conf, ConfVars.HIVE_SERVER2_ZOOKEEPER_NAMESPACE));\n    props.setProperty(\"hosts\", HiveConf.getVar(conf, ConfVars.HIVE_ZOOKEEPER_QUORUM));\n  ",
    "testColumnMappingCreatesAccumuloColumnMapping": "\n    ColumnMapping mapping = ColumnMappingFactory.get(\"cf:cq\", ColumnEncoding.STRING, \"col\",\n        TypeInfoFactory.stringTypeInfo);\n\n    Assert.assertEquals(HiveAccumuloColumnMapping.class, mapping.getClass());\n    Assert.assertEquals(\"col\", mapping.getColumnName());\n    Assert.assertEquals(TypeInfoFactory.stringTypeInfo.toString(), mapping.getColumnType());\n  ",
    "getTables": "\n      if (tables != null) {\n        return tables;\n      }\n\n      List<Table> tnames = new LinkedList<Table>();\n\n      try {\n        ResultSet rs = getDatabaseMetaData().getTables(getConnection().getCatalog(),\n            null, \"%\", new String[] {\"TABLE\"});\n        try {\n          while (rs.next()) {\n            tnames.add(new Table(rs.getString(\"TABLE_NAME\")));\n          }\n        } finally {\n          try {\n            rs.close();\n          } catch (Exception e) {\n          }\n        }\n      } catch (Throwable t) {\n      }\n      return tables = tnames.toArray(new Table[0]);\n    ",
    "getUtf8Value": "\n    // TODO is there a more correct way to get the literal value for the Object?\n    return new Text(objInspector.getWritableConstantValue().toString());\n  ",
    "setAutosave": "\n    this.autosave = autosave;\n  ",
    "testRangeConjunction": "\n    // rowId >= 'f'\n    ExprNodeDesc column = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null, false);\n    ExprNodeDesc constant = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"f\");\n    List<ExprNodeDesc> children = Lists.newArrayList();\n    children.add(column);\n    children.add(constant);\n    ExprNodeDesc node = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrGreaterThan(), children);\n    assertNotNull(node);\n\n    // rowId <= 'm'\n    ExprNodeDesc column2 = new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, \"rid\", null,\n        false);\n    ExprNodeDesc constant2 = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo, \"m\");\n    List<ExprNodeDesc> children2 = Lists.newArrayList();\n    children2.add(column2);\n    children2.add(constant2);\n    ExprNodeDesc node2 = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPEqualOrLessThan(), children2);\n    assertNotNull(node2);\n\n    // And UDF\n    List<ExprNodeDesc> bothFilters = Lists.newArrayList();\n    bothFilters.add(node);\n    bothFilters.add(node2);\n    ExprNodeGenericFuncDesc both = new ExprNodeGenericFuncDesc(TypeInfoFactory.stringTypeInfo,\n        new GenericUDFOPAnd(), bothFilters);\n\n    // Should generate [f,m]\n    List<Range> expectedRanges = Arrays\n        .asList(new Range(new Key(\"f\"), true, new Key(\"m\\0\"), false));\n\n    AccumuloRangeGenerator rangeGenerator = new AccumuloRangeGenerator(conf, handler, rowIdMapping, \"rid\");\n    SemanticDispatcher disp = new DefaultRuleDispatcher(rangeGenerator,\n        Collections.<SemanticRule, SemanticNodeProcessor> emptyMap(), null);\n    SemanticGraphWalker ogw = new DefaultGraphWalker(disp);\n    ArrayList<Node> topNodes = new ArrayList<Node>();\n    topNodes.add(both);\n    HashMap<Node,Object> nodeOutput = new HashMap<Node,Object>();\n\n    try {\n      ogw.startWalking(topNodes, nodeOutput);\n    } catch (SemanticException ex) {\n      throw new RuntimeException(ex);\n    }\n\n    Object result = nodeOutput.get(both);\n    Assert.assertNotNull(result);\n    Assert.assertTrue(\"Result from graph walk was not a List\", result instanceof List);\n    @SuppressWarnings(\"unchecked\")\n    List<Range> actualRanges = (List<Range>) result;\n    Assert.assertEquals(expectedRanges, actualRanges);\n  ",
    "millisToSeconds": "\n    if (millis >= 0) {\n      return millis / 1000;\n    } else {\n      return (millis - 999) / 1000;\n    }\n  ",
    "testEncodingDecoding": "\n    HiveConf conf = new HiveConf();\n    String query = \"select blah, '\\u0001' from random_table\";\n    conf.setQueryString(query);\n    Assert.assertEquals(URLEncoder.encode(query, \"UTF-8\"), conf.get(ConfVars.HIVE_QUERY_STRING.varname));\n    Assert.assertEquals(query, conf.getQueryString());\n  ",
    "writeArrayToByteStream": "\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\n\n    if (url != null) {\n      writeCmdLine(\"!connect \" + url, out);\n    }\n    writeCmdLine(\"!brief\", out);\n    writeCmdLine(\"!set silent true\", out);\n    resultFile = File.createTempFile(tabName, \".out\");\n    if (!\"true\".equals(System.getProperty(\"proxyAuth.debug\", \"false\"))) {\n      resultFile.deleteOnExit();\n    }\n    writeCmdLine(\"!record \" + resultFile.getPath(), out);\n\n    for (String stmt: dmlStmts) {\n      writeSqlLine(stmt, out);\n    }\n\n    for (String stmt: selectStmts) {\n      writeSqlLine(stmt, out);\n    }\n\n    for (String stmt: cleanUpStmts) {\n      writeSqlLine(stmt, out);\n    }\n    writeCmdLine(\"!record\", out);\n    writeCmdLine(\"!quit\", out);\n\n    File tmpFile = File.createTempFile(tabName, \".q\");\n    tmpFile.deleteOnExit();\n    scriptFileName = tmpFile.getPath();\n    FileOutputStream fstream = new FileOutputStream(scriptFileName);\n    out.writeTo(fstream);\n\n    inpStream = new ByteArrayInputStream(out.toByteArray());\n    return resultFile;\n  ",
    "testParseNoAuthentication": "\n    String url = getParsedUrlFromConfigFile(\"test-hs2-connection-config-noauth.xml\");\n    String expectedUrl = \"jdbc:hive2://localhost:10000/default;user=hive\";\n    Assert.assertTrue(\"Expected \" + expectedUrl + \" got \" + url, expectedUrl.equals(url));\n  ",
    "findNext": "\n    int numPreEscapes = 0;\n    for (int i = start; i < str.length(); i++) {\n      char curChar = str.charAt(i);\n      if (numPreEscapes == 0 && curChar == separator) { // separator\n        return i;\n      } else {\n        split.append(curChar);\n        numPreEscapes = (curChar == escapeChar)\n                        ? (++numPreEscapes) % 2\n                        : 0;\n      }\n    }\n    return -1;\n  ",
    "go": "\n    String[] parts = beeLine.split(line, 2, \"Usage: go <connection index>\");\n    if (parts == null) {\n      return false;\n    }\n    int index = Integer.parseInt(parts[1]);\n    if (!(beeLine.getDatabaseConnections().setIndex(index))) {\n      beeLine.error(beeLine.loc(\"invalid-connection\", \"\" + index));\n      list(\"\"); // list the current connections\n      return false;\n    }\n    return true;\n  ",
    "getColumn": "\n      return rowData[idx - 1];\n    }\n  }\n",
    "getMergedPropertiesString": "\n    String properties = \"\";\n    if ((userConnectionProperties != null) && (userConnectionProperties.containsKey(propertyKey))) {\n      properties =\n          extractHiveVariables((String) userConnectionProperties.remove(propertyKey), true);\n    }\n    String propertiesFromJdbcUri = \"\";\n    for (Map.Entry<String, String> entry : propertiesFromJdbcConnParams.entrySet()) {\n      if (!properties.contains(entry.getKey())) {\n        if (!propertiesFromJdbcUri.isEmpty()) {\n          propertiesFromJdbcUri = propertiesFromJdbcUri + \",\";\n        }\n        propertiesFromJdbcUri = propertiesFromJdbcUri + entry.getKey() + \"=\" + entry.getValue();\n      }\n    }\n    if (!propertiesFromJdbcUri.isEmpty()) {\n      if (!properties.isEmpty()) {\n        properties = properties + \",\";\n      }\n      properties = properties + propertiesFromJdbcUri;\n    }\n    return properties;\n  ",
    "tokenizeCmd": "\n    return cmd.split(\"\\\\s+\");\n  ",
    "extractHiveVariables": "\n    StringBuilder hivePropertiesList = new StringBuilder();\n    String delimiter;\n    if (isHiveConf) {\n      delimiter = \"?\";\n    } else {\n      delimiter = \"#\";\n    }\n    hivePropertiesList.append(delimiter);\n    addPropertyValues(propertyValue, hivePropertiesList);\n    return hivePropertiesList.toString();\n  ",
    "bitLength": "\n    if (v3 != 0) {\n      return (short) (bitLengthInWord(v3) + 96);\n    }\n    if (v2 != 0) {\n      return (short) (bitLengthInWord(v2) + 64);\n    }\n    if (v1 != 0) {\n      return (short) (bitLengthInWord(v1) + 32);\n    }\n    return bitLengthInWord(v0);\n  }\n\n  /**\n   * If we can assume JDK 1.8, this should use\n   * java.lang.Integer.compareUnsigned(), which will be replaced with intrinsics\n   * in JVM.\n   *\n   * @param x\n   *          the first {@code int} to compare\n   * @param y\n   *          the second {@code int} to compare\n   * @return the value {@code 0} if {@code x == y}; a value less than {@code 0}\n   *         if {@code x < y} as unsigned values; and a value greater than\n   *         {@code 0} if {@code x > y",
    "testRelativePathToAbsolutePath": "\n    LocalFileSystem localFileSystem = new LocalFileSystem();\n    Path actualPath = FileUtils.makeAbsolute(localFileSystem, new Path(\"relative/path\"));\n    Path expectedPath = new Path(localFileSystem.getWorkingDirectory(), \"relative/path\");\n    assertEquals(expectedPath.toString(), actualPath.toString());\n\n    Path absolutePath = new Path(\"/absolute/path\");\n    Path unchangedPath = FileUtils.makeAbsolute(localFileSystem, new Path(\"/absolute/path\"));\n\n    assertEquals(unchangedPath.toString(), absolutePath.toString());\n  ",
    "testBinaryTypes": "\n    final String tableName = test.getMethodName(), user = \"root\", pass = \"\";\n\n    MockInstance mockInstance = new MockInstance(test.getMethodName());\n    Connector conn = mockInstance.getConnector(user, new PasswordToken(pass));\n    HiveAccumuloTableInputFormat inputformat = new HiveAccumuloTableInputFormat();\n    JobConf conf = new JobConf();\n\n    conf.set(AccumuloSerDeParameters.TABLE_NAME, tableName);\n    conf.set(AccumuloSerDeParameters.USE_MOCK_INSTANCE, \"true\");\n    conf.set(AccumuloSerDeParameters.INSTANCE_NAME, test.getMethodName());\n    conf.set(AccumuloSerDeParameters.USER_NAME, user);\n    conf.set(AccumuloSerDeParameters.USER_PASS, pass);\n    conf.set(AccumuloSerDeParameters.ZOOKEEPERS, \"localhost:2181\"); // not used for mock, but\n                                                                    // required by input format.\n\n    conf.set(AccumuloSerDeParameters.COLUMN_MAPPINGS, AccumuloHiveConstants.ROWID\n        + \",cf:string,cf:boolean,cf:tinyint,cf:smallint,cf:int,cf:bigint\"\n        + \",cf:float,cf:double,cf:decimal,cf:date,cf:timestamp,cf:char,cf:varchar\");\n    conf.set(\n        serdeConstants.LIST_COLUMNS,\n        \"string,string,boolean,tinyint,smallint,int,bigint,float,double,decimal,date,timestamp,char(4),varchar(7)\");\n    conf.set(\n        serdeConstants.LIST_COLUMN_TYPES,\n        \"string,string,boolean,tinyint,smallint,int,bigint,float,double,decimal,date,timestamp,char(4),varchar(7)\");\n    conf.set(AccumuloSerDeParameters.DEFAULT_STORAGE_TYPE, \"binary\");\n\n    conn.tableOperations().create(tableName);\n    BatchWriterConfig writerConf = new BatchWriterConfig();\n    BatchWriter writer = conn.createBatchWriter(tableName, writerConf);\n\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    DataOutputStream out = new DataOutputStream(baos);\n\n    String cf = \"cf\";\n    byte[] cfBytes = cf.getBytes();\n\n    Mutation m = new Mutation(\"row1\");\n\n    // string\n    String stringValue = \"string\";\n    JavaStringObjectInspector stringOI = (JavaStringObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME));\n    LazyUtils.writePrimitiveUTF8(baos, stringOI.create(stringValue), stringOI, false, (byte) 0,\n        null);\n    m.put(cfBytes, \"string\".getBytes(), baos.toByteArray());\n\n    // boolean\n    boolean booleanValue = true;\n    baos.reset();\n    JavaBooleanObjectInspector booleanOI = (JavaBooleanObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, booleanOI.create(booleanValue), booleanOI);\n    m.put(cfBytes, \"boolean\".getBytes(), baos.toByteArray());\n\n    // tinyint\n    byte tinyintValue = -127;\n    baos.reset();\n    JavaByteObjectInspector byteOI = (JavaByteObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.TINYINT_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, tinyintValue, byteOI);\n    m.put(cfBytes, \"tinyint\".getBytes(), baos.toByteArray());\n\n    // smallint\n    short smallintValue = Short.MAX_VALUE;\n    baos.reset();\n    JavaShortObjectInspector shortOI = (JavaShortObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, smallintValue, shortOI);\n    m.put(cfBytes, \"smallint\".getBytes(), baos.toByteArray());\n\n    // int\n    int intValue = Integer.MAX_VALUE;\n    baos.reset();\n    JavaIntObjectInspector intOI = (JavaIntObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, intValue, intOI);\n    m.put(cfBytes, \"int\".getBytes(), baos.toByteArray());\n\n    // bigint\n    long bigintValue = Long.MAX_VALUE;\n    baos.reset();\n    JavaLongObjectInspector longOI = (JavaLongObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, bigintValue, longOI);\n    m.put(cfBytes, \"bigint\".getBytes(), baos.toByteArray());\n\n    // float\n    float floatValue = Float.MAX_VALUE;\n    baos.reset();\n    JavaFloatObjectInspector floatOI = (JavaFloatObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, floatValue, floatOI);\n    m.put(cfBytes, \"float\".getBytes(), baos.toByteArray());\n\n    // double\n    double doubleValue = Double.MAX_VALUE;\n    baos.reset();\n    JavaDoubleObjectInspector doubleOI = (JavaDoubleObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME));\n    LazyUtils.writePrimitive(baos, doubleValue, doubleOI);\n    m.put(cfBytes, \"double\".getBytes(), baos.toByteArray());\n\n    // decimal\n    baos.reset();\n    HiveDecimal decimalValue = HiveDecimal.create(65536l);\n    HiveDecimalWritable decimalWritable = new HiveDecimalWritable(decimalValue);\n    decimalWritable.write(out);\n    m.put(cfBytes, \"decimal\".getBytes(), baos.toByteArray());\n\n    // date\n    baos.reset();\n    Date now = Date.ofEpochMilli(System.currentTimeMillis());\n    DateWritableV2 dateWritable = new DateWritableV2(now);\n    Date dateValue = dateWritable.get();\n    dateWritable.write(out);\n    m.put(cfBytes, \"date\".getBytes(), baos.toByteArray());\n\n    // tiemestamp\n    baos.reset();\n    Timestamp timestampValue = Timestamp.ofEpochMilli(System.currentTimeMillis());\n    ByteStream.Output output = new ByteStream.Output();\n    TimestampWritableV2 timestampWritable = new TimestampWritableV2(timestampValue);\n    timestampWritable.write(new DataOutputStream(output));\n    output.close();\n    m.put(cfBytes, \"timestamp\".getBytes(), output.toByteArray());\n\n    // char\n    baos.reset();\n    HiveChar charValue = new HiveChar(\"char\", 4);\n    JavaHiveCharObjectInspector charOI = (JavaHiveCharObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(new CharTypeInfo(4));\n    LazyUtils.writePrimitiveUTF8(baos, charOI.create(charValue), charOI, false, (byte) 0, null);\n    m.put(cfBytes, \"char\".getBytes(), baos.toByteArray());\n\n    baos.reset();\n    HiveVarchar varcharValue = new HiveVarchar(\"varchar\", 7);\n    JavaHiveVarcharObjectInspector varcharOI = (JavaHiveVarcharObjectInspector) PrimitiveObjectInspectorFactory\n        .getPrimitiveJavaObjectInspector(new VarcharTypeInfo(7));\n    LazyUtils.writePrimitiveUTF8(baos, varcharOI.create(varcharValue), varcharOI, false, (byte) 0,\n        null);\n    m.put(cfBytes, \"varchar\".getBytes(), baos.toByteArray());\n\n    writer.addMutation(m);\n\n    writer.close();\n\n    for (Entry<Key,Value> e : conn.createScanner(tableName, new Authorizations())) {\n      System.out.println(e);\n    }\n\n    // Create the RecordReader\n    FileInputFormat.addInputPath(conf, new Path(\"unused\"));\n    InputSplit[] splits = inputformat.getSplits(conf, 0);\n    assertEquals(splits.length, 1);\n    RecordReader<Text,AccumuloHiveRow> reader = inputformat.getRecordReader(splits[0], conf, null);\n\n    Text key = reader.createKey();\n    AccumuloHiveRow value = reader.createValue();\n\n    reader.next(key, value);\n\n    Assert.assertEquals(13, value.getTuples().size());\n\n    ByteArrayRef byteRef = new ByteArrayRef();\n\n    // string\n    Text cfText = new Text(cf), cqHolder = new Text();\n    cqHolder.set(\"string\");\n    byte[] valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyStringObjectInspector lazyStringOI = LazyPrimitiveObjectInspectorFactory\n        .getLazyStringObjectInspector(false, (byte) 0);\n    LazyString lazyString = (LazyString) LazyFactory.createLazyObject(lazyStringOI);\n    lazyString.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(stringValue, lazyString.getWritableObject().toString());\n\n    // boolean\n    cqHolder.set(\"boolean\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyBooleanObjectInspector lazyBooleanOI = (LazyBooleanObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME));\n    LazyBoolean lazyBoolean = (LazyBoolean) LazyFactory\n        .createLazyPrimitiveBinaryClass(lazyBooleanOI);\n    lazyBoolean.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(booleanValue, lazyBoolean.getWritableObject().get());\n\n    // tinyint\n    cqHolder.set(\"tinyint\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyByteObjectInspector lazyByteOI = (LazyByteObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.TINYINT_TYPE_NAME));\n    LazyByte lazyByte = (LazyByte) LazyFactory.createLazyPrimitiveBinaryClass(lazyByteOI);\n    lazyByte.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(tinyintValue, lazyByte.getWritableObject().get());\n\n    // smallint\n    cqHolder.set(\"smallint\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyShortObjectInspector lazyShortOI = (LazyShortObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME));\n    LazyShort lazyShort = (LazyShort) LazyFactory.createLazyPrimitiveBinaryClass(lazyShortOI);\n    lazyShort.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(smallintValue, lazyShort.getWritableObject().get());\n\n    // int\n    cqHolder.set(\"int\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyIntObjectInspector lazyIntOI = (LazyIntObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME));\n    LazyInteger lazyInt = (LazyInteger) LazyFactory.createLazyPrimitiveBinaryClass(lazyIntOI);\n    lazyInt.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(intValue, lazyInt.getWritableObject().get());\n\n    // bigint\n    cqHolder.set(\"bigint\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyLongObjectInspector lazyLongOI = (LazyLongObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME));\n    LazyLong lazyLong = (LazyLong) LazyFactory.createLazyPrimitiveBinaryClass(lazyLongOI);\n    lazyLong.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(bigintValue, lazyLong.getWritableObject().get());\n\n    // float\n    cqHolder.set(\"float\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyFloatObjectInspector lazyFloatOI = (LazyFloatObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME));\n    LazyFloat lazyFloat = (LazyFloat) LazyFactory.createLazyPrimitiveBinaryClass(lazyFloatOI);\n    lazyFloat.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(floatValue, lazyFloat.getWritableObject().get(), 0);\n\n    // double\n    cqHolder.set(\"double\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyDoubleObjectInspector lazyDoubleOI = (LazyDoubleObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(TypeInfoFactory\n            .getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME));\n    LazyDouble lazyDouble = (LazyDouble) LazyFactory.createLazyPrimitiveBinaryClass(lazyDoubleOI);\n    lazyDouble.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(doubleValue, lazyDouble.getWritableObject().get(), 0);\n\n    // decimal\n    cqHolder.set(\"decimal\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    ByteArrayInputStream bais = new ByteArrayInputStream(valueBytes);\n    DataInputStream in = new DataInputStream(bais);\n    decimalWritable.readFields(in);\n\n    Assert.assertEquals(decimalValue, decimalWritable.getHiveDecimal());\n\n    // date\n    cqHolder.set(\"date\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    bais = new ByteArrayInputStream(valueBytes);\n    in = new DataInputStream(bais);\n    dateWritable.readFields(in);\n\n    Assert.assertEquals(dateValue, dateWritable.get());\n\n    // timestamp\n    cqHolder.set(\"timestamp\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    bais = new ByteArrayInputStream(valueBytes);\n    in = new DataInputStream(bais);\n    timestampWritable.readFields(in);\n\n    Assert.assertEquals(timestampValue, timestampWritable.getTimestamp());\n\n    // char\n    cqHolder.set(\"char\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyHiveCharObjectInspector lazyCharOI = (LazyHiveCharObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(new CharTypeInfo(4));\n    LazyHiveChar lazyChar = (LazyHiveChar) LazyFactory.createLazyObject(lazyCharOI);\n    lazyChar.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(charValue, lazyChar.getWritableObject().getHiveChar());\n\n    // varchar\n    cqHolder.set(\"varchar\");\n    valueBytes = value.getValue(cfText, cqHolder);\n    Assert.assertNotNull(valueBytes);\n\n    byteRef.setData(valueBytes);\n    LazyHiveVarcharObjectInspector lazyVarcharOI = (LazyHiveVarcharObjectInspector) LazyPrimitiveObjectInspectorFactory\n        .getLazyObjectInspector(new VarcharTypeInfo(7));\n    LazyHiveVarchar lazyVarchar = (LazyHiveVarchar) LazyFactory.createLazyObject(lazyVarcharOI);\n    lazyVarchar.init(byteRef, 0, valueBytes.length);\n\n    Assert.assertEquals(varcharValue.toString(), lazyVarchar.getWritableObject().getHiveVarchar()\n        .toString());\n  ",
    "testExtractVertexTezWithOneEdge": "\n    String jsonString = \"{\\\"Tez\\\":{\\\"a\\\":\\\"A\\\",\" +\n            \"\\\"Vertices:\\\":{\\\"v1\\\":{},\\\"v2\\\":{}},\" +\n            \"\\\"Edges:\\\":{\\\"v2\\\":{\\\"parent\\\":\\\"v1\\\",\\\"type\\\":\\\"TYPE\\\"}}}}\";\n    JSONObject object = new JSONObject(jsonString);\n    uut.extractVertex(object);\n\n    assertEquals(2, uut.vertices.size());\n    assertTrue(uut.vertices.containsKey(\"v1\"));\n    assertTrue(uut.vertices.containsKey(\"v2\"));\n\n    assertEquals(0, uut.vertices.get(\"v1\").parentConnections.size());\n    assertEquals(1, uut.vertices.get(\"v2\").parentConnections.size());\n    assertEquals(\"v1\", uut.vertices.get(\"v2\").parentConnections.get(0).from.name);\n    assertEquals(\"TYPE\", uut.vertices.get(\"v2\").parentConnections.get(0).type);\n\n  ",
    "isReport": "\n    return report;\n  ",
    "internUriStringsInPathArray": "\n    if (paths != null) {\n      for (Path path : paths) {\n        internUriStringsInPath(path);\n      }\n    }\n    return paths;\n  ",
    "getTuples": "\n    return Collections.unmodifiableList(tuples);\n  ",
    "getLlapDaemonConfVars": "\n    return llapDaemonVarsSet;\n  ",
    "hasRowIdMapping": "\n    return null != rowIdMapping;\n  ",
    "writeResponse": "\n    //redact the sensitive information from the configuration values\n    Configuration hconf = new Configuration(conf);\n    HiveConfUtil.stripConfigurations(hconf);\n    if (FORMAT_JSON.equals(format)) {\n      Configuration.dumpConfiguration(hconf, out);\n    } else if (FORMAT_XML.equals(format)) {\n      hconf.writeXml(out);\n    } else {\n      throw new BadFormatException(\"Bad format: \" + format);\n    }\n  ",
    "write": "\n    if ((off < 0) || (off > b.length) || (len < 0) || ((off + len) > b.length)\n        || ((off + len) < 0)) {\n      throw new IndexOutOfBoundsException();\n    }\n    if (len == 0) {\n      return;\n    }\n    enLargeBuffer(len);\n    System.arraycopy(b, off, buf, count, len);\n    count += len;\n  }\n\n  /**\n   * {@inheritDoc",
    "squareDestructive": "\n     this.multiplyDestructive(this, this.getScale());\n     return this;\n  ",
    "testParseAuthorizationsFromConf": "\n    Configuration conf = new Configuration(false);\n    conf.set(AccumuloSerDeParameters.AUTHORIZATIONS_KEY, \"foo,bar\");\n\n    Authorizations auths = AccumuloSerDeParameters.getAuthorizationsFromConf(conf);\n    Assert.assertEquals(new Authorizations(\"foo,bar\"), auths);\n  ",
    "inRange": "protected abstract boolean inRange(String value, Object lower, Object upper);\n  ",
    "testMissingColumnMappingFails": "\n    MockInstance inst = new MockInstance(test.getMethodName());\n    Connector conn = inst.getConnector(\"root\", new PasswordToken(\"\"));\n    String tableName = \"table\";\n\n    // Empty parameters are sent, no COLUMN_MAPPING\n    Map<String,String> params = new HashMap<String,String>();\n\n    AccumuloConnectionParameters connectionParams = Mockito\n        .mock(AccumuloConnectionParameters.class);\n    AccumuloStorageHandler storageHandler = Mockito.mock(AccumuloStorageHandler.class);\n    StorageDescriptor sd = Mockito.mock(StorageDescriptor.class);\n    Table table = Mockito.mock(Table.class);\n    SerDeInfo serDeInfo = Mockito.mock(SerDeInfo.class);\n\n    // Call the real preCreateTable method\n    Mockito.doCallRealMethod().when(storageHandler).preCreateTable(table);\n\n    // Return our known table name\n    Mockito.when(storageHandler.getTableName(table)).thenReturn(tableName);\n\n    // Is marked for purge\n    Mockito.when(storageHandler.isPurge(table)).thenReturn(true);\n\n    // Return the mocked StorageDescriptor\n    Mockito.when(table.getSd()).thenReturn(sd);\n\n    // No location expected with AccumuloStorageHandler\n    Mockito.when(sd.getLocation()).thenReturn(null);\n\n    // Return mocked SerDeInfo\n    Mockito.when(sd.getSerdeInfo()).thenReturn(serDeInfo);\n\n    // Custom parameters\n    Mockito.when(serDeInfo.getParameters()).thenReturn(params);\n\n    // Return the MockInstance's Connector\n    Mockito.when(connectionParams.getConnector()).thenReturn(conn);\n\n    storageHandler.connectionParams = connectionParams;\n\n    storageHandler.preCreateTable(table);\n  ",
    "setDefaultAccumuloTableName": "\n    AccumuloIndexedOutputFormat.setDefaultTableName(conf, tableName);\n  ",
    "greaterThanOrEqual": "\n    GreaterThanOrEqual greaterThanOrEqualObj = new GreaterThanOrEqual(strCompare);\n    byte[] val = \"aab\".getBytes();\n\n    assertTrue(greaterThanOrEqualObj.accept(val));\n\n    val = \"aa\".getBytes();\n    assertFalse(greaterThanOrEqualObj.accept(val));\n\n    val = \"aaa\".getBytes();\n    assertTrue(greaterThanOrEqualObj.accept(val));\n  ",
    "setTemplateFile": "\n    this.templateFile = templateFile;\n  ",
    "getOperatorNoStats": "\n    return operatorNoStats;\n  ",
    "moreHiveColumnsThanAccumuloColumns": "\n    Properties properties = new Properties();\n    Configuration conf = new Configuration();\n\n    properties.setProperty(AccumuloSerDeParameters.COLUMN_MAPPINGS, \":rowID,cf:f3\");\n    properties.setProperty(serdeConstants.LIST_COLUMNS, \"row,field1,field2,field3,field4\");\n    properties.setProperty(serdeConstants.LIST_COLUMN_TYPES, \"string,string,string,string,string\");\n\n    serde.initialize(conf, properties, null);\n    serde.deserialize(new Text(\"fail\"));\n  ",
    "testGetHookBeeLineWithoutShowDbInPrompt": "\n    BeeLine beeLine = setupMockData(true, false);\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"set a;\"));\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"set a=b;\"));\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"USE a.b\"));\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"coNNect a.b\"));\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"gO 1\"));\n    Assert.assertNull(ClientCommandHookFactory.get().getHook(beeLine, \"g\"));\n  ",
    "findYarnBinary": "\n      String val = findHadoopHome();\n      val = (val == null ? \"yarn\" : val + File.separator + \"bin\" + File.separator + \"yarn\");\n      return val;\n    ",
    "getNanos": "\n    return zonedDateTime.toInstant().getNano();\n  ",
    "isIso8601Delimiter": "\n    return candidate.length() == 1 && VALID_ISO_8601_DELIMITERS.contains(candidate);\n  ",
    "getSleepTime": "",
    "testMockInstance": "\n    HiveAccumuloTableOutputFormat outputFormat = Mockito.mock(HiveAccumuloTableOutputFormat.class);\n    HiveAccumuloHelper helper = Mockito.mock(HiveAccumuloHelper.class);\n    conf.setBoolean(AccumuloConnectionParameters.USE_MOCK_INSTANCE, true);\n    conf.unset(AccumuloConnectionParameters.ZOOKEEPERS);\n\n    Mockito.when(outputFormat.getHelper()).thenReturn(helper);\n    Mockito.doCallRealMethod().when(outputFormat).configureAccumuloOutputFormat(conf);\n    Mockito.doCallRealMethod().when(outputFormat).getConnectionParams(conf);\n\n    outputFormat.configureAccumuloOutputFormat(conf);\n\n    Mockito.verify(helper).setOutputFormatConnectorInfo(conf, user, new PasswordToken(password));\n    Mockito.verify(helper).setOutputFormatMockInstance(conf, instanceName);\n    Mockito.verify(outputFormat).setDefaultAccumuloTableName(conf, outputTable);\n  ",
    "divideDestructive": "\n    this.mag.divideDestructive(right.mag, remainder.mag);\n    remainder.negative = false; // remainder is always positive\n    this.negative = this.negative ^ right.negative;\n  ",
    "checkFormatDate": "\n    formatter = new HiveSqlDateTimeFormatter(pattern, false,\n        Optional.of(LocalDateTime.ofInstant(Instant.EPOCH, ZoneOffset.UTC)));\n    assertEquals(\"Format date to string failed with pattern: \" + pattern,\n        expectedOutput, formatter.format(Date.valueOf(input)));\n  ",
    "createScanner": "\n    AccumuloIndexScanner handler;\n\n    String classname = conf.get(INDEX_SCANNER);\n    if (classname != null) {\n      try {\n        handler = (AccumuloIndexScanner) Class.forName(classname).newInstance();\n      } catch (ClassCastException | InstantiationException |  IllegalAccessException\n          | ClassNotFoundException e) {\n        throw new AccumuloIndexScannerException(\"Cannot use index scanner class: \" + classname, e);\n      }\n    } else {\n      handler = new AccumuloDefaultIndexScanner();\n    }\n    if (handler != null) {\n      handler.init(conf);\n    }\n    return handler;\n  ",
    "checkConfVar": "\n    Assert.assertEquals(expectedConfVarVal, var.getDefaultValue());\n  ",
    "verifyIsPathWithInSubTree": "\n    boolean result = FileUtils.isPathWithinSubtree(splitPath, key);\n    assertEquals(\"splitPath=\" + splitPath + \", key=\" + key, expected, result);\n  ",
    "runCmdAsync": "\n    try {\n      LOG.info(\"Running command async: \" + cmd);\n      return new ProcessBuilder(cmd).inheritIO().start();\n    } catch (IOException ex) {\n      throw new IllegalStateException(ex);\n    }\n  ",
    "parseCharacterTemporalToken": "\n    // keep original case\n    candidate = originalPattern.substring(begin, begin + candidate.length());\n\n    Token lastAddedToken = new Token(TokenType.CHARACTER_TEMPORAL,\n        CHARACTER_TEMPORAL_TOKENS.get(candidate.toLowerCase()), candidate,\n        getTokenStringLength(candidate), fillMode);\n    tokens.add(lastAddedToken);\n    return lastAddedToken;\n  "
}